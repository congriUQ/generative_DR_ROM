{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/constantin/anaconda3/envs/genDRROM/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/constantin/anaconda3/envs/genDRROM/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/constantin/anaconda3/envs/genDRROM/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/constantin/anaconda3/envs/genDRROM/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/constantin/anaconda3/envs/genDRROM/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/constantin/anaconda3/envs/genDRROM/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from poisson_fem import PoissonFEM\n",
    "import ROM\n",
    "import GenerativeSurrogate as gs\n",
    "import Data as dta\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "import scipy.sparse.linalg as lg\n",
    "import time\n",
    "import petsc4py\n",
    "import sys\n",
    "petsc4py.init(sys.argv)\n",
    "from petsc4py import PETSc\n",
    "import torch\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Some parameters\n",
    "lin_dim_rom = 4                      # Linear number of rom elements\n",
    "a = np.array([1, 1, 0])              # Boundary condition function coefficients\n",
    "dtype = torch.float                  # Tensor data type\n",
    "supervised_samples = {n for n in range(16)}\n",
    "unsupervised_samples = {n for n in range(16, 272)}\n",
    "dim_z = 20\n",
    "dim_z_supervised=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Define mesh and boundary conditions\n",
    "mesh = PoissonFEM.RectangularMesh(np.ones(lin_dim_rom)/lin_dim_rom)\n",
    "# mesh.plot()\n",
    "\n",
    "def origin(x):\n",
    "    return np.abs(x[0]) < np.finfo(float).eps and np.abs(x[1]) < np.finfo(float).eps\n",
    "\n",
    "def ess_boundary_fun(x):\n",
    "    return 0.0\n",
    "mesh.set_essential_boundary(origin, ess_boundary_fun)\n",
    "\n",
    "def domain_boundary(x):\n",
    "    # unit square\n",
    "    return np.abs(x[0]) < np.finfo(float).eps or np.abs(x[1]) < np.finfo(float).eps or \\\n",
    "            np.abs(x[0]) > 1.0 - np.finfo(float).eps or np.abs(x[1]) > 1.0 - np.finfo(float).eps\n",
    "mesh.set_natural_boundary(domain_boundary)\n",
    "\n",
    "def flux(x):\n",
    "    q = np.array([a[0] + a[2]*x[1], a[1] + a[2]*x[0]])\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify right hand side and stiffness matrix\n",
    "rhs = PoissonFEM.RightHandSide(mesh)\n",
    "rhs.set_natural_rhs(mesh, flux)\n",
    "K = PoissonFEM.StiffnessMatrix(mesh)\n",
    "rhs.set_rhs_stencil(mesh, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = dta.StokesData(supervised_samples, unsupervised_samples)\n",
    "trainingData.read_data()\n",
    "# trainingData.plotMicrostruct(1)\n",
    "trainingData.reshape_microstructure_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define rom\n",
    "rom = ROM.ROM(mesh, K, rhs, trainingData.output_resolution**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gs.GenerativeSurrogate(rom, trainingData, dim_z=dim_z, dim_z_supervised=dim_z_supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save()\n",
    "# loaded_model = gs.GenerativeSurrogate()\n",
    "# loaded_model.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample ==  0\n",
      "loss_lambda_c =  45945471369216.0\n",
      "loss_lambda_c =  357083152384.0\n",
      "loss_lambda_c =  341057994752.0\n",
      "loss_lambda_c =  335339487232.0\n",
      "loss_lambda_c =  332537856000.0\n",
      "loss_lambda_c =  331105796096.0\n",
      "Epoch  2503: reducing learning rate of group 0 to 3.0000e-03.\n",
      "Epoch  2520: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  2536: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  2552: reducing learning rate of group 0 to 3.0000e-06.\n",
      "sample ==  1\n",
      "loss_lambda_c =  20042003513344.0\n",
      "loss_lambda_c =  196687839232.0\n",
      "loss_lambda_c =  190270406656.0\n",
      "loss_lambda_c =  188073230336.0\n",
      "Epoch  1718: reducing learning rate of group 0 to 3.0000e-03.\n",
      "Epoch  1735: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  1751: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  1767: reducing learning rate of group 0 to 3.0000e-06.\n",
      "sample ==  2\n",
      "loss_lambda_c =  655100825567232.0\n",
      "Epoch    83: reducing learning rate of group 0 to 3.0000e-03.\n",
      "loss_lambda_c =  2471118766080.0\n",
      "loss_lambda_c =  1786405715968.0\n",
      "loss_lambda_c =  1599470829568.0\n",
      "loss_lambda_c =  1516699123712.0\n",
      "loss_lambda_c =  1457400971264.0\n",
      "loss_lambda_c =  1410348613632.0\n",
      "loss_lambda_c =  1372206399488.0\n",
      "loss_lambda_c =  1341790093312.0\n",
      "loss_lambda_c =  1318096207872.0\n",
      "loss_lambda_c =  1299740098560.0\n",
      "loss_lambda_c =  1285424021504.0\n",
      "loss_lambda_c =  1274175422464.0\n",
      "loss_lambda_c =  1265317183488.0\n",
      "loss_lambda_c =  1258358571008.0\n",
      "loss_lambda_c =  1252903616512.0\n",
      "Epoch  7947: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  7964: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  7980: reducing learning rate of group 0 to 3.0000e-06.\n",
      "sample ==  3\n",
      "loss_lambda_c =  779831037919232.0\n",
      "Epoch    84: reducing learning rate of group 0 to 3.0000e-03.\n",
      "loss_lambda_c =  2819128033280.0\n",
      "loss_lambda_c =  2009115918336.0\n",
      "loss_lambda_c =  1829797101568.0\n",
      "loss_lambda_c =  1734603177984.0\n",
      "loss_lambda_c =  1679384379392.0\n",
      "loss_lambda_c =  1654825418752.0\n",
      "Epoch  3453: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  3470: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  3486: reducing learning rate of group 0 to 3.0000e-06.\n",
      "sample ==  4\n",
      "loss_lambda_c =  695270513836032.0\n",
      "Epoch    83: reducing learning rate of group 0 to 3.0000e-03.\n",
      "loss_lambda_c =  2652665806848.0\n",
      "loss_lambda_c =  1775968976896.0\n",
      "loss_lambda_c =  1304877334528.0\n",
      "loss_lambda_c =  1080256430080.0\n",
      "loss_lambda_c =  973300301824.0\n",
      "loss_lambda_c =  925223485440.0\n",
      "loss_lambda_c =  904564572160.0\n",
      "loss_lambda_c =  895728877568.0\n",
      "Epoch  4452: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  4469: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  4485: reducing learning rate of group 0 to 3.0000e-06.\n",
      "sample ==  5\n",
      "loss_lambda_c =  1371030736076800.0\n",
      "loss_lambda_c =  1836765151232.0\n",
      "loss_lambda_c =  1561565855744.0\n",
      "loss_lambda_c =  1520573874176.0\n",
      "Epoch  1786: reducing learning rate of group 0 to 3.0000e-03.\n",
      "Epoch  1803: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  1819: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  1835: reducing learning rate of group 0 to 3.0000e-06.\n",
      "sample ==  6\n",
      "loss_lambda_c =  361779456638976.0\n",
      "Epoch    77: reducing learning rate of group 0 to 3.0000e-03.\n",
      "Epoch    93: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch   109: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch   125: reducing learning rate of group 0 to 3.0000e-06.\n",
      "sample ==  7\n",
      "loss_lambda_c =  163783980875776.0\n",
      "Epoch    70: reducing learning rate of group 0 to 3.0000e-03.\n",
      "Epoch    86: reducing learning rate of group 0 to 3.0000e-04.\n",
      "loss_lambda_c =  1620044021760.0\n",
      "loss_lambda_c =  1425674993664.0\n",
      "loss_lambda_c =  1257710682112.0\n",
      "loss_lambda_c =  1122097692672.0\n",
      "loss_lambda_c =  1019680784384.0\n",
      "loss_lambda_c =  942511161344.0\n",
      "loss_lambda_c =  881311088640.0\n",
      "loss_lambda_c =  830269947904.0\n",
      "loss_lambda_c =  786342871040.0\n",
      "loss_lambda_c =  746676289536.0\n",
      "loss_lambda_c =  707812130816.0\n",
      "loss_lambda_c =  666863271936.0\n",
      "loss_lambda_c =  624305635328.0\n",
      "loss_lambda_c =  585112158208.0\n",
      "loss_lambda_c =  553318088704.0\n",
      "loss_lambda_c =  528293429248.0\n",
      "loss_lambda_c =  508781592576.0\n",
      "loss_lambda_c =  494325497856.0\n",
      "loss_lambda_c =  484303896576.0\n",
      "loss_lambda_c =  477790011392.0\n",
      "loss_lambda_c =  473761021952.0\n",
      "loss_lambda_c =  471282876416.0\n",
      "Epoch 11431: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch 11448: reducing learning rate of group 0 to 3.0000e-06.\n",
      "sample ==  8\n",
      "loss_lambda_c =  25884161474560.0\n",
      "loss_lambda_c =  93003218944.0\n",
      "loss_lambda_c =  82223169536.0\n",
      "loss_lambda_c =  79944826880.0\n",
      "loss_lambda_c =  79025684480.0\n",
      "loss_lambda_c =  78508548096.0\n",
      "loss_lambda_c =  78177673216.0\n",
      "Epoch  3107: reducing learning rate of group 0 to 3.0000e-03.\n",
      "Epoch  3124: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  3140: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  3156: reducing learning rate of group 0 to 3.0000e-06.\n",
      "sample ==  9\n",
      "loss_lambda_c =  135633121050624.0\n",
      "loss_lambda_c =  304567713792.0\n",
      "loss_lambda_c =  277502361600.0\n",
      "loss_lambda_c =  273332764672.0\n",
      "Epoch  1620: reducing learning rate of group 0 to 3.0000e-03.\n",
      "Epoch  1637: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  1653: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  1669: reducing learning rate of group 0 to 3.0000e-06.\n",
      "sample ==  10\n",
      "loss_lambda_c =  282008659951616.0\n",
      "Epoch    75: reducing learning rate of group 0 to 3.0000e-03.\n",
      "Epoch    91: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch   123: reducing learning rate of group 0 to 3.0000e-06.\n",
      "sample ==  11\n",
      "loss_lambda_c =  124475399995392.0\n",
      "Epoch    67: reducing learning rate of group 0 to 3.0000e-03.\n",
      "loss_lambda_c =  756383154176.0\n",
      "loss_lambda_c =  515495690240.0\n",
      "loss_lambda_c =  429511639040.0\n",
      "loss_lambda_c =  384510197760.0\n",
      "loss_lambda_c =  359558742016.0\n",
      "loss_lambda_c =  345267306496.0\n",
      "loss_lambda_c =  336426500096.0\n",
      "loss_lambda_c =  330687545344.0\n",
      "loss_lambda_c =  326801096704.0\n",
      "loss_lambda_c =  323929866240.0\n",
      "loss_lambda_c =  321539375104.0\n",
      "loss_lambda_c =  319377997824.0\n",
      "loss_lambda_c =  317436264448.0\n",
      "loss_lambda_c =  315812904960.0\n",
      "loss_lambda_c =  314557267968.0\n",
      "Epoch  7668: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  7685: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  7701: reducing learning rate of group 0 to 3.0000e-06.\n",
      "sample ==  12\n",
      "loss_lambda_c =  53850761330688.0\n",
      "loss_lambda_c =  286617501696.0\n",
      "loss_lambda_c =  274603950080.0\n",
      "loss_lambda_c =  269035864064.0\n",
      "loss_lambda_c =  265735946240.0\n",
      "loss_lambda_c =  263702478848.0\n",
      "loss_lambda_c =  262388219904.0\n",
      "Epoch  3376: reducing learning rate of group 0 to 3.0000e-03.\n",
      "Epoch  3393: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  3409: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  3425: reducing learning rate of group 0 to 3.0000e-06.\n",
      "sample ==  13\n",
      "loss_lambda_c =  50397792174080.0\n",
      "Epoch    58: reducing learning rate of group 0 to 3.0000e-03.\n",
      "loss_lambda_c =  429674823680.0\n",
      "loss_lambda_c =  315639463936.0\n",
      "loss_lambda_c =  285630201856.0\n",
      "loss_lambda_c =  272981786624.0\n",
      "loss_lambda_c =  266270212096.0\n",
      "loss_lambda_c =  262607388672.0\n",
      "loss_lambda_c =  260524179456.0\n",
      "loss_lambda_c =  259255664640.0\n",
      "Epoch  4253: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  4270: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  4286: reducing learning rate of group 0 to 3.0000e-06.\n",
      "sample ==  14\n",
      "loss_lambda_c =  309383774863360.0\n",
      "loss_lambda_c =  595492864000.0\n",
      "loss_lambda_c =  520263139328.0\n",
      "loss_lambda_c =  508618014720.0\n",
      "Epoch  1701: reducing learning rate of group 0 to 3.0000e-03.\n",
      "Epoch  1718: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  1734: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  1750: reducing learning rate of group 0 to 3.0000e-06.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1874911035392.0\n",
      "loss_lambda_c =  55413809152.0\n",
      "loss_lambda_c =  54490316800.0\n",
      "Epoch  1304: reducing learning rate of group 0 to 3.0000e-03.\n",
      "Epoch  1321: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  1337: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  1353: reducing learning rate of group 0 to 3.0000e-06.\n"
     ]
    }
   ],
   "source": [
    "for n in range(model.data.n_supervised_samples):\n",
    "    print('sample == ', n)\n",
    "    model.log_lambdac_mean[n].max_iter = 3e4\n",
    "    model.log_lambdac_mean[n].converge(model, model.data.n_supervised_samples, mode=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  0\n",
      "loss z =  12748624.0\n",
      "loss z =  12723624.0\n",
      "loss z =  12700334.0\n",
      "loss z =  12678778.0\n",
      "loss z =  12658641.0\n",
      "loss z =  12639934.0\n",
      "loss z =  12622554.0\n",
      "loss z =  12606394.0\n",
      "loss z =  12591324.0\n",
      "z step =  5.24725079536438 s\n",
      "loss_f =  12559499.0\n",
      "loss_f =  12141416.0\n",
      "thetaf step =  6.934699296951294 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  331093868544.0\n",
      "Epoch  2568: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  2584: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  2600: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2616: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample ==  1\n",
      "loss_lambda_c =  187719385088.0\n",
      "Epoch  1783: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  1799: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  1815: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  1831: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample ==  2\n",
      "loss_lambda_c =  1249009074176.0\n",
      "loss_lambda_c =  1241229295616.0\n",
      "Epoch  8610: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  8627: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  8643: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8659: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample ==  3\n",
      "loss_lambda_c =  1646716780544.0\n",
      "Epoch  3648: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  3667: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  3683: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3699: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample ==  4\n",
      "loss_lambda_c =  892111945728.0\n",
      "Epoch  4712: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  4729: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  4745: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4761: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample ==  5\n",
      "loss_lambda_c =  1515545427968.0\n",
      "Epoch  1851: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  1867: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  1883: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  1899: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample ==  6\n",
      "loss_lambda_c =  2544456171520.0\n",
      "loss_lambda_c =  784932929536.0\n",
      "loss_lambda_c =  754776276992.0\n",
      "loss_lambda_c =  743557562368.0\n",
      "loss_lambda_c =  737795178496.0\n",
      "loss_lambda_c =  734320001024.0\n",
      "Epoch  2737: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  2754: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  2770: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2786: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample ==  7\n",
      "loss_lambda_c =  469827846144.0\n",
      "loss_lambda_c =  461219659776.0\n",
      "Epoch 12056: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 12073: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 12089: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12105: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample ==  8\n",
      "loss_lambda_c =  78122237952.0\n",
      "Epoch  3172: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  3188: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  3204: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3220: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample ==  9\n",
      "loss_lambda_c =  273080680448.0\n",
      "Epoch  1685: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  1701: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  1717: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  1733: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample ==  10\n",
      "loss_lambda_c =  3064338579456.0\n",
      "loss_lambda_c =  1046500933632.0\n",
      "loss_lambda_c =  749221642240.0\n",
      "loss_lambda_c =  696999870464.0\n",
      "loss_lambda_c =  677116182528.0\n",
      "loss_lambda_c =  668260958208.0\n",
      "loss_lambda_c =  664451022848.0\n",
      "Epoch  3220: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  3237: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  3253: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3269: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample ==  11\n",
      "loss_lambda_c =  314206486528.0\n",
      "loss_lambda_c =  312413257728.0\n",
      "Epoch  8204: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  8221: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  8237: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8253: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample ==  12\n",
      "loss_lambda_c =  261682905088.0\n",
      "Epoch  3441: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  3457: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  3473: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3489: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample ==  13\n",
      "loss_lambda_c =  258800484352.0\n",
      "Epoch  4641: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  4658: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  4674: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4690: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample ==  14\n",
      "loss_lambda_c =  507627995136.0\n",
      "Epoch  1766: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  1782: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  1798: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  1814: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample ==  15\n",
      "loss_lambda_c =  54346006528.0\n",
      "Epoch  1369: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  1385: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  1401: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  1417: reducing learning rate of group 0 to 1.0000e-06.\n",
      "lambdac step =  10.125890254974365 s\n",
      "loss_c =  34405.6640625\n",
      "loss_c =  34158.3984375\n",
      "loss_c =  34141.828125\n",
      "loss_c =  34122.1875\n",
      "loss_c =  34098.4296875\n",
      "loss_c =  34070.46875\n",
      "loss_c =  34037.5\n",
      "loss_c =  33997.40625\n",
      "loss_c =  33945.0546875\n",
      "loss_c =  33865.21875\n",
      "loss_c =  33726.765625\n",
      "loss_c =  33641.51171875\n",
      "loss_c =  33585.859375\n",
      "loss_c =  33538.06640625\n",
      "loss_c =  33493.796875\n",
      "loss_c =  33451.3203125\n",
      "loss_c =  33409.71875\n",
      "loss_c =  33368.44140625\n",
      "loss_c =  33327.15234375\n",
      "loss_c =  33285.59375\n",
      "loss_c =  33243.5859375\n",
      "loss_c =  33200.9921875\n",
      "loss_c =  33157.6953125\n",
      "loss_c =  33113.6015625\n",
      "loss_c =  33068.62890625\n",
      "loss_c =  33022.69921875\n",
      "loss_c =  32975.75\n",
      "loss_c =  32927.73046875\n",
      "loss_c =  32878.57421875\n",
      "loss_c =  32828.2421875\n",
      "loss_c =  32776.671875\n",
      "loss_c =  32723.82421875\n",
      "loss_c =  32669.66015625\n",
      "loss_c =  32614.12890625\n",
      "loss_c =  32557.197265625\n",
      "loss_c =  32498.818359375\n",
      "loss_c =  32438.95703125\n",
      "loss_c =  32377.572265625\n",
      "loss_c =  32314.626953125\n",
      "loss_c =  32250.08203125\n",
      "loss_c =  32183.904296875\n",
      "loss_c =  32116.05078125\n",
      "loss_c =  32046.484375\n",
      "loss_c =  31975.171875\n",
      "loss_c =  31902.068359375\n",
      "loss_c =  31827.146484375\n",
      "loss_c =  31750.361328125\n",
      "loss_c =  31671.673828125\n",
      "loss_c =  31591.048828125\n",
      "loss_c =  31508.4453125\n",
      "loss_c =  31423.82421875\n",
      "loss_c =  31337.1484375\n",
      "loss_c =  31248.37109375\n",
      "loss_c =  31157.45703125\n",
      "loss_c =  31064.36328125\n",
      "loss_c =  30969.046875\n",
      "loss_c =  30871.46484375\n",
      "loss_c =  30771.5703125\n",
      "loss_c =  30669.32421875\n",
      "loss_c =  30564.67578125\n",
      "loss_c =  30457.57421875\n",
      "loss_c =  30347.9765625\n",
      "loss_c =  30235.8203125\n",
      "loss_c =  30121.052734375\n",
      "loss_c =  30003.60546875\n",
      "loss_c =  29883.40234375\n",
      "loss_c =  29760.34765625\n",
      "loss_c =  29634.30859375\n",
      "loss_c =  29505.0625\n",
      "loss_c =  29372.169921875\n",
      "loss_c =  29234.1953125\n",
      "loss_c =  29084.44140625\n",
      "loss_c =  28931.48828125\n",
      "loss_c =  28777.603515625\n",
      "loss_c =  28620.27734375\n",
      "loss_c =  28459.26953125\n",
      "loss_c =  28294.572265625\n",
      "loss_c =  28126.19921875\n",
      "loss_c =  27954.16796875\n",
      "loss_c =  27778.474609375\n",
      "loss_c =  27599.12109375\n",
      "loss_c =  27416.09375\n",
      "loss_c =  27229.3828125\n",
      "loss_c =  27038.96875\n",
      "loss_c =  26844.830078125\n",
      "loss_c =  26646.9453125\n",
      "loss_c =  26445.294921875\n",
      "loss_c =  26239.853515625\n",
      "loss_c =  26030.6015625\n",
      "loss_c =  25817.51953125\n",
      "loss_c =  25600.5859375\n",
      "loss_c =  25379.787109375\n",
      "loss_c =  25155.10546875\n",
      "loss_c =  24926.53515625\n",
      "loss_c =  24694.05859375\n",
      "loss_c =  24457.677734375\n",
      "loss_c =  24217.388671875\n",
      "loss_c =  23973.19140625\n",
      "loss_c =  23725.09765625\n",
      "loss_c =  23473.11328125\n",
      "loss_c =  23217.25390625\n",
      "loss_c =  22957.546875\n",
      "loss_c =  22694.01171875\n",
      "loss_c =  22426.681640625\n",
      "loss_c =  22155.59765625\n",
      "loss_c =  21880.796875\n",
      "loss_c =  21602.3359375\n",
      "loss_c =  21320.26953125\n",
      "loss_c =  21034.65625\n",
      "loss_c =  20745.56640625\n",
      "loss_c =  20453.080078125\n",
      "loss_c =  20157.279296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_c =  19858.251953125\n",
      "loss_c =  19556.1015625\n",
      "loss_c =  19250.921875\n",
      "loss_c =  18942.8359375\n",
      "loss_c =  18631.95703125\n",
      "loss_c =  18318.4140625\n",
      "loss_c =  18002.341796875\n",
      "loss_c =  17683.876953125\n",
      "loss_c =  17363.169921875\n",
      "loss_c =  17040.376953125\n",
      "loss_c =  16715.65625\n",
      "loss_c =  16389.18359375\n",
      "loss_c =  16061.1259765625\n",
      "loss_c =  15731.669921875\n",
      "loss_c =  15401.0029296875\n",
      "loss_c =  15069.318359375\n",
      "loss_c =  14736.818359375\n",
      "loss_c =  14403.705078125\n",
      "loss_c =  14070.1884765625\n",
      "loss_c =  13736.482421875\n",
      "loss_c =  13402.8046875\n",
      "loss_c =  13069.3779296875\n",
      "loss_c =  12736.421875\n",
      "loss_c =  12404.1611328125\n",
      "loss_c =  12072.8125\n",
      "loss_c =  11742.59375\n",
      "loss_c =  11413.697265625\n",
      "loss_c =  11086.2861328125\n",
      "loss_c =  10760.421875\n",
      "loss_c =  10435.8564453125\n",
      "loss_c =  10110.6826171875\n",
      "loss_c =  9751.181640625\n",
      "loss_c =  9406.8818359375\n",
      "loss_c =  9070.755859375\n",
      "loss_c =  8738.939453125\n",
      "loss_c =  8411.693359375\n",
      "loss_c =  8089.2568359375\n",
      "loss_c =  7771.7646484375\n",
      "loss_c =  7459.13134765625\n",
      "loss_c =  7150.58544921875\n",
      "loss_c =  6841.45556640625\n",
      "loss_c =  6504.72412109375\n",
      "loss_c =  6197.3095703125\n",
      "loss_c =  5899.5908203125\n",
      "loss_c =  5610.044921875\n",
      "loss_c =  5328.7236328125\n",
      "loss_c =  5055.7685546875\n",
      "loss_c =  4791.30419921875\n",
      "loss_c =  4535.42529296875\n",
      "loss_c =  4288.19287109375\n",
      "loss_c =  4049.629638671875\n",
      "loss_c =  3819.71142578125\n",
      "loss_c =  3598.33642578125\n",
      "loss_c =  3385.214599609375\n",
      "loss_c =  3179.306396484375\n",
      "loss_c =  2971.12939453125\n",
      "loss_c =  2761.63037109375\n",
      "loss_c =  2576.419189453125\n",
      "loss_c =  2400.91357421875\n",
      "loss_c =  2234.7939453125\n",
      "loss_c =  2077.85888671875\n",
      "loss_c =  1929.89794921875\n",
      "loss_c =  1790.6844482421875\n",
      "loss_c =  1659.97509765625\n",
      "loss_c =  1537.51025390625\n",
      "loss_c =  1423.01953125\n",
      "loss_c =  1316.2203369140625\n",
      "loss_c =  1216.8218994140625\n",
      "loss_c =  1124.5260009765625\n",
      "loss_c =  1039.028564453125\n",
      "loss_c =  960.022216796875\n",
      "loss_c =  887.1959228515625\n",
      "loss_c =  820.238037109375\n",
      "loss_c =  758.8375244140625\n",
      "loss_c =  702.6836547851562\n",
      "loss_c =  651.47021484375\n",
      "loss_c =  604.8937377929688\n",
      "loss_c =  562.6566162109375\n",
      "loss_c =  524.468017578125\n",
      "loss_c =  490.04400634765625\n",
      "loss_c =  459.1092834472656\n",
      "loss_c =  431.39776611328125\n",
      "loss_c =  406.6533508300781\n",
      "loss_c =  384.631103515625\n",
      "loss_c =  365.0965881347656\n",
      "loss_c =  347.82733154296875\n",
      "loss_c =  332.61322021484375\n",
      "loss_c =  319.25592041015625\n",
      "thetac step =  19.51884126663208 s\n",
      "step =  1\n",
      "loss z =  14818029.0\n",
      "loss z =  14782635.0\n",
      "loss z =  14753690.0\n",
      "loss z =  14729354.0\n",
      "loss z =  14708076.0\n",
      "loss z =  14689359.0\n",
      "loss z =  14672798.0\n",
      "loss z =  14657958.0\n",
      "loss z =  14644400.0\n",
      "z step =  5.317031383514404 s\n",
      "loss_f =  11917521.0\n",
      "loss_f =  10831584.0\n",
      "thetaf step =  7.057702541351318 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  331081678848.0\n",
      "Epoch  2632: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  2648: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  2664: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  2680: reducing learning rate of group 0 to 3.0000e-07.\n",
      "sample ==  1\n",
      "loss_lambda_c =  187712847872.0\n",
      "Epoch  1847: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  1863: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  1879: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  1895: reducing learning rate of group 0 to 3.0000e-07.\n",
      "sample ==  2\n",
      "loss_lambda_c =  1240130125824.0\n",
      "Epoch  8675: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  8691: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  8707: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  8723: reducing learning rate of group 0 to 3.0000e-07.\n",
      "sample ==  3\n",
      "loss_lambda_c =  1643709726720.0\n",
      "Epoch  3715: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  3731: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  3747: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  3763: reducing learning rate of group 0 to 3.0000e-07.\n",
      "sample ==  4\n",
      "loss_lambda_c =  889817530368.0\n",
      "Epoch  4777: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  4793: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  4809: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  4825: reducing learning rate of group 0 to 3.0000e-07.\n",
      "sample ==  5\n",
      "loss_lambda_c =  1515493261312.0\n",
      "Epoch  1915: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  1931: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  1947: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  1963: reducing learning rate of group 0 to 3.0000e-07.\n",
      "sample ==  6\n",
      "loss_lambda_c =  733766877184.0\n",
      "Epoch  2802: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  2818: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  2834: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  2850: reducing learning rate of group 0 to 3.0000e-07.\n",
      "sample ==  7\n",
      "loss_lambda_c =  460863668224.0\n",
      "Epoch 12121: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 12137: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch 12153: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch 12169: reducing learning rate of group 0 to 3.0000e-07.\n",
      "sample ==  8\n",
      "loss_lambda_c =  78119346176.0\n",
      "Epoch  3236: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  3252: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  3268: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  3284: reducing learning rate of group 0 to 3.0000e-07.\n",
      "sample ==  9\n",
      "loss_lambda_c =  273071194112.0\n",
      "Epoch  1749: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  1765: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  1781: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  1797: reducing learning rate of group 0 to 3.0000e-07.\n",
      "sample ==  10\n",
      "loss_lambda_c =  664017043456.0\n",
      "Epoch  3285: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  3301: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  3317: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  3333: reducing learning rate of group 0 to 3.0000e-07.\n",
      "sample ==  11\n",
      "loss_lambda_c =  312404017152.0\n",
      "Epoch  8269: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  8285: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  8301: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  8317: reducing learning rate of group 0 to 3.0000e-07.\n",
      "sample ==  12\n",
      "loss_lambda_c =  261673107456.0\n",
      "Epoch  3505: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  3521: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  3537: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  3553: reducing learning rate of group 0 to 3.0000e-07.\n",
      "sample ==  13\n",
      "loss_lambda_c =  257753694208.0\n",
      "Epoch  4706: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  4722: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  4738: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  4754: reducing learning rate of group 0 to 3.0000e-07.\n",
      "sample ==  14\n",
      "loss_lambda_c =  507610365952.0\n",
      "Epoch  1830: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  1846: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  1862: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  1878: reducing learning rate of group 0 to 3.0000e-07.\n",
      "sample ==  15\n",
      "loss_lambda_c =  54344085504.0\n",
      "Epoch  1433: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch  1449: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  1465: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  1481: reducing learning rate of group 0 to 3.0000e-07.\n",
      "lambdac step =  1.1809334754943848 s\n",
      "loss_c =  264.64764404296875\n",
      "loss_c =  249.28244018554688\n",
      "loss_c =  244.59793090820312\n",
      "loss_c =  240.6053466796875\n",
      "loss_c =  237.18153381347656\n",
      "loss_c =  234.25848388671875\n",
      "loss_c =  231.77426147460938\n",
      "loss_c =  229.67303466796875\n",
      "loss_c =  227.90432739257812\n",
      "loss_c =  226.42288208007812\n",
      "loss_c =  225.1883544921875\n",
      "loss_c =  224.1649932861328\n",
      "loss_c =  223.32115173339844\n",
      "loss_c =  222.629150390625\n",
      "loss_c =  222.06488037109375\n",
      "loss_c =  221.60739135742188\n",
      "loss_c =  221.23870849609375\n",
      "loss_c =  220.94337463378906\n",
      "loss_c =  220.70822143554688\n",
      "loss_c =  220.52224731445312\n",
      "loss_c =  220.3760986328125\n",
      "loss_c =  220.26199340820312\n",
      "loss_c =  220.17352294921875\n",
      "loss_c =  220.10543823242188\n",
      "loss_c =  220.05340576171875\n",
      "loss_c =  220.013916015625\n",
      "loss_c =  219.9841766357422\n",
      "loss_c =  219.9619903564453\n",
      "loss_c =  219.94554138183594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_c =  219.93344116210938\n",
      "loss_c =  219.9246368408203\n",
      "loss_c =  219.91827392578125\n",
      "loss_c =  219.91372680664062\n",
      "loss_c =  219.9104461669922\n",
      "loss_c =  219.90818786621094\n",
      "loss_c =  219.9066162109375\n",
      "loss_c =  219.905517578125\n",
      "loss_c =  219.90475463867188\n",
      "loss_c =  219.90426635742188\n",
      "loss_c =  219.90390014648438\n",
      "loss_c =  219.9036865234375\n",
      "loss_c =  219.903564453125\n",
      "loss_c =  219.90345764160156\n",
      "loss_c =  219.90338134765625\n",
      "loss_c =  219.90335083007812\n",
      "loss_c =  219.90335083007812\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.90328979492188\n",
      "loss_c =  219.9033203125\n",
      "Epoch 12501: reducing learning rate of group 0 to 1.4000e-04.\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90328979492188\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.90328979492188\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90328979492188\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90328979492188\n",
      "loss_c =  219.90328979492188\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.90328979492188\n",
      "loss_c =  219.90328979492188\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90328979492188\n",
      "loss_c =  219.90328979492188\n",
      "Epoch 13502: reducing learning rate of group 0 to 2.8000e-05.\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.90328979492188\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90328979492188\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.90328979492188\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90328979492188\n",
      "loss_c =  219.9033203125\n",
      "Epoch 14503: reducing learning rate of group 0 to 5.6000e-06.\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.90328979492188\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90328979492188\n",
      "loss_c =  219.90328979492188\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90330505371094\n",
      "Epoch 15504: reducing learning rate of group 0 to 1.1200e-06.\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.90328979492188\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90328979492188\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90328979492188\n",
      "loss_c =  219.9033203125\n",
      "Epoch 16505: reducing learning rate of group 0 to 2.2400e-07.\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.90328979492188\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.90328979492188\n",
      "loss_c =  219.90328979492188\n",
      "loss_c =  219.90328979492188\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.90330505371094\n",
      "loss_c =  219.9033203125\n",
      "loss_c =  219.9033203125\n",
      "Epoch 17506: reducing learning rate of group 0 to 4.4800e-08.\n",
      "thetac step =  18.173798084259033 s\n",
      "step =  2\n",
      "loss z =  13087106.0\n",
      "Epoch    21: reducing learning rate of group 0 to 2.0000e-04.\n",
      "loss z =  13076047.0\n",
      "loss z =  13074376.0\n",
      "loss z =  13072726.0\n",
      "loss z =  13071125.0\n",
      "loss z =  13069590.0\n",
      "loss z =  13068046.0\n",
      "loss z =  13066575.0\n",
      "loss z =  13065134.0\n",
      "z step =  5.564908027648926 s\n",
      "loss_f =  10371920.0\n",
      "loss_f =  10024081.0\n",
      "thetaf step =  7.13580584526062 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  331077910528.0\n",
      "Epoch  2696: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  2712: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2728: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2744: reducing learning rate of group 0 to 1.0000e-07.\n",
      "sample ==  1\n",
      "loss_lambda_c =  187710865408.0\n",
      "Epoch  1911: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  1927: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  1943: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  1959: reducing learning rate of group 0 to 1.0000e-07.\n",
      "sample ==  2\n",
      "loss_lambda_c =  1240089231360.0\n",
      "Epoch  8739: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  8755: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8771: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8787: reducing learning rate of group 0 to 1.0000e-07.\n",
      "sample ==  3\n",
      "loss_lambda_c =  1643663327232.0\n",
      "Epoch  3779: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  3795: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3811: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3827: reducing learning rate of group 0 to 1.0000e-07.\n",
      "sample ==  4\n",
      "loss_lambda_c =  889790988288.0\n",
      "Epoch  4841: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  4857: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4873: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4889: reducing learning rate of group 0 to 1.0000e-07.\n",
      "sample ==  5\n",
      "loss_lambda_c =  1515477401600.0\n",
      "Epoch  1979: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  1995: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2011: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2027: reducing learning rate of group 0 to 1.0000e-07.\n",
      "sample ==  6\n",
      "loss_lambda_c =  733742366720.0\n",
      "Epoch  2866: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  2882: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2898: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2914: reducing learning rate of group 0 to 1.0000e-07.\n",
      "sample ==  7\n",
      "loss_lambda_c =  460848693248.0\n",
      "Epoch 12185: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 12201: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12217: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12233: reducing learning rate of group 0 to 1.0000e-07.\n",
      "sample ==  8\n",
      "loss_lambda_c =  78118445056.0\n",
      "Epoch  3300: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  3316: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3332: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3348: reducing learning rate of group 0 to 1.0000e-07.\n",
      "sample ==  9\n",
      "loss_lambda_c =  273068326912.0\n",
      "Epoch  1813: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  1829: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  1845: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  1861: reducing learning rate of group 0 to 1.0000e-07.\n",
      "sample ==  10\n",
      "loss_lambda_c =  663995547648.0\n",
      "Epoch  3349: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  3365: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3381: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3397: reducing learning rate of group 0 to 1.0000e-07.\n",
      "sample ==  11\n",
      "loss_lambda_c =  312393859072.0\n",
      "Epoch  8333: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  8349: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8365: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8381: reducing learning rate of group 0 to 1.0000e-07.\n",
      "sample ==  12\n",
      "loss_lambda_c =  261670109184.0\n",
      "Epoch  3569: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  3585: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3601: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3617: reducing learning rate of group 0 to 1.0000e-07.\n",
      "sample ==  13\n",
      "loss_lambda_c =  257745780736.0\n",
      "Epoch  4770: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  4786: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4802: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4818: reducing learning rate of group 0 to 1.0000e-07.\n",
      "sample ==  14\n",
      "loss_lambda_c =  507604893696.0\n",
      "Epoch  1894: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  1910: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  1926: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  1942: reducing learning rate of group 0 to 1.0000e-07.\n",
      "sample ==  15\n",
      "loss_lambda_c =  54343503872.0\n",
      "Epoch  1497: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  1513: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  1529: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  1545: reducing learning rate of group 0 to 1.0000e-07.\n",
      "lambdac step =  1.1960394382476807 s\n",
      "loss_c =  219.94992065429688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.94985961914062\n",
      "loss_c =  219.94985961914062\n",
      "loss_c =  219.94985961914062\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "Epoch 18507: reducing learning rate of group 0 to 1.4000e-04.\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "Epoch 19508: reducing learning rate of group 0 to 2.8000e-05.\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.94985961914062\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "Epoch 20509: reducing learning rate of group 0 to 5.6000e-06.\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.94985961914062\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "Epoch 21510: reducing learning rate of group 0 to 1.1200e-06.\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.94985961914062\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.9498748779297\n",
      "Epoch 22511: reducing learning rate of group 0 to 2.2400e-07.\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.94985961914062\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.94989013671875\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.9498748779297\n",
      "loss_c =  219.94989013671875\n",
      "Epoch 23512: reducing learning rate of group 0 to 4.4800e-08.\n",
      "thetac step =  17.492064237594604 s\n",
      "step =  3\n",
      "loss z =  12353938.0\n",
      "loss z =  12352694.0\n",
      "Epoch    32: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  12351486.0\n",
      "loss z =  12351252.0\n",
      "loss z =  12351017.0\n",
      "loss z =  12350786.0\n",
      "loss z =  12350552.0\n",
      "loss z =  12350325.0\n",
      "loss z =  12350094.0\n",
      "z step =  5.483656644821167 s\n",
      "loss_f =  9660918.0\n",
      "loss_f =  9480657.0\n",
      "thetaf step =  6.978035926818848 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  331076665344.0\n",
      "Epoch  2760: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  2776: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  2792: reducing learning rate of group 0 to 3.0000e-07.\n",
      "Epoch  2808: reducing learning rate of group 0 to 3.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  187710210048.0\n",
      "Epoch  1975: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  1991: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  2007: reducing learning rate of group 0 to 3.0000e-07.\n",
      "Epoch  2023: reducing learning rate of group 0 to 3.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  1240075337728.0\n",
      "Epoch  8803: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  8819: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  8835: reducing learning rate of group 0 to 3.0000e-07.\n",
      "Epoch  8851: reducing learning rate of group 0 to 3.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  1643647860736.0\n",
      "Epoch  3843: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  3859: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  3875: reducing learning rate of group 0 to 3.0000e-07.\n",
      "Epoch  3891: reducing learning rate of group 0 to 3.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  889782009856.0\n",
      "Epoch  4905: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  4921: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  4937: reducing learning rate of group 0 to 3.0000e-07.\n",
      "Epoch  4953: reducing learning rate of group 0 to 3.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  1515471896576.0\n",
      "Epoch  2043: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  2059: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  2075: reducing learning rate of group 0 to 3.0000e-07.\n",
      "Epoch  2091: reducing learning rate of group 0 to 3.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  733733978112.0\n",
      "Epoch  2930: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  2946: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  2962: reducing learning rate of group 0 to 3.0000e-07.\n",
      "Epoch  2978: reducing learning rate of group 0 to 3.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  460843646976.0\n",
      "Epoch 12249: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch 12265: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch 12281: reducing learning rate of group 0 to 3.0000e-07.\n",
      "Epoch 12297: reducing learning rate of group 0 to 3.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  78118150144.0\n",
      "Epoch  3364: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  3380: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  3396: reducing learning rate of group 0 to 3.0000e-07.\n",
      "Epoch  3412: reducing learning rate of group 0 to 3.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  273067311104.0\n",
      "Epoch  1877: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  1893: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  1909: reducing learning rate of group 0 to 3.0000e-07.\n",
      "Epoch  1925: reducing learning rate of group 0 to 3.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  663988273152.0\n",
      "Epoch  3413: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  3429: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  3445: reducing learning rate of group 0 to 3.0000e-07.\n",
      "Epoch  3461: reducing learning rate of group 0 to 3.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  312390385664.0\n",
      "Epoch  8397: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  8413: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  8429: reducing learning rate of group 0 to 3.0000e-07.\n",
      "Epoch  8445: reducing learning rate of group 0 to 3.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  261669093376.0\n",
      "Epoch  3633: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  3649: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  3665: reducing learning rate of group 0 to 3.0000e-07.\n",
      "Epoch  3681: reducing learning rate of group 0 to 3.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  257743126528.0\n",
      "Epoch  4834: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  4850: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  4866: reducing learning rate of group 0 to 3.0000e-07.\n",
      "Epoch  4882: reducing learning rate of group 0 to 3.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  507603124224.0\n",
      "Epoch  1958: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  1974: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  1990: reducing learning rate of group 0 to 3.0000e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2006: reducing learning rate of group 0 to 3.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  54343307264.0\n",
      "Epoch  1561: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch  1577: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch  1593: reducing learning rate of group 0 to 3.0000e-07.\n",
      "Epoch  1609: reducing learning rate of group 0 to 3.0000e-08.\n",
      "lambdac step =  1.1580958366394043 s\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.964111328125\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.9640655517578\n",
      "Epoch 24513: reducing learning rate of group 0 to 1.0000e-04.\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96409606933594\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96409606933594\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.9640655517578\n",
      "Epoch 25514: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.9640655517578\n",
      "Epoch 26515: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "Epoch 27516: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "Epoch 28517: reducing learning rate of group 0 to 1.6000e-07.\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.96408081054688\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.9640655517578\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "loss_c =  219.96405029296875\n",
      "Epoch 29518: reducing learning rate of group 0 to 3.2000e-08.\n",
      "thetac step =  19.938409566879272 s\n",
      "step =  4\n",
      "loss z =  12152479.0\n",
      "loss z =  12151688.0\n",
      "loss z =  12150902.0\n",
      "Epoch    43: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  12150132.0\n",
      "loss z =  12149980.0\n",
      "loss z =  12149828.0\n",
      "loss z =  12149680.0\n",
      "loss z =  12149527.0\n",
      "loss z =  12149374.0\n",
      "z step =  5.968760251998901 s\n",
      "loss_f =  9462972.0\n",
      "loss_f =  9487792.0\n",
      "thetaf step =  7.703076601028442 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  331076239360.0\n",
      "Epoch  2824: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2840: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2856: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2872: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  187710013440.0\n",
      "Epoch  2039: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2055: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2071: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2087: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  1240071012352.0\n",
      "Epoch  8867: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8883: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8899: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8915: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  1643643273216.0\n",
      "Epoch  3907: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3923: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3939: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3955: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  889779257344.0\n",
      "Epoch  4969: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4985: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5001: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5017: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  1515470323712.0\n",
      "Epoch  2107: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2123: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2139: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2155: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  733731487744.0\n",
      "Epoch  2994: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3010: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3026: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3042: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  460842074112.0\n",
      "Epoch 12313: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12329: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12345: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12361: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  78118035456.0\n",
      "Epoch  3428: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3444: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3460: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3476: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  273067081728.0\n",
      "Epoch  1941: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  1957: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  1973: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  1989: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  663986044928.0\n",
      "Epoch  3477: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3493: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3509: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3525: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  312389304320.0\n",
      "Epoch  8461: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8477: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8493: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8509: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  261668732928.0\n",
      "Epoch  3697: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3713: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3729: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3745: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_lambda_c =  257742323712.0\n",
      "Epoch  4898: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4914: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4930: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4946: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  507602567168.0\n",
      "Epoch  2022: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2038: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2054: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2070: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  54343245824.0\n",
      "Epoch  1625: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  1641: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  1657: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  1673: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.297306776046753 s\n",
      "loss_c =  219.9687042236328\n",
      "loss_c =  219.9686279296875\n",
      "loss_c =  219.96810913085938\n",
      "loss_c =  215.35873413085938\n",
      "loss_c =  213.194580078125\n",
      "loss_c =  211.92474365234375\n",
      "loss_c =  211.03555297851562\n",
      "loss_c =  210.36141967773438\n",
      "loss_c =  209.82606506347656\n",
      "loss_c =  209.38702392578125\n",
      "loss_c =  209.01724243164062\n",
      "loss_c =  208.69692993164062\n",
      "loss_c =  208.4078369140625\n",
      "loss_c =  208.12489318847656\n",
      "loss_c =  207.7931365966797\n",
      "loss_c =  207.3238067626953\n",
      "loss_c =  206.8690185546875\n",
      "loss_c =  206.43653869628906\n",
      "loss_c =  206.01727294921875\n",
      "loss_c =  205.595947265625\n",
      "loss_c =  205.15313720703125\n",
      "loss_c =  204.6664276123047\n",
      "loss_c =  204.10321044921875\n",
      "loss_c =  203.4025115966797\n",
      "loss_c =  202.434326171875\n",
      "loss_c =  201.01258850097656\n",
      "loss_c =  199.16708374023438\n",
      "loss_c =  197.10260009765625\n",
      "loss_c =  194.94447326660156\n",
      "loss_c =  192.75030517578125\n",
      "loss_c =  190.5541229248047\n",
      "loss_c =  188.38211059570312\n",
      "loss_c =  186.25497436523438\n",
      "loss_c =  184.1883544921875\n",
      "loss_c =  182.19361877441406\n",
      "loss_c =  180.27833557128906\n",
      "loss_c =  178.4471893310547\n",
      "loss_c =  176.70257568359375\n",
      "loss_c =  175.0452880859375\n",
      "loss_c =  173.4746551513672\n",
      "loss_c =  171.9892120361328\n",
      "loss_c =  170.58663940429688\n",
      "loss_c =  169.26437377929688\n",
      "loss_c =  168.01934814453125\n",
      "loss_c =  166.84841918945312\n",
      "loss_c =  165.748291015625\n",
      "loss_c =  165.52728271484375\n",
      "loss_c =  164.51678466796875\n",
      "loss_c =  163.9395751953125\n",
      "loss_c =  163.40260314941406\n",
      "loss_c =  162.89767456054688\n",
      "loss_c =  162.4204864501953\n",
      "loss_c =  161.9677734375\n",
      "loss_c =  161.5369873046875\n",
      "loss_c =  161.12612915039062\n",
      "loss_c =  160.73361206054688\n",
      "loss_c =  160.35801696777344\n",
      "loss_c =  159.99819946289062\n",
      "loss_c =  159.653076171875\n",
      "loss_c =  159.32176208496094\n",
      "loss_c =  159.00343322753906\n",
      "loss_c =  158.69729614257812\n",
      "loss_c =  158.40267944335938\n",
      "loss_c =  158.11898803710938\n",
      "loss_c =  157.84568786621094\n",
      "loss_c =  157.58213806152344\n",
      "loss_c =  157.3280029296875\n",
      "loss_c =  157.08273315429688\n",
      "loss_c =  156.845947265625\n",
      "loss_c =  156.61727905273438\n",
      "loss_c =  156.3963623046875\n",
      "loss_c =  156.18289184570312\n",
      "loss_c =  155.9764404296875\n",
      "loss_c =  155.77687072753906\n",
      "loss_c =  155.583740234375\n",
      "loss_c =  155.39674377441406\n",
      "loss_c =  155.2152557373047\n",
      "loss_c =  155.03244018554688\n",
      "loss_c =  155.50958251953125\n",
      "loss_c =  154.85824584960938\n",
      "loss_c =  154.6900634765625\n",
      "loss_c =  154.53936767578125\n",
      "loss_c =  154.39971923828125\n",
      "loss_c =  154.2682647705078\n",
      "loss_c =  154.14332580566406\n",
      "loss_c =  154.02381896972656\n",
      "loss_c =  153.9090118408203\n",
      "loss_c =  153.79837036132812\n",
      "loss_c =  153.691650390625\n",
      "loss_c =  153.58859252929688\n",
      "loss_c =  153.48899841308594\n",
      "loss_c =  153.3926544189453\n",
      "loss_c =  153.2995147705078\n",
      "loss_c =  153.2094268798828\n",
      "loss_c =  153.12225341796875\n",
      "loss_c =  153.03793334960938\n",
      "loss_c =  152.95632934570312\n",
      "loss_c =  152.87741088867188\n",
      "loss_c =  152.80096435546875\n",
      "loss_c =  152.72702026367188\n",
      "loss_c =  152.65542602539062\n",
      "loss_c =  152.5861053466797\n",
      "loss_c =  152.51898193359375\n",
      "loss_c =  152.45401000976562\n",
      "loss_c =  152.3910675048828\n",
      "loss_c =  152.33010864257812\n",
      "loss_c =  152.27105712890625\n",
      "loss_c =  152.21383666992188\n",
      "loss_c =  152.15838623046875\n",
      "loss_c =  152.1046142578125\n",
      "loss_c =  152.05255126953125\n",
      "loss_c =  152.00198364257812\n",
      "loss_c =  151.95294189453125\n",
      "loss_c =  151.9053955078125\n",
      "loss_c =  151.85922241210938\n",
      "loss_c =  151.81439208984375\n",
      "loss_c =  151.7708740234375\n",
      "loss_c =  151.72860717773438\n",
      "loss_c =  151.68751525878906\n",
      "loss_c =  151.6475372314453\n",
      "loss_c =  151.60867309570312\n",
      "loss_c =  151.57090759277344\n",
      "loss_c =  151.53407287597656\n",
      "loss_c =  151.49822998046875\n",
      "loss_c =  151.46331787109375\n",
      "loss_c =  151.42929077148438\n",
      "loss_c =  151.39605712890625\n",
      "loss_c =  151.36370849609375\n",
      "loss_c =  151.33203125\n",
      "loss_c =  151.3011474609375\n",
      "loss_c =  151.27096557617188\n",
      "loss_c =  151.24139404296875\n",
      "loss_c =  151.21249389648438\n",
      "loss_c =  151.18414306640625\n",
      "loss_c =  151.15640258789062\n",
      "loss_c =  151.12921142578125\n",
      "loss_c =  151.1025390625\n",
      "loss_c =  151.07632446289062\n",
      "loss_c =  151.05056762695312\n",
      "loss_c =  151.02523803710938\n",
      "loss_c =  151.00033569335938\n",
      "loss_c =  150.975830078125\n",
      "loss_c =  150.95169067382812\n",
      "loss_c =  150.92787170410156\n",
      "loss_c =  150.9043731689453\n",
      "loss_c =  150.88119506835938\n",
      "loss_c =  150.8583221435547\n",
      "loss_c =  150.83567810058594\n",
      "loss_c =  150.81332397460938\n",
      "loss_c =  150.7911834716797\n",
      "loss_c =  150.76925659179688\n",
      "loss_c =  150.74758911132812\n",
      "loss_c =  150.72607421875\n",
      "loss_c =  150.7047576904297\n",
      "loss_c =  150.68359375\n",
      "loss_c =  150.66259765625\n",
      "loss_c =  150.6417694091797\n",
      "loss_c =  150.62103271484375\n",
      "loss_c =  150.60047912597656\n",
      "loss_c =  150.57998657226562\n",
      "loss_c =  150.55960083007812\n",
      "loss_c =  150.53936767578125\n",
      "loss_c =  150.51919555664062\n",
      "loss_c =  150.49911499023438\n",
      "loss_c =  150.47909545898438\n",
      "loss_c =  150.45913696289062\n",
      "loss_c =  150.43927001953125\n",
      "loss_c =  150.41944885253906\n",
      "loss_c =  150.399658203125\n",
      "loss_c =  150.37994384765625\n",
      "loss_c =  150.36026000976562\n",
      "loss_c =  150.340576171875\n",
      "loss_c =  150.3209228515625\n",
      "loss_c =  150.30133056640625\n",
      "loss_c =  150.28170776367188\n",
      "loss_c =  150.26211547851562\n",
      "loss_c =  150.24252319335938\n",
      "loss_c =  150.22293090820312\n",
      "loss_c =  150.2032928466797\n",
      "loss_c =  150.18368530273438\n",
      "loss_c =  150.16400146484375\n",
      "loss_c =  150.144287109375\n",
      "loss_c =  150.12461853027344\n",
      "loss_c =  150.1048583984375\n",
      "loss_c =  150.08502197265625\n",
      "loss_c =  150.06515502929688\n",
      "loss_c =  150.0452423095703\n",
      "loss_c =  150.0252227783203\n",
      "loss_c =  150.005126953125\n",
      "loss_c =  149.98495483398438\n",
      "loss_c =  149.9647216796875\n",
      "loss_c =  149.9443359375\n",
      "loss_c =  149.9239044189453\n",
      "loss_c =  149.90328979492188\n",
      "loss_c =  149.88259887695312\n",
      "loss_c =  149.86175537109375\n",
      "loss_c =  149.84078979492188\n",
      "loss_c =  149.81967163085938\n",
      "loss_c =  149.79840087890625\n",
      "loss_c =  149.77694702148438\n",
      "thetac step =  36.980499029159546 s\n"
     ]
    }
   ],
   "source": [
    "model.fit(n_steps=5, with_precisions=False, z_iterations=10, thetac_iterations=10000, thetaf_iterations=10, lambdac_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  5\n",
      "loss z =  12102745.0\n",
      "loss z =  12101985.0\n",
      "loss z =  12101238.0\n",
      "loss z =  12100503.0\n",
      "Epoch    54: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  12099777.0\n",
      "loss z =  12099635.0\n",
      "loss z =  12099492.0\n",
      "loss z =  12099349.0\n",
      "loss z =  12099207.0\n",
      "loss z =  12099068.0\n",
      "loss z =  12098927.0\n",
      "loss z =  12098785.0\n",
      "loss z =  12098645.0\n",
      "loss z =  12098506.0\n",
      "loss z =  12098366.0\n",
      "Epoch    65: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  12098228.0\n",
      "loss z =  12098199.0\n",
      "loss z =  12098171.0\n",
      "loss z =  12098142.0\n",
      "loss z =  12098087.0\n",
      "loss z =  12098060.0\n",
      "loss z =  12098031.0\n",
      "loss z =  12098003.0\n",
      "loss z =  12097976.0\n",
      "loss z =  12097948.0\n",
      "Epoch    76: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  12097920.0\n",
      "loss z =  12097916.0\n",
      "loss z =  12097909.0\n",
      "loss z =  12097904.0\n",
      "loss z =  12097898.0\n",
      "loss z =  12097893.0\n",
      "loss z =  12097888.0\n",
      "loss z =  12097881.0\n",
      "loss z =  12097876.0\n",
      "loss z =  12097870.0\n",
      "loss z =  12097864.0\n",
      "Epoch    87: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  22.441741943359375 s\n",
      "loss_f =  9415824.0\n",
      "loss_f =  9408652.0\n",
      "loss_f =  9410279.0\n",
      "loss_f =  9394393.0\n",
      "loss_f =  9385028.0\n",
      "loss_f =  9377900.0\n",
      "loss_f =  9369823.0\n",
      "loss_f =  9364421.0\n",
      "loss_f =  9361124.0\n",
      "loss_f =  9357868.0\n",
      "loss_f =  9354733.0\n",
      "loss_f =  9351932.0\n",
      "loss_f =  9349114.0\n",
      "loss_f =  9346300.0\n",
      "loss_f =  9343571.0\n",
      "loss_f =  9340834.0\n",
      "loss_f =  9338069.0\n",
      "loss_f =  9335304.0\n",
      "loss_f =  9332506.0\n",
      "loss_f =  9329676.0\n",
      "loss_f =  9326818.0\n",
      "loss_f =  9323920.0\n",
      "loss_f =  9320986.0\n",
      "loss_f =  9318018.0\n",
      "loss_f =  9315013.0\n",
      "loss_f =  9311975.0\n",
      "loss_f =  9308893.0\n",
      "loss_f =  9305772.0\n",
      "loss_f =  9302608.0\n",
      "loss_f =  9299399.0\n",
      "loss_f =  9295434.0\n",
      "loss_f =  9292204.0\n",
      "loss_f =  9288050.0\n",
      "loss_f =  9284371.0\n",
      "loss_f =  9280996.0\n",
      "loss_f =  9276946.0\n",
      "loss_f =  9272983.0\n",
      "loss_f =  9268369.0\n",
      "loss_f =  9263982.0\n",
      "loss_f =  9259554.0\n",
      "thetaf step =  140.8268117904663 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  331076141056.0\n",
      "Epoch  2888: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2904: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2920: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2936: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  187709947904.0\n",
      "Epoch  2103: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2119: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2135: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2151: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  1240069439488.0\n",
      "Epoch  8931: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8947: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8963: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8979: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  1643642093568.0\n",
      "Epoch  3971: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3987: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4003: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4019: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  889778470912.0\n",
      "Epoch  5033: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5049: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5065: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5081: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  1515470061568.0\n",
      "Epoch  2171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2187: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2203: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2219: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  733730701312.0\n",
      "Epoch  3058: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3074: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3090: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3106: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  460841549824.0\n",
      "Epoch 12377: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12393: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12409: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12425: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  78118019072.0\n",
      "Epoch  3492: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3508: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3524: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3540: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  273066983424.0\n",
      "Epoch  2005: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2021: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2037: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2053: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  663985324032.0\n",
      "Epoch  3541: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3557: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3573: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3589: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  312388943872.0\n",
      "Epoch  8525: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8541: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8557: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8573: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  261668651008.0\n",
      "Epoch  3761: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3777: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3793: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3809: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  257742045184.0\n",
      "Epoch  4962: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4978: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4994: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5010: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  507602436096.0\n",
      "Epoch  2086: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2102: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2118: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2134: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  54343225344.0\n",
      "Epoch  1689: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  1705: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  1721: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  1737: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.155773639678955 s\n",
      "loss_c =  324.56890869140625\n",
      "loss_c =  308.8802185058594\n",
      "loss_c =  304.50860595703125\n",
      "loss_c =  301.11865234375\n",
      "loss_c =  298.5491638183594\n",
      "loss_c =  296.607177734375\n",
      "loss_c =  295.08416748046875\n",
      "loss_c =  293.81402587890625\n",
      "loss_c =  292.6889953613281\n",
      "loss_c =  291.6475830078125\n",
      "thetac step =  2.0719141960144043 s\n",
      "step =  6\n",
      "loss z =  12002496.0\n",
      "loss z =  11993901.0\n",
      "loss z =  11986245.0\n",
      "loss z =  11979400.0\n",
      "loss z =  11973264.0\n",
      "loss z =  11967731.0\n",
      "loss z =  11962728.0\n",
      "loss z =  11958184.0\n",
      "loss z =  11954039.0\n",
      "loss z =  11950234.0\n",
      "Epoch    98: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  11946716.0\n",
      "loss z =  11946052.0\n",
      "loss z =  11945400.0\n",
      "loss z =  11944753.0\n",
      "loss z =  11944123.0\n",
      "loss z =  11943503.0\n",
      "loss z =  11942886.0\n",
      "loss z =  11942278.0\n",
      "loss z =  11941678.0\n",
      "loss z =  11940504.0\n",
      "Epoch   109: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  11939931.0\n",
      "loss z =  11939815.0\n",
      "loss z =  11939702.0\n",
      "loss z =  11939590.0\n",
      "loss z =  11939476.0\n",
      "loss z =  11939364.0\n",
      "loss z =  11939253.0\n",
      "loss z =  11939140.0\n",
      "loss z =  11939030.0\n",
      "loss z =  11938918.0\n",
      "loss z =  11938809.0\n",
      "Epoch   120: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  11938696.0\n",
      "loss z =  11938676.0\n",
      "loss z =  11938653.0\n",
      "loss z =  11938631.0\n",
      "loss z =  11938609.0\n",
      "loss z =  11938587.0\n",
      "loss z =  11938563.0\n",
      "loss z =  11938519.0\n",
      "loss z =  11938498.0\n",
      "loss z =  11938476.0\n",
      "Epoch   131: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  23.768852949142456 s\n",
      "loss_f =  9262284.0\n",
      "loss_f =  9299760.0\n",
      "loss_f =  9300895.0\n",
      "loss_f =  9235836.0\n",
      "loss_f =  9196811.0\n",
      "loss_f =  9174318.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_f =  9161749.0\n",
      "loss_f =  9149955.0\n",
      "loss_f =  9137338.0\n",
      "loss_f =  9127921.0\n",
      "loss_f =  9119121.0\n",
      "loss_f =  9110574.0\n",
      "loss_f =  9102600.0\n",
      "loss_f =  9094747.0\n",
      "loss_f =  9087185.0\n",
      "loss_f =  9079846.0\n",
      "loss_f =  9072704.0\n",
      "loss_f =  9065800.0\n",
      "loss_f =  9059102.0\n",
      "loss_f =  9052601.0\n",
      "loss_f =  9046259.0\n",
      "loss_f =  9040058.0\n",
      "loss_f =  9033962.0\n",
      "loss_f =  9027976.0\n",
      "loss_f =  9041393.0\n",
      "loss_f =  9016392.0\n",
      "loss_f =  9010658.0\n",
      "loss_f =  9004854.0\n",
      "loss_f =  8999164.0\n",
      "loss_f =  8993521.0\n",
      "loss_f =  8987916.0\n",
      "loss_f =  8982344.0\n",
      "loss_f =  8976816.0\n",
      "loss_f =  8971324.0\n",
      "loss_f =  8965867.0\n",
      "loss_f =  8960444.0\n",
      "loss_f =  8955057.0\n",
      "loss_f =  8949705.0\n",
      "loss_f =  8944394.0\n",
      "loss_f =  8939115.0\n",
      "thetaf step =  139.2079062461853 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11067.1513671875\n",
      "Epoch  2953: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2969: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2985: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3001: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6421.5087890625\n",
      "Epoch  2168: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2184: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2200: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2216: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30088.83203125\n",
      "Epoch  8996: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9012: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9028: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9044: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.375\n",
      "Epoch  4036: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4052: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4068: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4084: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24290.91796875\n",
      "Epoch  5098: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5114: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5130: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5146: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37792.625\n",
      "Epoch  2236: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2252: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2268: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2284: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23236.720703125\n",
      "Epoch  3123: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3139: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3155: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3171: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14112.12109375\n",
      "Epoch 12442: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12458: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12474: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12490: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2426.617919921875\n",
      "Epoch  3557: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3573: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3589: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3605: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8142.53125\n",
      "Epoch  2070: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2086: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2102: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2118: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19091.9921875\n",
      "Epoch  3606: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3622: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3638: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3654: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10363.1044921875\n",
      "Epoch  8590: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8606: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8622: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8638: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9314.8134765625\n",
      "Epoch  3826: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3842: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3858: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3874: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8886.6748046875\n",
      "Epoch  5027: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5043: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5059: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5075: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15623.2568359375\n",
      "Epoch  2151: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2167: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2183: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2199: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1817.3194580078125\n",
      "Epoch  1754: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  1770: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  1786: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  1802: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1559481620788574 s\n",
      "loss_c =  262.53204345703125\n",
      "loss_c =  258.0552062988281\n",
      "loss_c =  257.2852478027344\n",
      "loss_c =  256.6554260253906\n",
      "loss_c =  256.1064453125\n",
      "loss_c =  255.6114501953125\n",
      "loss_c =  255.15374755859375\n",
      "loss_c =  254.7228240966797\n",
      "loss_c =  254.31166076660156\n",
      "loss_c =  253.91470336914062\n",
      "Epoch 40514: reducing learning rate of group 0 to 2.0000e-05.\n",
      "thetac step =  2.145897150039673 s\n",
      "step =  7\n",
      "loss z =  12142199.0\n",
      "loss z =  11925054.0\n",
      "loss z =  11825940.0\n",
      "loss z =  11777455.0\n",
      "loss z =  11745868.0\n",
      "loss z =  11721730.0\n",
      "loss z =  11702831.0\n",
      "loss z =  11687960.0\n",
      "loss z =  11676046.0\n",
      "loss z =  11666181.0\n",
      "loss z =  11657680.0\n",
      "Epoch   143: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  11650130.0\n",
      "loss z =  11648730.0\n",
      "loss z =  11647367.0\n",
      "loss z =  11646019.0\n",
      "loss z =  11644697.0\n",
      "loss z =  11643400.0\n",
      "loss z =  11642123.0\n",
      "loss z =  11640866.0\n",
      "loss z =  11638418.0\n",
      "loss z =  11637226.0\n",
      "Epoch   154: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  11636053.0\n",
      "loss z =  11635819.0\n",
      "loss z =  11635591.0\n",
      "loss z =  11635363.0\n",
      "loss z =  11635132.0\n",
      "loss z =  11634904.0\n",
      "loss z =  11634675.0\n",
      "loss z =  11634448.0\n",
      "loss z =  11634218.0\n",
      "loss z =  11633996.0\n",
      "loss z =  11633768.0\n",
      "Epoch   165: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  11633546.0\n",
      "loss z =  11633500.0\n",
      "loss z =  11633456.0\n",
      "loss z =  11633410.0\n",
      "loss z =  11633367.0\n",
      "loss z =  11633320.0\n",
      "loss z =  11633232.0\n",
      "loss z =  11633188.0\n",
      "loss z =  11633142.0\n",
      "loss z =  11633096.0\n",
      "Epoch   176: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.624147415161133 s\n",
      "loss_f =  8961305.0\n",
      "loss_f =  9033498.0\n",
      "loss_f =  9043473.0\n",
      "loss_f =  8905988.0\n",
      "loss_f =  8833847.0\n",
      "loss_f =  8817679.0\n",
      "loss_f =  8801994.0\n",
      "loss_f =  8783642.0\n",
      "loss_f =  8770821.0\n",
      "loss_f =  8761644.0\n",
      "loss_f =  8752357.0\n",
      "loss_f =  8744181.0\n",
      "loss_f =  8736646.0\n",
      "loss_f =  8728324.0\n",
      "loss_f =  8720578.0\n",
      "loss_f =  8714442.0\n",
      "loss_f =  8712566.0\n",
      "loss_f =  8704568.0\n",
      "loss_f =  8698860.0\n",
      "loss_f =  8693033.0\n",
      "loss_f =  8687166.0\n",
      "loss_f =  8681350.0\n",
      "loss_f =  8675607.0\n",
      "loss_f =  8669934.0\n",
      "loss_f =  8664229.0\n",
      "loss_f =  8658846.0\n",
      "loss_f =  8653880.0\n",
      "loss_f =  8648628.0\n",
      "loss_f =  8643548.0\n",
      "loss_f =  8638343.0\n",
      "loss_f =  8632921.0\n",
      "loss_f =  8627610.0\n",
      "loss_f =  8622369.0\n",
      "loss_f =  8618547.0\n",
      "loss_f =  8614449.0\n",
      "loss_f =  8609340.0\n",
      "loss_f =  8603760.0\n",
      "loss_f =  8598203.0\n",
      "loss_f =  8592784.0\n",
      "loss_f =  8587846.0\n",
      "thetaf step =  140.2554008960724 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11068.5478515625\n",
      "Epoch  3017: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3033: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3049: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3065: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6419.46533203125\n",
      "Epoch  2233: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2249: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2265: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2281: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30089.009765625\n",
      "Epoch  9060: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9076: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9092: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9108: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.3984375\n",
      "Epoch  4100: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4116: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4132: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4148: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24291.0078125\n",
      "Epoch  5162: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5178: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5194: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5210: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37792.8046875\n",
      "Epoch  2300: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2316: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2332: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2348: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23237.1640625\n",
      "Epoch  3187: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3203: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3219: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3235: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14113.2314453125\n",
      "Epoch 12506: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12522: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12538: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12554: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2425.589111328125\n",
      "Epoch  3622: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3638: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3654: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3670: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8172.73876953125\n",
      "Epoch  2134: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2150: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2166: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2182: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19092.568359375\n",
      "Epoch  3670: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3686: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3702: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3718: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10363.36328125\n",
      "Epoch  8654: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8670: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8686: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8702: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9316.693359375\n",
      "Epoch  3890: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3906: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3922: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3938: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8884.556640625\n",
      "Epoch  5092: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5108: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5124: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5140: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15624.0283203125\n",
      "Epoch  2215: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2231: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2247: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2263: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.452392578125\n",
      "Epoch  1819: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  1835: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  1851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  1867: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1967909336090088 s\n",
      "loss_c =  290.57867431640625\n",
      "loss_c =  287.76220703125\n",
      "loss_c =  286.953125\n",
      "loss_c =  286.2200622558594\n",
      "loss_c =  285.5302734375\n",
      "loss_c =  284.855224609375\n",
      "loss_c =  284.15728759765625\n",
      "loss_c =  283.3986511230469\n",
      "loss_c =  282.58062744140625\n",
      "loss_c =  281.7525329589844\n",
      "thetac step =  2.3717105388641357 s\n",
      "step =  8\n",
      "loss z =  12401038.0\n",
      "loss z =  12168404.0\n",
      "loss z =  12045281.0\n",
      "loss z =  11975629.0\n",
      "loss z =  11932376.0\n",
      "loss z =  11900309.0\n",
      "loss z =  11872089.0\n",
      "loss z =  11845504.0\n",
      "loss z =  11820222.0\n",
      "loss z =  11796261.0\n",
      "loss z =  11773494.0\n",
      "Epoch   188: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  11751792.0\n",
      "loss z =  11747485.0\n",
      "loss z =  11743501.0\n",
      "loss z =  11739278.0\n",
      "loss z =  11735094.0\n",
      "loss z =  11730937.0\n",
      "loss z =  11726680.0\n",
      "loss z =  11722524.0\n",
      "loss z =  11714477.0\n",
      "loss z =  11710354.0\n",
      "Epoch   199: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  11706147.0\n",
      "loss z =  11705368.0\n",
      "loss z =  11704531.0\n",
      "loss z =  11703692.0\n",
      "loss z =  11702748.0\n",
      "loss z =  11701934.0\n",
      "loss z =  11701101.0\n",
      "loss z =  11700200.0\n",
      "loss z =  11699367.0\n",
      "loss z =  11698576.0\n",
      "loss z =  11697701.0\n",
      "Epoch   210: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  11696961.0\n",
      "loss z =  11696828.0\n",
      "loss z =  11696677.0\n",
      "loss z =  11696481.0\n",
      "loss z =  11696330.0\n",
      "loss z =  11696155.0\n",
      "loss z =  11695834.0\n",
      "loss z =  11695705.0\n",
      "loss z =  11695551.0\n",
      "loss z =  11695400.0\n",
      "Epoch   221: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.946878671646118 s\n",
      "loss_f =  9026562.0\n",
      "loss_f =  9101460.0\n",
      "loss_f =  9104290.0\n",
      "loss_f =  8859940.0\n",
      "loss_f =  8735736.0\n",
      "loss_f =  8695136.0\n",
      "loss_f =  8653884.0\n",
      "Epoch   680: reducing learning rate of group 0 to 1.5000e-03.\n",
      "loss_f =  8628532.0\n",
      "loss_f =  8605606.0\n",
      "loss_f =  8588116.0\n",
      "loss_f =  8575933.0\n",
      "loss_f =  8565935.0\n",
      "loss_f =  8556719.0\n",
      "loss_f =  8548481.0\n",
      "loss_f =  8541264.0\n",
      "loss_f =  8534751.0\n",
      "loss_f =  8528728.0\n",
      "loss_f =  8523158.0\n",
      "loss_f =  8517962.0\n",
      "loss_f =  8513079.0\n",
      "loss_f =  8508457.0\n",
      "loss_f =  8504057.0\n",
      "loss_f =  8499807.0\n",
      "loss_f =  8495750.0\n",
      "loss_f =  8491824.0\n",
      "loss_f =  8487969.0\n",
      "loss_f =  8484227.0\n",
      "loss_f =  8480561.0\n",
      "loss_f =  8476967.0\n",
      "loss_f =  8473442.0\n",
      "loss_f =  8472574.0\n",
      "loss_f =  8466687.0\n",
      "loss_f =  8463563.0\n",
      "loss_f =  8459144.0\n",
      "loss_f =  8456523.0\n",
      "loss_f =  8452762.0\n",
      "loss_f =  8448995.0\n",
      "loss_f =  8445911.0\n",
      "loss_f =  8442413.0\n",
      "loss_f =  8439220.0\n",
      "thetaf step =  143.84172129631042 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11069.1630859375\n",
      "Epoch  3081: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3097: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3113: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3129: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6414.28125\n",
      "Epoch  2298: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2314: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2330: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2346: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30088.423828125\n",
      "Epoch  9124: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9140: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9156: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9172: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.15625\n",
      "Epoch  4164: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4180: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4196: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4212: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24291.39453125\n",
      "Epoch  5226: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5242: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5258: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5274: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37790.3359375\n",
      "Epoch  2364: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2380: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2396: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2412: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23237.583984375\n",
      "Epoch  3251: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3267: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3283: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3299: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14113.2255859375\n",
      "Epoch 12570: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12586: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12602: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12618: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2423.056396484375\n",
      "Epoch  3687: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3703: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3719: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3735: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8165.09423828125\n",
      "Epoch  2198: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2214: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2230: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2246: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19093.2734375\n",
      "Epoch  3734: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3750: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3766: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3782: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10354.796875\n",
      "Epoch  8719: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8735: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8751: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8767: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9315.5791015625\n",
      "Epoch  3954: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3970: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3986: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4002: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8877.1220703125\n",
      "Epoch  5157: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5173: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5189: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5205: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15623.9267578125\n",
      "Epoch  2279: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2295: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2311: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2327: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1818.47509765625\n",
      "Epoch  1883: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  1899: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  1915: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  1931: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1884195804595947 s\n",
      "loss_c =  261.84490966796875\n",
      "loss_c =  256.9898681640625\n",
      "loss_c =  256.08355712890625\n",
      "loss_c =  255.33880615234375\n",
      "loss_c =  254.69029235839844\n",
      "loss_c =  254.10858154296875\n",
      "loss_c =  253.57557678222656\n",
      "loss_c =  253.07901000976562\n",
      "loss_c =  252.60775756835938\n",
      "loss_c =  252.1329803466797\n",
      "Epoch 41515: reducing learning rate of group 0 to 2.0000e-05.\n",
      "thetac step =  2.696503162384033 s\n",
      "step =  9\n",
      "loss z =  11987114.0\n",
      "loss z =  11839246.0\n",
      "loss z =  11738152.0\n",
      "loss z =  11668728.0\n",
      "loss z =  11619355.0\n",
      "loss z =  11580634.0\n",
      "loss z =  11547355.0\n",
      "loss z =  11518185.0\n",
      "loss z =  11491592.0\n",
      "loss z =  11466644.0\n",
      "loss z =  11443079.0\n",
      "Epoch   233: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  11420943.0\n",
      "loss z =  11416736.0\n",
      "loss z =  11412582.0\n",
      "loss z =  11408438.0\n",
      "loss z =  11404380.0\n",
      "loss z =  11400342.0\n",
      "loss z =  11396368.0\n",
      "loss z =  11392440.0\n",
      "loss z =  11384710.0\n",
      "loss z =  11380906.0\n",
      "Epoch   244: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  11377147.0\n",
      "loss z =  11376403.0\n",
      "loss z =  11375662.0\n",
      "loss z =  11374920.0\n",
      "loss z =  11374180.0\n",
      "loss z =  11373447.0\n",
      "loss z =  11372710.0\n",
      "loss z =  11371977.0\n",
      "loss z =  11371246.0\n",
      "loss z =  11370515.0\n",
      "loss z =  11369785.0\n",
      "Epoch   255: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  11369058.0\n",
      "loss z =  11368914.0\n",
      "loss z =  11368770.0\n",
      "loss z =  11368624.0\n",
      "loss z =  11368478.0\n",
      "loss z =  11368333.0\n",
      "loss z =  11368044.0\n",
      "loss z =  11367902.0\n",
      "loss z =  11367758.0\n",
      "loss z =  11367611.0\n",
      "Epoch   266: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.599756002426147 s\n",
      "loss_f =  8701623.0\n",
      "loss_f =  9047613.0\n",
      "loss_f =  9211262.0\n",
      "loss_f =  8898808.0\n",
      "loss_f =  8610870.0\n",
      "loss_f =  8506280.0\n",
      "Epoch   879: reducing learning rate of group 0 to 1.5000e-03.\n",
      "loss_f =  8468846.0\n",
      "loss_f =  8442644.0\n",
      "loss_f =  8416060.0\n",
      "loss_f =  8397083.0\n",
      "loss_f =  8383657.0\n",
      "loss_f =  8373055.0\n",
      "loss_f =  8364278.5\n",
      "loss_f =  8356818.0\n",
      "loss_f =  8350133.0\n",
      "loss_f =  8343958.0\n",
      "loss_f =  8338244.0\n",
      "loss_f =  8332961.5\n",
      "loss_f =  8327992.5\n",
      "loss_f =  8323254.0\n",
      "loss_f =  8318695.0\n",
      "loss_f =  8314305.0\n",
      "loss_f =  8310051.0\n",
      "loss_f =  8305902.5\n",
      "loss_f =  8301840.0\n",
      "loss_f =  8297875.0\n",
      "loss_f =  8293989.0\n",
      "loss_f =  8290174.5\n",
      "loss_f =  8286400.5\n",
      "loss_f =  8282695.0\n",
      "loss_f =  8279040.0\n",
      "loss_f =  8275443.0\n",
      "loss_f =  8271881.5\n",
      "loss_f =  8268374.5\n",
      "loss_f =  8264903.5\n",
      "loss_f =  8261458.5\n",
      "loss_f =  8258056.0\n",
      "loss_f =  8254677.0\n",
      "loss_f =  8251347.0\n",
      "loss_f =  8248040.0\n",
      "thetaf step =  142.40323853492737 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11068.541015625\n",
      "Epoch  3145: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3161: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3177: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3193: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6413.2939453125\n",
      "Epoch  2363: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2379: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2395: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2411: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30088.5625\n",
      "Epoch  9188: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9204: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9220: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9236: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.1640625\n",
      "Epoch  4228: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4244: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4260: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4276: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24291.59375\n",
      "Epoch  5290: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5306: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5322: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5338: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37790.359375\n",
      "Epoch  2428: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2444: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2460: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2476: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23238.072265625\n",
      "Epoch  3315: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3331: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3347: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3363: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14113.8505859375\n",
      "Epoch 12634: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12650: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12666: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12682: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.64111328125\n",
      "Epoch  3752: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3768: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3784: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3800: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8164.6279296875\n",
      "Epoch  2262: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2278: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2294: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2310: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19093.947265625\n",
      "Epoch  3798: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3814: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3830: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3846: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10353.78515625\n",
      "Epoch  8783: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8799: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8815: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8831: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9316.5107421875\n",
      "Epoch  4018: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4034: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4050: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4066: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8876.2119140625\n",
      "Epoch  5222: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5238: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5254: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5270: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15625.32421875\n",
      "Epoch  2343: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2359: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2375: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2391: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1820.1455078125\n",
      "Epoch  1947: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  1963: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  1979: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  1995: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1771996021270752 s\n",
      "loss_c =  263.59075927734375\n",
      "loss_c =  258.0021057128906\n",
      "loss_c =  256.96026611328125\n",
      "loss_c =  256.08941650390625\n",
      "loss_c =  255.296875\n",
      "loss_c =  254.5257568359375\n",
      "loss_c =  253.7234344482422\n",
      "loss_c =  252.83477783203125\n",
      "loss_c =  251.8072509765625\n",
      "loss_c =  250.62158203125\n",
      "thetac step =  2.325016736984253 s\n",
      "step =  10\n",
      "loss z =  12059262.0\n",
      "loss z =  11890665.0\n",
      "loss z =  11769614.0\n",
      "loss z =  11680665.0\n",
      "loss z =  11611761.0\n",
      "loss z =  11556654.0\n",
      "loss z =  11511103.0\n",
      "loss z =  11472936.0\n",
      "loss z =  11440896.0\n",
      "loss z =  11412808.0\n",
      "loss z =  11386527.0\n",
      "Epoch   278: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  11361330.0\n",
      "loss z =  11356523.0\n",
      "loss z =  11351631.0\n",
      "loss z =  11346777.0\n",
      "loss z =  11341950.0\n",
      "loss z =  11337181.0\n",
      "loss z =  11332540.0\n",
      "loss z =  11327860.0\n",
      "loss z =  11318644.0\n",
      "loss z =  11314115.0\n",
      "Epoch   289: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  11309565.0\n",
      "loss z =  11308678.0\n",
      "loss z =  11307750.0\n",
      "loss z =  11306866.0\n",
      "loss z =  11305961.0\n",
      "loss z =  11305080.0\n",
      "loss z =  11304197.0\n",
      "loss z =  11303293.0\n",
      "loss z =  11302414.0\n",
      "loss z =  11301494.0\n",
      "loss z =  11300616.0\n",
      "Epoch   300: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  11299722.0\n",
      "loss z =  11299548.0\n",
      "loss z =  11299374.0\n",
      "loss z =  11299199.0\n",
      "loss z =  11299024.0\n",
      "loss z =  11298849.0\n",
      "loss z =  11298478.0\n",
      "loss z =  11298304.0\n",
      "loss z =  11298130.0\n",
      "loss z =  11297955.0\n",
      "Epoch   311: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.650843381881714 s\n",
      "loss_f =  8632652.0\n",
      "loss_f =  8560526.0\n",
      "loss_f =  8541656.0\n",
      "loss_f =  8533922.0\n",
      "loss_f =  8524562.0\n",
      "loss_f =  8511853.0\n",
      "loss_f =  8498818.0\n",
      "Epoch  1080: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  8491334.0\n",
      "loss_f =  8485592.0\n",
      "loss_f =  8480462.0\n",
      "loss_f =  8475801.0\n",
      "loss_f =  8471565.0\n",
      "loss_f =  8467718.0\n",
      "Epoch  1111: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  8465213.0\n",
      "loss_f =  8463446.0\n",
      "loss_f =  8461826.0\n",
      "loss_f =  8460215.0\n",
      "loss_f =  8458682.0\n",
      "loss_f =  8457160.0\n",
      "Epoch  1142: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  8455956.0\n",
      "loss_f =  8455223.0\n",
      "loss_f =  8454495.0\n",
      "loss_f =  8453776.0\n",
      "loss_f =  8453058.0\n",
      "loss_f =  8452344.0\n",
      "Epoch  1173: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  8451716.0\n",
      "loss_f =  8451338.0\n",
      "loss_f =  8450982.0\n",
      "loss_f =  8450625.0\n",
      "loss_f =  8450274.0\n",
      "loss_f =  8449924.0\n",
      "Epoch  1204: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  8449572.0\n",
      "loss_f =  8449402.0\n",
      "loss_f =  8449222.0\n",
      "loss_f =  8449052.0\n",
      "loss_f =  8448870.0\n",
      "loss_f =  8448687.0\n",
      "loss_f =  8448518.0\n",
      "Epoch  1235: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  8448412.0\n",
      "loss_f =  8448320.0\n",
      "thetaf step =  140.93225836753845 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11068.611328125\n",
      "Epoch  3209: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3225: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3241: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3257: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6413.08935546875\n",
      "Epoch  2427: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2443: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2459: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2475: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30088.892578125\n",
      "Epoch  9252: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9268: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9284: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9300: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.11328125\n",
      "Epoch  4292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4308: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4324: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4340: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24291.611328125\n",
      "Epoch  5354: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5370: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5386: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5402: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37790.5\n",
      "Epoch  2492: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2508: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2524: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2540: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23238.40234375\n",
      "Epoch  3379: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3395: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3411: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3427: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.529296875\n",
      "Epoch 12698: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12714: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12730: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12746: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.449951171875\n",
      "Epoch  3816: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3832: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3848: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3864: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8163.8603515625\n",
      "Epoch  2326: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2342: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2358: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2374: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19094.548828125\n",
      "Epoch  3862: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3878: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3894: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3910: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10354.099609375\n",
      "Epoch  8847: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8863: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8879: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8895: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9316.5673828125\n",
      "Epoch  4082: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4098: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4114: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4130: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8875.677734375\n",
      "Epoch  5286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5302: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5318: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5334: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15625.0859375\n",
      "Epoch  2407: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2423: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2439: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2455: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.430419921875\n",
      "Epoch  2011: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2027: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2043: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2059: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1840929985046387 s\n",
      "loss_c =  257.42755126953125\n",
      "loss_c =  256.04638671875\n",
      "loss_c =  255.54173278808594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_c =  255.03109741210938\n",
      "loss_c =  254.50762939453125\n",
      "loss_c =  253.97039794921875\n",
      "loss_c =  253.4197235107422\n",
      "loss_c =  252.8570556640625\n",
      "loss_c =  252.2843780517578\n",
      "loss_c =  251.70428466796875\n",
      "Epoch 42516: reducing learning rate of group 0 to 2.0000e-05.\n",
      "thetac step =  2.802664041519165 s\n",
      "step =  11\n",
      "loss z =  13748923.0\n",
      "loss z =  13563496.0\n",
      "loss z =  13396714.0\n",
      "loss z =  13243769.0\n",
      "loss z =  13102802.0\n",
      "loss z =  12972042.0\n",
      "loss z =  12848922.0\n",
      "loss z =  12732394.0\n",
      "loss z =  12621846.0\n",
      "loss z =  12517354.0\n",
      "Epoch   322: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  12419062.0\n",
      "loss z =  12400548.0\n",
      "loss z =  12382292.0\n",
      "loss z =  12364484.0\n",
      "loss z =  12346878.0\n",
      "loss z =  12329594.0\n",
      "loss z =  12312718.0\n",
      "loss z =  12296100.0\n",
      "loss z =  12279680.0\n",
      "loss z =  12247698.0\n",
      "Epoch   333: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  12232208.0\n",
      "loss z =  12229174.0\n",
      "loss z =  12226106.0\n",
      "loss z =  12223090.0\n",
      "loss z =  12220076.0\n",
      "loss z =  12217035.0\n",
      "loss z =  12214001.0\n",
      "loss z =  12211018.0\n",
      "loss z =  12208044.0\n",
      "loss z =  12205082.0\n",
      "loss z =  12202106.0\n",
      "Epoch   344: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  12199137.0\n",
      "loss z =  12198552.0\n",
      "loss z =  12197965.0\n",
      "loss z =  12197378.0\n",
      "loss z =  12196769.0\n",
      "loss z =  12196137.0\n",
      "loss z =  12195550.0\n",
      "loss z =  12194357.0\n",
      "loss z =  12193771.0\n",
      "loss z =  12193183.0\n",
      "Epoch   355: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.01718783378601 s\n",
      "loss_f =  9513824.0\n",
      "loss_f =  9343671.0\n",
      "loss_f =  9265178.0\n",
      "loss_f =  9218933.0\n",
      "Epoch  1266: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  9193335.0\n",
      "loss_f =  9176743.0\n",
      "loss_f =  9161294.0\n",
      "loss_f =  9146920.0\n",
      "loss_f =  9133618.0\n",
      "loss_f =  9121264.0\n",
      "Epoch  1297: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  9111955.0\n",
      "loss_f =  9106444.0\n",
      "loss_f =  9101158.0\n",
      "loss_f =  9096010.0\n",
      "loss_f =  9090956.0\n",
      "loss_f =  9086036.0\n",
      "Epoch  1328: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  9081628.0\n",
      "loss_f =  9079263.0\n",
      "loss_f =  9076894.0\n",
      "loss_f =  9074568.0\n",
      "loss_f =  9072252.0\n",
      "loss_f =  9069888.0\n",
      "Epoch  1359: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  9067594.0\n",
      "loss_f =  9066454.0\n",
      "loss_f =  9065312.0\n",
      "loss_f =  9064155.0\n",
      "loss_f =  9063001.0\n",
      "loss_f =  9061827.0\n",
      "loss_f =  9060691.0\n",
      "Epoch  1390: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  9060014.0\n",
      "loss_f =  9059428.0\n",
      "loss_f =  9058854.0\n",
      "loss_f =  9058288.0\n",
      "loss_f =  9057726.0\n",
      "loss_f =  9057150.0\n",
      "Epoch  1421: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  9056755.0\n",
      "loss_f =  9056464.0\n",
      "loss_f =  9056178.0\n",
      "loss_f =  9055901.0\n",
      "loss_f =  9055605.0\n",
      "thetaf step =  140.05347084999084 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11067.7109375\n",
      "Epoch  3273: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3289: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3305: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3321: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6412.21533203125\n",
      "Epoch  2492: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2508: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2524: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2540: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30089.265625\n",
      "Epoch  9316: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9332: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9348: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9364: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.12890625\n",
      "Epoch  4356: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4372: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4388: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4404: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.056640625\n",
      "Epoch  5418: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5434: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5450: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5466: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37790.73046875\n",
      "Epoch  2556: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2572: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2588: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2604: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23239.0078125\n",
      "Epoch  3443: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3459: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3475: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3491: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14115.6123046875\n",
      "Epoch 12762: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12778: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12794: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12810: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.057861328125\n",
      "Epoch  3881: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3897: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3913: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3929: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8162.314453125\n",
      "Epoch  2390: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2406: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2422: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2438: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19095.548828125\n",
      "Epoch  3926: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3942: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3958: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3974: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10352.9384765625\n",
      "Epoch  8912: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8928: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8944: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8960: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9314.05078125\n",
      "Epoch  4146: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4162: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4178: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4194: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8874.6650390625\n",
      "Epoch  5351: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5367: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5383: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5399: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15622.3955078125\n",
      "Epoch  2471: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2487: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2503: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2519: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1814.9395751953125\n",
      "Epoch  2075: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2091: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2107: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2123: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.19834303855896 s\n",
      "loss_c =  251.5964813232422\n",
      "loss_c =  249.0504608154297\n",
      "loss_c =  248.37371826171875\n",
      "loss_c =  247.75323486328125\n",
      "loss_c =  247.16549682617188\n",
      "loss_c =  246.6002960205078\n",
      "loss_c =  246.05072021484375\n",
      "loss_c =  245.51171875\n",
      "loss_c =  244.97962951660156\n",
      "loss_c =  244.45147705078125\n",
      "thetac step =  3.0778470039367676 s\n",
      "step =  12\n",
      "loss z =  14161964.0\n",
      "loss z =  14058191.0\n",
      "loss z =  13965276.0\n",
      "loss z =  13881838.0\n",
      "loss z =  13805520.0\n",
      "loss z =  13734172.0\n",
      "loss z =  13667111.0\n",
      "loss z =  13603883.0\n",
      "loss z =  13544817.0\n",
      "loss z =  13489948.0\n",
      "Epoch   366: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  13439501.0\n",
      "loss z =  13429907.0\n",
      "loss z =  13420450.0\n",
      "loss z =  13411119.0\n",
      "loss z =  13401842.0\n",
      "loss z =  13392790.0\n",
      "loss z =  13383776.0\n",
      "loss z =  13374761.0\n",
      "loss z =  13365946.0\n",
      "loss z =  13348335.0\n",
      "Epoch   377: reducing learning rate of group 0 to 4.0000e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss z =  13339704.0\n",
      "loss z =  13338000.0\n",
      "loss z =  13336281.0\n",
      "loss z =  13334559.0\n",
      "loss z =  13332868.0\n",
      "loss z =  13331131.0\n",
      "loss z =  13329442.0\n",
      "loss z =  13327734.0\n",
      "loss z =  13326010.0\n",
      "loss z =  13324303.0\n",
      "loss z =  13322601.0\n",
      "Epoch   388: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  13320927.0\n",
      "loss z =  13320570.0\n",
      "loss z =  13320236.0\n",
      "loss z =  13319900.0\n",
      "loss z =  13319545.0\n",
      "loss z =  13319212.0\n",
      "loss z =  13318877.0\n",
      "loss z =  13318166.0\n",
      "loss z =  13317830.0\n",
      "loss z =  13317497.0\n",
      "Epoch   399: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.19505214691162 s\n",
      "loss_f =  10564726.0\n",
      "Epoch  1452: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  10210422.0\n",
      "loss_f =  10094984.0\n",
      "loss_f =  10020502.0\n",
      "loss_f =  9966384.0\n",
      "loss_f =  9923632.0\n",
      "loss_f =  9887497.0\n",
      "Epoch  1483: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  9858521.0\n",
      "loss_f =  9843812.0\n",
      "loss_f =  9829938.0\n",
      "loss_f =  9816634.0\n",
      "loss_f =  9803703.0\n",
      "loss_f =  9791027.0\n",
      "Epoch  1514: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  9778591.0\n",
      "loss_f =  9772468.0\n",
      "loss_f =  9766430.0\n",
      "loss_f =  9760416.0\n",
      "loss_f =  9754468.0\n",
      "loss_f =  9748554.0\n",
      "loss_f =  9742708.0\n",
      "Epoch  1545: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  9739218.0\n",
      "loss_f =  9736324.0\n",
      "loss_f =  9733396.0\n",
      "loss_f =  9730503.0\n",
      "loss_f =  9727628.0\n",
      "loss_f =  9724757.0\n",
      "Epoch  1576: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  9722727.0\n",
      "loss_f =  9721275.0\n",
      "loss_f =  9719840.0\n",
      "loss_f =  9718386.0\n",
      "loss_f =  9716952.0\n",
      "loss_f =  9715516.0\n",
      "Epoch  1607: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  9714369.0\n",
      "loss_f =  9713648.0\n",
      "loss_f =  9712933.0\n",
      "loss_f =  9712220.0\n",
      "loss_f =  9711496.0\n",
      "loss_f =  9710768.0\n",
      "Epoch  1638: reducing learning rate of group 0 to 3.9063e-06.\n",
      "thetaf step =  131.5713336467743 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11068.6904296875\n",
      "Epoch  3337: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3353: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3369: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3385: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6412.09912109375\n",
      "Epoch  2556: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2572: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2588: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2604: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30090.080078125\n",
      "Epoch  9380: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9396: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9412: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9428: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.2265625\n",
      "Epoch  4420: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4436: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4452: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4468: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24291.69140625\n",
      "Epoch  5482: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5498: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5514: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5530: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.00390625\n",
      "Epoch  2620: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2636: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2652: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2668: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23239.865234375\n",
      "Epoch  3507: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3523: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3539: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3555: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14117.4873046875\n",
      "Epoch 12826: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12842: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12858: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12874: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2421.892822265625\n",
      "Epoch  3945: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3961: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3977: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3993: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8161.79345703125\n",
      "Epoch  2454: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2470: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2486: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2502: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19096.498046875\n",
      "Epoch  3990: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4022: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4038: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10353.6845703125\n",
      "Epoch  8976: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8992: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9008: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9024: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9314.966796875\n",
      "Epoch  4210: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4226: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4242: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4258: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8874.5908203125\n",
      "Epoch  5415: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5431: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5447: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5463: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15629.1845703125\n",
      "Epoch  2535: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2551: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2567: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2583: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.8841552734375\n",
      "Epoch  2139: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2155: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2171: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2187: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1404814720153809 s\n",
      "loss_c =  263.6002502441406\n",
      "loss_c =  253.90969848632812\n",
      "loss_c =  251.56903076171875\n",
      "loss_c =  249.7305908203125\n",
      "loss_c =  248.26539611816406\n",
      "loss_c =  247.09136962890625\n",
      "loss_c =  246.1396484375\n",
      "loss_c =  245.354248046875\n",
      "loss_c =  244.69091796875\n",
      "loss_c =  244.11508178710938\n",
      "Epoch 43517: reducing learning rate of group 0 to 2.0000e-05.\n",
      "thetac step =  3.3034751415252686 s\n",
      "step =  13\n",
      "loss z =  13235326.0\n",
      "loss z =  13147968.0\n",
      "loss z =  13064976.0\n",
      "loss z =  12986258.0\n",
      "loss z =  12911321.0\n",
      "loss z =  12840522.0\n",
      "loss z =  12773350.0\n",
      "loss z =  12709401.0\n",
      "loss z =  12648736.0\n",
      "loss z =  12591586.0\n",
      "Epoch   410: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  12538022.0\n",
      "loss z =  12527930.0\n",
      "loss z =  12517983.0\n",
      "loss z =  12508255.0\n",
      "loss z =  12498679.0\n",
      "loss z =  12489235.0\n",
      "loss z =  12479982.0\n",
      "loss z =  12470866.0\n",
      "loss z =  12461856.0\n",
      "loss z =  12444360.0\n",
      "Epoch   421: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  12435862.0\n",
      "loss z =  12434192.0\n",
      "loss z =  12432528.0\n",
      "loss z =  12430845.0\n",
      "loss z =  12429191.0\n",
      "loss z =  12427546.0\n",
      "loss z =  12425904.0\n",
      "loss z =  12424248.0\n",
      "loss z =  12422616.0\n",
      "loss z =  12420992.0\n",
      "loss z =  12419375.0\n",
      "Epoch   432: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  12417762.0\n",
      "loss z =  12417441.0\n",
      "loss z =  12417122.0\n",
      "loss z =  12416802.0\n",
      "loss z =  12416480.0\n",
      "loss z =  12416160.0\n",
      "loss z =  12415839.0\n",
      "loss z =  12415199.0\n",
      "loss z =  12414880.0\n",
      "loss z =  12414560.0\n",
      "Epoch   443: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.129053115844727 s\n",
      "loss_f =  9651000.0\n",
      "loss_f =  9434667.0\n",
      "loss_f =  9324687.0\n",
      "loss_f =  9256708.0\n",
      "loss_f =  9207285.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_f =  9168032.0\n",
      "loss_f =  9135566.0\n",
      "Epoch  1669: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  9118657.0\n",
      "loss_f =  9105987.0\n",
      "loss_f =  9094174.0\n",
      "loss_f =  9083057.0\n",
      "loss_f =  9072471.0\n",
      "loss_f =  9062275.0\n",
      "Epoch  1700: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  9055354.0\n",
      "loss_f =  9050520.0\n",
      "loss_f =  9045754.0\n",
      "loss_f =  9041049.0\n",
      "loss_f =  9036396.0\n",
      "loss_f =  9031788.0\n",
      "Epoch  1731: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  9028134.0\n",
      "loss_f =  9025866.0\n",
      "loss_f =  9023612.0\n",
      "loss_f =  9021368.0\n",
      "loss_f =  9019128.0\n",
      "loss_f =  9016865.0\n",
      "Epoch  1762: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  9014870.0\n",
      "loss_f =  9013758.0\n",
      "loss_f =  9012642.0\n",
      "loss_f =  9011532.0\n",
      "loss_f =  9010424.0\n",
      "loss_f =  9009320.0\n",
      "Epoch  1793: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  9008198.0\n",
      "loss_f =  9007648.0\n",
      "loss_f =  9007078.0\n",
      "loss_f =  9006516.0\n",
      "loss_f =  9005964.0\n",
      "loss_f =  9005402.0\n",
      "loss_f =  9004840.0\n",
      "Epoch  1824: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  9004502.0\n",
      "loss_f =  9004221.0\n",
      "thetaf step =  139.50169706344604 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11076.244140625\n",
      "Epoch  3401: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3417: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3433: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3449: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6411.29248046875\n",
      "Epoch  2621: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2637: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2653: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2669: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30091.330078125\n",
      "Epoch  9444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9460: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9476: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9492: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.76953125\n",
      "Epoch  4484: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4500: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4516: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4532: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.1484375\n",
      "Epoch  5546: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5562: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5578: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5594: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37792.2265625\n",
      "Epoch  2684: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2700: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2716: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2732: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23242.15234375\n",
      "Epoch  3571: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3587: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3603: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3619: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14120.0048828125\n",
      "Epoch 12890: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12906: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12922: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12938: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2421.652099609375\n",
      "Epoch  4010: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4026: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4042: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4058: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8158.80712890625\n",
      "Epoch  2518: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2534: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2550: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2566: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19099.078125\n",
      "Epoch  4054: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4070: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4086: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4102: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10353.1357421875\n",
      "Epoch  9040: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9056: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9072: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9088: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9318.5947265625\n",
      "Epoch  4274: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4290: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4306: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4322: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8872.37890625\n",
      "Epoch  5480: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5496: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5512: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5528: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15640.52734375\n",
      "Epoch  2599: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2615: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2631: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2647: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1814.52978515625\n",
      "Epoch  2203: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2219: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2235: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2251: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.188805103302002 s\n",
      "loss_c =  290.83233642578125\n",
      "loss_c =  288.1675720214844\n",
      "loss_c =  287.16925048828125\n",
      "loss_c =  286.1864013671875\n",
      "loss_c =  285.20941162109375\n",
      "loss_c =  284.2371826171875\n",
      "loss_c =  283.2685241699219\n",
      "loss_c =  282.30194091796875\n",
      "loss_c =  281.3365783691406\n",
      "loss_c =  280.37176513671875\n",
      "thetac step =  2.776319742202759 s\n",
      "step =  14\n",
      "loss z =  12179849.0\n",
      "loss z =  12143336.0\n",
      "loss z =  12111834.0\n",
      "loss z =  12083948.0\n",
      "loss z =  12058288.0\n",
      "loss z =  12034201.0\n",
      "loss z =  12011316.0\n",
      "loss z =  11989223.0\n",
      "loss z =  11968424.0\n",
      "loss z =  11948814.0\n",
      "Epoch   454: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  11930023.0\n",
      "loss z =  11926461.0\n",
      "loss z =  11922900.0\n",
      "loss z =  11919379.0\n",
      "loss z =  11915890.0\n",
      "loss z =  11912501.0\n",
      "loss z =  11909100.0\n",
      "loss z =  11905748.0\n",
      "loss z =  11902421.0\n",
      "loss z =  11895840.0\n",
      "Epoch   465: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  11892626.0\n",
      "loss z =  11891995.0\n",
      "loss z =  11891347.0\n",
      "loss z =  11890722.0\n",
      "loss z =  11890098.0\n",
      "loss z =  11889475.0\n",
      "loss z =  11888852.0\n",
      "loss z =  11888191.0\n",
      "loss z =  11887550.0\n",
      "loss z =  11886934.0\n",
      "loss z =  11886316.0\n",
      "Epoch   476: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  11885678.0\n",
      "loss z =  11885556.0\n",
      "loss z =  11885433.0\n",
      "loss z =  11885311.0\n",
      "loss z =  11885189.0\n",
      "loss z =  11885068.0\n",
      "loss z =  11884945.0\n",
      "loss z =  11884681.0\n",
      "loss z =  11884559.0\n",
      "loss z =  11884435.0\n",
      "Epoch   487: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  23.910993576049805 s\n",
      "loss_f =  9090591.0\n",
      "loss_f =  8976011.0\n",
      "loss_f =  8921612.0\n",
      "loss_f =  8886836.0\n",
      "Epoch  1855: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  8867815.0\n",
      "loss_f =  8856391.0\n",
      "loss_f =  8846289.0\n",
      "loss_f =  8837186.0\n",
      "loss_f =  8828923.0\n",
      "loss_f =  8821341.0\n",
      "Epoch  1886: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  8815625.0\n",
      "loss_f =  8812221.0\n",
      "loss_f =  8808896.0\n",
      "loss_f =  8805680.0\n",
      "loss_f =  8802546.0\n",
      "loss_f =  8799472.0\n",
      "Epoch  1917: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  8796747.0\n",
      "loss_f =  8795230.0\n",
      "loss_f =  8793728.0\n",
      "loss_f =  8792259.0\n",
      "loss_f =  8790792.0\n",
      "loss_f =  8789341.0\n",
      "Epoch  1948: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  8787895.0\n",
      "loss_f =  8787166.0\n",
      "loss_f =  8786447.0\n",
      "loss_f =  8785732.0\n",
      "loss_f =  8784990.0\n",
      "loss_f =  8784272.0\n",
      "loss_f =  8783554.0\n",
      "Epoch  1979: reducing learning rate of group 0 to 1.5625e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_f =  8783126.0\n",
      "loss_f =  8782764.0\n",
      "loss_f =  8782403.0\n",
      "loss_f =  8782050.0\n",
      "loss_f =  8781688.0\n",
      "loss_f =  8781326.0\n",
      "Epoch  2010: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  8781074.0\n",
      "loss_f =  8780896.0\n",
      "loss_f =  8780720.0\n",
      "loss_f =  8780537.0\n",
      "loss_f =  8780358.0\n",
      "thetaf step =  138.17585372924805 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11070.74609375\n",
      "Epoch  3465: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3481: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3497: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3513: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6408.7548828125\n",
      "Epoch  2686: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2702: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2718: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2734: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30089.958984375\n",
      "Epoch  9508: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9524: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9540: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9556: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.2890625\n",
      "Epoch  4548: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4564: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4580: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4596: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.646484375\n",
      "Epoch  5610: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5626: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5642: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5658: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37790.97265625\n",
      "Epoch  2748: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2764: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2780: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2796: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23240.7890625\n",
      "Epoch  3635: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3651: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3667: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3683: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14118.9013671875\n",
      "Epoch 12954: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12970: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12986: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13002: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2420.905029296875\n",
      "Epoch  4075: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4091: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4107: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4123: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8157.30859375\n",
      "Epoch  2582: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2598: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2614: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2630: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19097.046875\n",
      "Epoch  4118: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4134: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4150: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4166: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10349.9921875\n",
      "Epoch  9105: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9121: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9137: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9153: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9314.7294921875\n",
      "Epoch  4338: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4354: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4370: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4386: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8871.486328125\n",
      "Epoch  5545: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5561: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5577: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5593: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15636.529296875\n",
      "Epoch  2663: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2679: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2695: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2711: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1816.072265625\n",
      "Epoch  2267: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2283: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2299: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2315: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.142333745956421 s\n",
      "loss_c =  263.0869140625\n",
      "loss_c =  261.4205017089844\n",
      "loss_c =  260.7899475097656\n",
      "loss_c =  260.1496276855469\n",
      "loss_c =  259.4934997558594\n",
      "loss_c =  258.82110595703125\n",
      "loss_c =  258.13262939453125\n",
      "loss_c =  257.4281005859375\n",
      "loss_c =  256.707763671875\n",
      "loss_c =  255.9721221923828\n",
      "Epoch 44518: reducing learning rate of group 0 to 2.0000e-05.\n",
      "thetac step =  3.212620735168457 s\n",
      "step =  15\n",
      "loss z =  11572518.0\n",
      "loss z =  11562531.0\n",
      "loss z =  11553696.0\n",
      "loss z =  11545624.0\n",
      "loss z =  11538007.0\n",
      "loss z =  11530638.0\n",
      "loss z =  11523468.0\n",
      "loss z =  11516467.0\n",
      "loss z =  11509630.0\n",
      "loss z =  11502936.0\n",
      "Epoch   498: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  11496366.0\n",
      "loss z =  11495078.0\n",
      "loss z =  11493795.0\n",
      "loss z =  11492515.0\n",
      "loss z =  11491240.0\n",
      "loss z =  11489974.0\n",
      "loss z =  11488711.0\n",
      "loss z =  11487452.0\n",
      "loss z =  11486201.0\n",
      "loss z =  11483714.0\n",
      "Epoch   509: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  11482478.0\n",
      "loss z =  11482231.0\n",
      "loss z =  11481986.0\n",
      "loss z =  11481740.0\n",
      "loss z =  11481494.0\n",
      "loss z =  11481247.0\n",
      "loss z =  11481003.0\n",
      "loss z =  11480759.0\n",
      "loss z =  11480514.0\n",
      "loss z =  11480273.0\n",
      "loss z =  11480028.0\n",
      "Epoch   520: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  11479784.0\n",
      "loss z =  11479735.0\n",
      "loss z =  11479687.0\n",
      "loss z =  11479638.0\n",
      "loss z =  11479588.0\n",
      "loss z =  11479541.0\n",
      "loss z =  11479491.0\n",
      "loss z =  11479393.0\n",
      "loss z =  11479344.0\n",
      "loss z =  11479295.0\n",
      "Epoch   531: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  23.7780659198761 s\n",
      "loss_f =  8683352.0\n",
      "Epoch  2041: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  8635948.0\n",
      "loss_f =  8620896.0\n",
      "loss_f =  8610564.0\n",
      "loss_f =  8602441.0\n",
      "loss_f =  8595590.0\n",
      "loss_f =  8589687.0\n",
      "Epoch  2072: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  8585010.0\n",
      "loss_f =  8582646.0\n",
      "loss_f =  8580353.0\n",
      "loss_f =  8578210.0\n",
      "loss_f =  8576086.0\n",
      "loss_f =  8574024.0\n",
      "Epoch  2103: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  8571988.0\n",
      "loss_f =  8570968.0\n",
      "loss_f =  8569970.0\n",
      "loss_f =  8568976.0\n",
      "loss_f =  8567990.0\n",
      "loss_f =  8566997.0\n",
      "loss_f =  8566017.0\n",
      "Epoch  2134: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  8565429.0\n",
      "loss_f =  8564936.0\n",
      "loss_f =  8564436.0\n",
      "loss_f =  8563958.0\n",
      "loss_f =  8563468.0\n",
      "loss_f =  8562980.0\n",
      "Epoch  2165: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  8562632.0\n",
      "loss_f =  8562392.0\n",
      "loss_f =  8562148.0\n",
      "loss_f =  8561904.0\n",
      "loss_f =  8561651.0\n",
      "loss_f =  8561407.0\n",
      "Epoch  2196: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  8561215.0\n",
      "loss_f =  8561091.0\n",
      "loss_f =  8560965.0\n",
      "loss_f =  8560846.0\n",
      "loss_f =  8560720.0\n",
      "loss_f =  8560600.0\n",
      "Epoch  2227: reducing learning rate of group 0 to 3.9063e-06.\n",
      "thetaf step =  130.68142914772034 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11069.978515625\n",
      "Epoch  3529: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3545: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3561: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3577: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6408.7138671875\n",
      "Epoch  2750: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2766: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2782: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2798: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30089.900390625\n",
      "Epoch  9572: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9588: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9604: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9620: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.24609375\n",
      "Epoch  4612: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4628: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4644: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4660: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.5\n",
      "Epoch  5674: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5690: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5706: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5722: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37790.4921875\n",
      "Epoch  2812: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2828: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2844: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2860: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23240.693359375\n",
      "Epoch  3699: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3715: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3731: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3747: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14118.76953125\n",
      "Epoch 13018: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13034: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13050: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13066: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2420.921142578125\n",
      "Epoch  4139: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4155: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4171: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4187: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8157.2255859375\n",
      "Epoch  2646: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2662: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2678: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2694: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19096.8671875\n",
      "Epoch  4182: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4198: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4214: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4230: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10350.0595703125\n",
      "Epoch  9169: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9185: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9201: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9217: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9314.0478515625\n",
      "Epoch  4402: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4418: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4434: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4450: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8871.45703125\n",
      "Epoch  5609: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5625: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5641: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5657: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15627.5615234375\n",
      "Epoch  2727: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2743: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2759: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2775: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1814.1400146484375\n",
      "Epoch  2331: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2347: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2363: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.118652582168579 s\n",
      "loss_c =  249.53518676757812\n",
      "loss_c =  246.92922973632812\n",
      "loss_c =  246.34457397460938\n",
      "loss_c =  245.8330078125\n",
      "loss_c =  245.3596649169922\n",
      "loss_c =  244.90762329101562\n",
      "loss_c =  244.46551513671875\n",
      "loss_c =  244.025390625\n",
      "loss_c =  243.58148193359375\n",
      "loss_c =  243.1297607421875\n",
      "thetac step =  3.548508644104004 s\n",
      "step =  16\n",
      "loss z =  11371334.0\n",
      "loss z =  11363200.0\n",
      "loss z =  11355768.0\n",
      "loss z =  11348922.0\n",
      "loss z =  11342538.0\n",
      "loss z =  11336542.0\n",
      "loss z =  11330875.0\n",
      "loss z =  11325494.0\n",
      "loss z =  11320343.0\n",
      "loss z =  11315378.0\n",
      "Epoch   542: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  11310548.0\n",
      "loss z =  11309604.0\n",
      "loss z =  11308668.0\n",
      "loss z =  11307737.0\n",
      "loss z =  11306812.0\n",
      "loss z =  11305886.0\n",
      "loss z =  11304968.0\n",
      "loss z =  11304052.0\n",
      "loss z =  11303138.0\n",
      "loss z =  11301329.0\n",
      "Epoch   553: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  11300432.0\n",
      "loss z =  11300253.0\n",
      "loss z =  11300074.0\n",
      "loss z =  11299896.0\n",
      "loss z =  11299717.0\n",
      "loss z =  11299540.0\n",
      "loss z =  11299362.0\n",
      "loss z =  11299183.0\n",
      "loss z =  11299006.0\n",
      "loss z =  11298828.0\n",
      "loss z =  11298650.0\n",
      "Epoch   564: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  11298473.0\n",
      "loss z =  11298437.0\n",
      "loss z =  11298402.0\n",
      "loss z =  11298365.0\n",
      "loss z =  11298330.0\n",
      "loss z =  11298294.0\n",
      "loss z =  11298258.0\n",
      "loss z =  11298188.0\n",
      "loss z =  11298154.0\n",
      "loss z =  11298119.0\n",
      "Epoch   575: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.32742142677307 s\n",
      "loss_f =  8509086.0\n",
      "loss_f =  8476640.0\n",
      "loss_f =  8459868.0\n",
      "loss_f =  8447944.0\n",
      "loss_f =  8438137.0\n",
      "loss_f =  8430061.0\n",
      "loss_f =  8423448.0\n",
      "Epoch  2258: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  8419925.0\n",
      "loss_f =  8417167.0\n",
      "loss_f =  8414532.0\n",
      "loss_f =  8412030.0\n",
      "loss_f =  8409590.0\n",
      "loss_f =  8407232.0\n",
      "Epoch  2289: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  8405622.0\n",
      "loss_f =  8404486.0\n",
      "loss_f =  8403369.0\n",
      "loss_f =  8402256.0\n",
      "loss_f =  8401160.0\n",
      "loss_f =  8400048.0\n",
      "Epoch  2320: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  8399168.0\n",
      "loss_f =  8398619.0\n",
      "loss_f =  8398069.0\n",
      "loss_f =  8397523.0\n",
      "loss_f =  8396973.0\n",
      "loss_f =  8396436.0\n",
      "Epoch  2351: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  8395938.0\n",
      "loss_f =  8395670.0\n",
      "loss_f =  8395399.0\n",
      "loss_f =  8395127.0\n",
      "loss_f =  8394858.0\n",
      "loss_f =  8394580.0\n",
      "Epoch  2382: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  8394309.0\n",
      "loss_f =  8394174.0\n",
      "loss_f =  8394032.0\n",
      "loss_f =  8393896.0\n",
      "loss_f =  8393758.0\n",
      "loss_f =  8393623.0\n",
      "loss_f =  8393483.0\n",
      "Epoch  2413: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  8393397.0\n",
      "loss_f =  8393334.0\n",
      "thetaf step =  138.23814272880554 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11069.08984375\n",
      "Epoch  3593: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3609: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3625: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3641: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6411.30810546875\n",
      "Epoch  2814: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2830: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2846: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2862: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30090.205078125\n",
      "Epoch  9636: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9652: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9668: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9684: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.1484375\n",
      "Epoch  4676: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4692: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4708: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4724: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.12890625\n",
      "Epoch  5738: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5754: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5770: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5786: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37790.4609375\n",
      "Epoch  2876: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2892: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2908: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2924: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23240.41015625\n",
      "Epoch  3763: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3779: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3795: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3811: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14117.64453125\n",
      "Epoch 13082: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13098: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13114: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13130: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2421.74072265625\n",
      "Epoch  4203: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4219: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4235: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4251: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8160.7431640625\n",
      "Epoch  2710: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2726: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2742: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2758: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19097.138671875\n",
      "Epoch  4246: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4262: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4278: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4294: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10352.826171875\n",
      "Epoch  9233: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9249: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9265: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9281: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9313.9150390625\n",
      "Epoch  4466: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4482: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4498: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4514: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8873.94921875\n",
      "Epoch  5673: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5689: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5705: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5721: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15651.4580078125\n",
      "Epoch  2791: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2807: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2823: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2839: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1814.041259765625\n",
      "Epoch  2395: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2411: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2427: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2443: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.118600845336914 s\n",
      "loss_c =  283.16619873046875\n",
      "loss_c =  279.834716796875\n",
      "loss_c =  278.9423828125\n",
      "loss_c =  278.1131286621094\n",
      "loss_c =  277.30499267578125\n",
      "loss_c =  276.4974365234375\n",
      "loss_c =  275.67633056640625\n",
      "loss_c =  274.8312683105469\n",
      "loss_c =  273.9544372558594\n",
      "loss_c =  273.03936767578125\n",
      "thetac step =  3.5709714889526367 s\n",
      "step =  17\n",
      "loss z =  11252874.0\n",
      "loss z =  11246607.0\n",
      "loss z =  11241111.0\n",
      "loss z =  11236029.0\n",
      "loss z =  11231166.0\n",
      "loss z =  11226449.0\n",
      "loss z =  11221868.0\n",
      "loss z =  11217362.0\n",
      "loss z =  11212954.0\n",
      "loss z =  11208641.0\n",
      "Epoch   586: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  11204422.0\n",
      "loss z =  11203590.0\n",
      "loss z =  11202766.0\n",
      "loss z =  11201942.0\n",
      "loss z =  11201122.0\n",
      "loss z =  11200308.0\n",
      "loss z =  11199497.0\n",
      "loss z =  11198691.0\n",
      "loss z =  11197885.0\n",
      "loss z =  11196285.0\n",
      "Epoch   597: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  11195490.0\n",
      "loss z =  11195332.0\n",
      "loss z =  11195174.0\n",
      "loss z =  11195016.0\n",
      "loss z =  11194858.0\n",
      "loss z =  11194700.0\n",
      "loss z =  11194542.0\n",
      "loss z =  11194385.0\n",
      "loss z =  11194228.0\n",
      "loss z =  11194069.0\n",
      "loss z =  11193914.0\n",
      "Epoch   608: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  11193756.0\n",
      "loss z =  11193724.0\n",
      "loss z =  11193693.0\n",
      "loss z =  11193662.0\n",
      "loss z =  11193630.0\n",
      "loss z =  11193599.0\n",
      "loss z =  11193567.0\n",
      "loss z =  11193505.0\n",
      "loss z =  11193472.0\n",
      "loss z =  11193442.0\n",
      "Epoch   619: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  23.777570486068726 s\n",
      "loss_f =  8409580.0\n",
      "loss_f =  8386068.0\n",
      "loss_f =  8373531.5\n",
      "loss_f =  8363577.0\n",
      "Epoch  2444: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  8357368.0\n",
      "loss_f =  8353560.0\n",
      "loss_f =  8350270.0\n",
      "loss_f =  8347454.0\n",
      "loss_f =  8344950.0\n",
      "loss_f =  8342636.5\n",
      "Epoch  2475: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  8340871.0\n",
      "loss_f =  8339806.5\n",
      "loss_f =  8338763.0\n",
      "loss_f =  8337727.5\n",
      "loss_f =  8336695.5\n",
      "loss_f =  8335678.5\n",
      "Epoch  2506: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  8334767.0\n",
      "loss_f =  8334268.5\n",
      "loss_f =  8333761.5\n",
      "loss_f =  8333252.5\n",
      "loss_f =  8332762.0\n",
      "loss_f =  8332259.5\n",
      "Epoch  2537: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  8331758.0\n",
      "loss_f =  8331514.0\n",
      "loss_f =  8331258.5\n",
      "loss_f =  8331010.0\n",
      "loss_f =  8330755.0\n",
      "loss_f =  8330513.0\n",
      "loss_f =  8330259.5\n",
      "Epoch  2568: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  8330101.0\n",
      "loss_f =  8329988.5\n",
      "loss_f =  8329858.5\n",
      "loss_f =  8329729.5\n",
      "loss_f =  8329598.0\n",
      "loss_f =  8329474.0\n",
      "Epoch  2599: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  8329396.0\n",
      "loss_f =  8329324.0\n",
      "loss_f =  8329261.0\n",
      "loss_f =  8329196.0\n",
      "loss_f =  8329129.5\n",
      "thetaf step =  138.22013688087463 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11069.4150390625\n",
      "Epoch  3657: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3673: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3689: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3705: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6407.93115234375\n",
      "Epoch  2879: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2895: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2911: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2927: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30089.876953125\n",
      "Epoch  9700: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9716: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9732: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9748: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.25390625\n",
      "Epoch  4740: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4756: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4772: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4788: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.478515625\n",
      "Epoch  5802: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5818: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5834: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5850: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37790.2734375\n",
      "Epoch  2940: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2956: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2972: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2988: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23240.841796875\n",
      "Epoch  3827: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3843: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3859: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3875: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14118.9814453125\n",
      "Epoch 13146: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13162: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13178: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13194: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2420.748046875\n",
      "Epoch  4267: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4283: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4299: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4315: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8155.7568359375\n",
      "Epoch  2774: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2790: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2806: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2822: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19097.03125\n",
      "Epoch  4310: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4326: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4342: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4358: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10349.2197265625\n",
      "Epoch  9297: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9313: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9329: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9345: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9313.521484375\n",
      "Epoch  4531: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4547: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4563: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4579: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8870.6787109375\n",
      "Epoch  5737: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5753: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5769: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5785: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15622.5859375\n",
      "Epoch  2855: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2871: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2887: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2903: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1814.6275634765625\n",
      "Epoch  2459: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2475: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2491: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2507: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.147876262664795 s\n",
      "loss_c =  240.17901611328125\n",
      "Epoch 45519: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  239.10377502441406\n",
      "loss_c =  238.83616638183594\n",
      "loss_c =  238.58840942382812\n",
      "loss_c =  238.3559112548828\n",
      "loss_c =  238.13719177246094\n",
      "loss_c =  237.93121337890625\n",
      "loss_c =  237.736572265625\n",
      "loss_c =  237.55221557617188\n",
      "loss_c =  237.3772735595703\n",
      "thetac step =  3.757859945297241 s\n",
      "step =  18\n",
      "loss z =  11184908.0\n",
      "loss z =  11173978.0\n",
      "loss z =  11165483.0\n",
      "loss z =  11158794.0\n",
      "loss z =  11153286.0\n",
      "loss z =  11148480.0\n",
      "loss z =  11144121.0\n",
      "loss z =  11140076.0\n",
      "loss z =  11136276.0\n",
      "loss z =  11132647.0\n",
      "Epoch   630: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  11129166.0\n",
      "loss z =  11128490.0\n",
      "loss z =  11127817.0\n",
      "loss z =  11127148.0\n",
      "loss z =  11126482.0\n",
      "loss z =  11125821.0\n",
      "loss z =  11125165.0\n",
      "loss z =  11124511.0\n",
      "loss z =  11123860.0\n",
      "loss z =  11122568.0\n",
      "Epoch   641: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  11121925.0\n",
      "loss z =  11121796.0\n",
      "loss z =  11121669.0\n",
      "loss z =  11121542.0\n",
      "loss z =  11121413.0\n",
      "loss z =  11121285.0\n",
      "loss z =  11121158.0\n",
      "loss z =  11121030.0\n",
      "loss z =  11120903.0\n",
      "loss z =  11120777.0\n",
      "loss z =  11120648.0\n",
      "Epoch   652: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  11120524.0\n",
      "loss z =  11120498.0\n",
      "loss z =  11120472.0\n",
      "loss z =  11120447.0\n",
      "loss z =  11120421.0\n",
      "loss z =  11120397.0\n",
      "loss z =  11120371.0\n",
      "loss z =  11120321.0\n",
      "loss z =  11120295.0\n",
      "loss z =  11120270.0\n",
      "Epoch   663: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  23.78491973876953 s\n",
      "loss_f =  8340879.5\n",
      "Epoch  2630: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  8321734.0\n",
      "loss_f =  8315219.0\n",
      "loss_f =  8310501.0\n",
      "loss_f =  8306625.5\n",
      "loss_f =  8303316.0\n",
      "loss_f =  8300416.5\n",
      "Epoch  2661: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  8298104.0\n",
      "loss_f =  8296938.0\n",
      "loss_f =  8295837.5\n",
      "loss_f =  8294762.5\n",
      "loss_f =  8293731.0\n",
      "loss_f =  8292684.0\n",
      "Epoch  2692: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  8291661.0\n",
      "loss_f =  8291163.0\n",
      "loss_f =  8290664.0\n",
      "loss_f =  8290170.0\n",
      "loss_f =  8289669.0\n",
      "loss_f =  8289175.0\n",
      "loss_f =  8288683.0\n",
      "Epoch  2723: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  8288385.0\n",
      "loss_f =  8288139.0\n",
      "loss_f =  8287892.0\n",
      "loss_f =  8287648.5\n",
      "loss_f =  8287400.5\n",
      "loss_f =  8287150.5\n",
      "Epoch  2754: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  8286980.0\n",
      "loss_f =  8286855.0\n",
      "loss_f =  8286725.0\n",
      "loss_f =  8286605.0\n",
      "loss_f =  8286475.0\n",
      "loss_f =  8286354.5\n",
      "Epoch  2785: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  8286245.0\n",
      "loss_f =  8286188.5\n",
      "loss_f =  8286128.5\n",
      "loss_f =  8286071.5\n",
      "loss_f =  8286008.5\n",
      "loss_f =  8285939.5\n",
      "Epoch  2816: reducing learning rate of group 0 to 3.9063e-06.\n",
      "thetaf step =  131.68585681915283 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11070.322265625\n",
      "Epoch  3721: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3737: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3753: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3769: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6410.083984375\n",
      "Epoch  2943: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2959: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2975: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2991: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30090.802734375\n",
      "Epoch  9764: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9780: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9796: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9812: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.37890625\n",
      "Epoch  4804: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4820: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4836: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4852: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.8203125\n",
      "Epoch  5866: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5882: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5898: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5914: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37790.56640625\n",
      "Epoch  3004: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3020: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3036: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3052: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23241.44140625\n",
      "Epoch  3891: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3907: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3923: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3939: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14119.4267578125\n",
      "Epoch 13210: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13226: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13242: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13258: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2421.388671875\n",
      "Epoch  4331: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4347: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4363: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4379: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8158.8740234375\n",
      "Epoch  2838: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2854: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2870: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2886: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19098.208984375\n",
      "Epoch  4374: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4390: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4406: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4422: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10351.7646484375\n",
      "Epoch  9361: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9377: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9393: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9409: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9315.4501953125\n",
      "Epoch  4595: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4611: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4627: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4643: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8872.8017578125\n",
      "Epoch  5801: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5817: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5833: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5849: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15654.1591796875\n",
      "Epoch  2919: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2935: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2951: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2967: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1817.8223876953125\n",
      "Epoch  2523: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2539: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2555: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2571: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.149688959121704 s\n",
      "loss_c =  292.2744140625\n",
      "loss_c =  288.7452697753906\n",
      "loss_c =  288.0325927734375\n",
      "loss_c =  287.40826416015625\n",
      "loss_c =  286.8346252441406\n",
      "loss_c =  286.2936706542969\n",
      "loss_c =  285.7732238769531\n",
      "loss_c =  285.26446533203125\n",
      "loss_c =  284.76116943359375\n",
      "loss_c =  284.2585754394531\n",
      "thetac step =  3.9465107917785645 s\n",
      "step =  19\n",
      "loss z =  11086652.0\n",
      "loss z =  11082509.0\n",
      "loss z =  11078907.0\n",
      "loss z =  11075594.0\n",
      "loss z =  11072460.0\n",
      "loss z =  11069458.0\n",
      "loss z =  11066553.0\n",
      "loss z =  11063732.0\n",
      "loss z =  11061003.0\n",
      "loss z =  11058350.0\n",
      "Epoch   674: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  11055763.0\n",
      "loss z =  11055256.0\n",
      "loss z =  11054751.0\n",
      "loss z =  11054253.0\n",
      "loss z =  11053755.0\n",
      "loss z =  11053256.0\n",
      "loss z =  11052762.0\n",
      "loss z =  11052270.0\n",
      "loss z =  11051781.0\n",
      "loss z =  11050808.0\n",
      "Epoch   685: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  11050324.0\n",
      "loss z =  11050226.0\n",
      "loss z =  11050128.0\n",
      "loss z =  11050029.0\n",
      "loss z =  11049934.0\n",
      "loss z =  11049836.0\n",
      "loss z =  11049737.0\n",
      "loss z =  11049640.0\n",
      "loss z =  11049543.0\n",
      "loss z =  11049446.0\n",
      "loss z =  11049348.0\n",
      "Epoch   696: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  11049252.0\n",
      "loss z =  11049232.0\n",
      "loss z =  11049213.0\n",
      "loss z =  11049194.0\n",
      "loss z =  11049174.0\n",
      "loss z =  11049154.0\n",
      "loss z =  11049135.0\n",
      "loss z =  11049097.0\n",
      "loss z =  11049077.0\n",
      "loss z =  11049058.0\n",
      "Epoch   707: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.382497787475586 s\n",
      "loss_f =  8275092.0\n",
      "loss_f =  8257823.0\n",
      "loss_f =  8247925.0\n",
      "loss_f =  8239365.0\n",
      "loss_f =  8232493.0\n",
      "loss_f =  8227382.0\n",
      "loss_f =  8223249.5\n",
      "loss_f =  8219464.5\n",
      "loss_f =  8215855.5\n",
      "loss_f =  8212533.5\n",
      "loss_f =  8209411.0\n",
      "loss_f =  8206349.5\n",
      "loss_f =  8203365.5\n",
      "loss_f =  8200448.0\n",
      "loss_f =  8197595.0\n",
      "loss_f =  8194781.0\n",
      "loss_f =  8192037.0\n",
      "loss_f =  8189317.0\n",
      "loss_f =  8186641.5\n",
      "loss_f =  8184001.5\n",
      "loss_f =  8181377.0\n",
      "loss_f =  8178767.0\n",
      "loss_f =  8176184.5\n",
      "loss_f =  8173622.0\n",
      "loss_f =  8171098.0\n",
      "loss_f =  8168603.5\n",
      "loss_f =  8166135.0\n",
      "loss_f =  8163694.0\n",
      "loss_f =  8161276.5\n",
      "loss_f =  8158889.5\n",
      "loss_f =  8156526.0\n",
      "loss_f =  8154181.0\n",
      "loss_f =  8151861.0\n",
      "loss_f =  8149557.0\n",
      "loss_f =  8147279.0\n",
      "loss_f =  8145018.0\n",
      "loss_f =  8142784.0\n",
      "loss_f =  8140556.0\n",
      "loss_f =  8138360.5\n",
      "loss_f =  8136191.0\n",
      "thetaf step =  142.9236307144165 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11069.203125\n",
      "Epoch  3785: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3801: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3817: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3833: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6406.63525390625\n",
      "Epoch  3008: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3024: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3040: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3056: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30089.7578125\n",
      "Epoch  9828: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9844: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9860: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9876: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.2734375\n",
      "Epoch  4868: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4884: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4900: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4916: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.724609375\n",
      "Epoch  5930: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5946: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5962: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5978: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37790.17578125\n",
      "Epoch  3068: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3084: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3100: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3116: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23241.099609375\n",
      "Epoch  3955: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3971: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3987: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4003: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14119.2021484375\n",
      "Epoch 13274: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13290: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13306: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13322: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2420.494873046875\n",
      "Epoch  4396: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4412: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4428: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4444: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8153.90087890625\n",
      "Epoch  2902: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2918: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2934: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2950: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19097.365234375\n",
      "Epoch  4438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4454: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4470: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4486: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10347.298828125\n",
      "Epoch  9426: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9442: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9458: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9474: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9313.13671875\n",
      "Epoch  4659: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4675: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4691: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4707: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8869.3251953125\n",
      "Epoch  5866: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5882: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5898: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5914: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15643.1640625\n",
      "Epoch  2983: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2999: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3015: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3031: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.3697509765625\n",
      "Epoch  2587: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2603: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2619: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2635: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1469011306762695 s\n",
      "loss_c =  253.08541870117188\n",
      "Epoch 46520: reducing learning rate of group 0 to 2.0000e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_c =  252.50633239746094\n",
      "loss_c =  252.3580780029297\n",
      "loss_c =  252.20738220214844\n",
      "loss_c =  252.0531768798828\n",
      "loss_c =  251.8949432373047\n",
      "loss_c =  251.7328643798828\n",
      "loss_c =  251.56649780273438\n",
      "loss_c =  251.39593505859375\n",
      "loss_c =  251.22085571289062\n",
      "thetac step =  4.041189670562744 s\n",
      "Saving model...\n",
      "...saving done.\n",
      "step =  20\n",
      "loss z =  10990074.0\n",
      "loss z =  10983318.0\n",
      "loss z =  10977654.0\n",
      "loss z =  10972662.0\n",
      "loss z =  10968103.0\n",
      "loss z =  10963862.0\n",
      "loss z =  10959904.0\n",
      "loss z =  10956206.0\n",
      "loss z =  10952742.0\n",
      "loss z =  10949490.0\n",
      "loss z =  10946424.0\n",
      "Epoch   719: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10943503.0\n",
      "loss z =  10942942.0\n",
      "loss z =  10942385.0\n",
      "loss z =  10941833.0\n",
      "loss z =  10941286.0\n",
      "loss z =  10940742.0\n",
      "loss z =  10940204.0\n",
      "loss z =  10939669.0\n",
      "loss z =  10938602.0\n",
      "loss z =  10938075.0\n",
      "Epoch   730: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10937550.0\n",
      "loss z =  10937446.0\n",
      "loss z =  10937340.0\n",
      "loss z =  10937237.0\n",
      "loss z =  10937133.0\n",
      "loss z =  10937029.0\n",
      "loss z =  10936924.0\n",
      "loss z =  10936820.0\n",
      "loss z =  10936716.0\n",
      "loss z =  10936614.0\n",
      "loss z =  10936510.0\n",
      "Epoch   741: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10936407.0\n",
      "loss z =  10936385.0\n",
      "loss z =  10936365.0\n",
      "loss z =  10936345.0\n",
      "loss z =  10936325.0\n",
      "loss z =  10936304.0\n",
      "loss z =  10936263.0\n",
      "loss z =  10936242.0\n",
      "loss z =  10936221.0\n",
      "loss z =  10936202.0\n",
      "Epoch   752: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.718995332717896 s\n",
      "loss_f =  8167061.5\n",
      "loss_f =  8149959.0\n",
      "loss_f =  8140491.0\n",
      "loss_f =  8133150.0\n",
      "loss_f =  8127499.0\n",
      "loss_f =  8122902.5\n",
      "loss_f =  8118810.0\n",
      "loss_f =  8115139.0\n",
      "loss_f =  8111848.0\n",
      "loss_f =  8108831.5\n",
      "loss_f =  8105974.0\n",
      "loss_f =  8103239.0\n",
      "loss_f =  8100595.0\n",
      "loss_f =  8098002.0\n",
      "loss_f =  8095458.0\n",
      "loss_f =  8092957.0\n",
      "loss_f =  8090492.5\n",
      "loss_f =  8088063.0\n",
      "loss_f =  8085678.0\n",
      "loss_f =  8083307.0\n",
      "loss_f =  8080978.0\n",
      "loss_f =  8078676.0\n",
      "loss_f =  8076406.0\n",
      "loss_f =  8074154.5\n",
      "loss_f =  8071937.0\n",
      "loss_f =  8069737.5\n",
      "loss_f =  8067569.0\n",
      "loss_f =  8065424.0\n",
      "loss_f =  8063287.5\n",
      "loss_f =  8061188.0\n",
      "loss_f =  8059094.5\n",
      "loss_f =  8057032.0\n",
      "loss_f =  8054987.5\n",
      "loss_f =  8052952.0\n",
      "loss_f =  8050937.0\n",
      "loss_f =  8048944.0\n",
      "loss_f =  8046965.0\n",
      "loss_f =  8045010.0\n",
      "loss_f =  8043071.0\n",
      "loss_f =  8041142.0\n",
      "thetaf step =  137.67178511619568 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11069.544921875\n",
      "Epoch  3849: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3865: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3881: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3897: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6406.82763671875\n",
      "Epoch  3072: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3088: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3104: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3120: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30089.990234375\n",
      "Epoch  9892: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9908: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9924: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9940: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.30078125\n",
      "Epoch  4932: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4948: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4964: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4980: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.9609375\n",
      "Epoch  5994: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6010: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6026: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6042: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37790.27734375\n",
      "Epoch  3132: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3148: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3164: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3180: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23241.369140625\n",
      "Epoch  4019: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4035: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4051: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4067: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14119.626953125\n",
      "Epoch 13338: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13354: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13370: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13386: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2420.57861328125\n",
      "Epoch  4460: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4476: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4492: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4508: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8154.16552734375\n",
      "Epoch  2966: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2982: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2998: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3014: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19097.78125\n",
      "Epoch  4502: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4518: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4534: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4550: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10347.599609375\n",
      "Epoch  9490: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9506: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9522: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9538: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9313.6279296875\n",
      "Epoch  4723: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4739: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4755: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4771: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8869.591796875\n",
      "Epoch  5930: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5946: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5962: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5978: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15645.626953125\n",
      "Epoch  3047: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3063: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3079: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3095: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1814.4881591796875\n",
      "Epoch  2651: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2667: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2683: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2699: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.181650161743164 s\n",
      "loss_c =  260.3193359375\n",
      "loss_c =  259.0248718261719\n",
      "loss_c =  258.56744384765625\n",
      "loss_c =  258.1101379394531\n",
      "loss_c =  257.6444396972656\n",
      "loss_c =  257.1666564941406\n",
      "loss_c =  256.6735534667969\n",
      "loss_c =  256.16192626953125\n",
      "loss_c =  255.62852478027344\n",
      "loss_c =  255.07009887695312\n",
      "thetac step =  4.296735048294067 s\n",
      "step =  21\n",
      "loss z =  10902510.0\n",
      "loss z =  10896928.0\n",
      "loss z =  10892532.0\n",
      "loss z =  10888840.0\n",
      "loss z =  10885532.0\n",
      "loss z =  10882434.0\n",
      "loss z =  10879457.0\n",
      "loss z =  10876527.0\n",
      "loss z =  10873659.0\n",
      "loss z =  10870862.0\n",
      "loss z =  10868125.0\n",
      "Epoch   764: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10865451.0\n",
      "loss z =  10864926.0\n",
      "loss z =  10864403.0\n",
      "loss z =  10863885.0\n",
      "loss z =  10863368.0\n",
      "loss z =  10862851.0\n",
      "loss z =  10862340.0\n",
      "loss z =  10861826.0\n",
      "loss z =  10860811.0\n",
      "loss z =  10860307.0\n",
      "Epoch   775: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10859805.0\n",
      "loss z =  10859704.0\n",
      "loss z =  10859605.0\n",
      "loss z =  10859505.0\n",
      "loss z =  10859405.0\n",
      "loss z =  10859306.0\n",
      "loss z =  10859206.0\n",
      "loss z =  10859108.0\n",
      "loss z =  10859008.0\n",
      "loss z =  10858910.0\n",
      "loss z =  10858809.0\n",
      "Epoch   786: reducing learning rate of group 0 to 8.0000e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss z =  10858709.0\n",
      "loss z =  10858690.0\n",
      "loss z =  10858670.0\n",
      "loss z =  10858650.0\n",
      "loss z =  10858630.0\n",
      "loss z =  10858610.0\n",
      "loss z =  10858571.0\n",
      "loss z =  10858551.0\n",
      "loss z =  10858532.0\n",
      "loss z =  10858512.0\n",
      "Epoch   797: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.493196964263916 s\n",
      "loss_f =  8093798.5\n",
      "loss_f =  8078272.0\n",
      "loss_f =  8069469.0\n",
      "loss_f =  8063256.0\n",
      "loss_f =  8058440.0\n",
      "loss_f =  8054440.5\n",
      "loss_f =  8050999.0\n",
      "Epoch  3247: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  8049143.0\n",
      "loss_f =  8047709.0\n",
      "loss_f =  8046375.0\n",
      "loss_f =  8045086.0\n",
      "loss_f =  8043841.5\n",
      "loss_f =  8042615.0\n",
      "Epoch  3278: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  8041773.5\n",
      "loss_f =  8041180.0\n",
      "loss_f =  8040589.0\n",
      "loss_f =  8040009.0\n",
      "loss_f =  8039430.5\n",
      "loss_f =  8038853.0\n",
      "loss_f =  8038271.5\n",
      "loss_f =  8037695.5\n",
      "loss_f =  8037131.0\n",
      "loss_f =  8036560.0\n",
      "loss_f =  8035994.5\n",
      "loss_f =  8035427.5\n",
      "loss_f =  8034870.0\n",
      "loss_f =  8034297.5\n",
      "loss_f =  8033742.0\n",
      "loss_f =  8033186.5\n",
      "loss_f =  8032628.5\n",
      "loss_f =  8032075.0\n",
      "loss_f =  8031523.0\n",
      "loss_f =  8030968.5\n",
      "loss_f =  8030410.5\n",
      "loss_f =  8029859.0\n",
      "loss_f =  8029309.0\n",
      "loss_f =  8028758.5\n",
      "loss_f =  8028221.0\n",
      "loss_f =  8027675.0\n",
      "loss_f =  8027123.5\n",
      "thetaf step =  137.40947580337524 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11069.130859375\n",
      "Epoch  3913: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3929: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3945: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3961: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6406.80126953125\n",
      "Epoch  3136: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3152: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3168: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3184: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30090.015625\n",
      "Epoch  9956: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9972: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9988: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10004: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.3046875\n",
      "Epoch  4996: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5012: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5028: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5044: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.869140625\n",
      "Epoch  6058: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6074: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6090: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6106: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37790.078125\n",
      "Epoch  3196: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3228: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3244: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23241.21875\n",
      "Epoch  4083: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4099: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4115: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4131: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14119.5224609375\n",
      "Epoch 13402: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13418: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13434: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13450: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2420.60888671875\n",
      "Epoch  4524: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4540: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4556: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4572: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8153.87744140625\n",
      "Epoch  3030: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3046: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3062: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3078: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19097.564453125\n",
      "Epoch  4566: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4582: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4598: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4614: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10347.806640625\n",
      "Epoch  9554: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9570: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9586: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9602: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9313.0283203125\n",
      "Epoch  4787: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4803: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4819: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4835: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8869.55078125\n",
      "Epoch  5994: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6010: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6026: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6042: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15644.4814453125\n",
      "Epoch  3111: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3127: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3143: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3159: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.7403564453125\n",
      "Epoch  2715: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2731: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2747: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2763: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1508491039276123 s\n",
      "loss_c =  256.5595703125\n",
      "Epoch 47521: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  255.98231506347656\n",
      "loss_c =  255.85977172851562\n",
      "loss_c =  255.73558044433594\n",
      "loss_c =  255.60873413085938\n",
      "loss_c =  255.47879028320312\n",
      "loss_c =  255.34591674804688\n",
      "loss_c =  255.20965576171875\n",
      "loss_c =  255.0699462890625\n",
      "loss_c =  254.92662048339844\n",
      "thetac step =  3.9739229679107666 s\n",
      "step =  22\n",
      "loss z =  10856754.0\n",
      "loss z =  10851028.0\n",
      "loss z =  10846736.0\n",
      "loss z =  10843274.0\n",
      "loss z =  10840217.0\n",
      "loss z =  10837382.0\n",
      "loss z =  10834668.0\n",
      "loss z =  10832048.0\n",
      "loss z =  10829491.0\n",
      "loss z =  10826997.0\n",
      "loss z =  10824554.0\n",
      "Epoch   809: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10822156.0\n",
      "loss z =  10821686.0\n",
      "loss z =  10821218.0\n",
      "loss z =  10820750.0\n",
      "loss z =  10820286.0\n",
      "loss z =  10819822.0\n",
      "loss z =  10819359.0\n",
      "loss z =  10818901.0\n",
      "loss z =  10817987.0\n",
      "loss z =  10817533.0\n",
      "Epoch   820: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10817080.0\n",
      "loss z =  10816991.0\n",
      "loss z =  10816900.0\n",
      "loss z =  10816809.0\n",
      "loss z =  10816721.0\n",
      "loss z =  10816632.0\n",
      "loss z =  10816541.0\n",
      "loss z =  10816452.0\n",
      "loss z =  10816362.0\n",
      "loss z =  10816273.0\n",
      "loss z =  10816184.0\n",
      "Epoch   831: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10816095.0\n",
      "loss z =  10816077.0\n",
      "loss z =  10816060.0\n",
      "loss z =  10816042.0\n",
      "loss z =  10816023.0\n",
      "loss z =  10816006.0\n",
      "loss z =  10815970.0\n",
      "loss z =  10815952.0\n",
      "loss z =  10815934.0\n",
      "loss z =  10815916.0\n",
      "Epoch   842: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  25.314623832702637 s\n",
      "loss_f =  8055597.0\n",
      "loss_f =  8040611.0\n",
      "loss_f =  8032499.5\n",
      "loss_f =  8026369.0\n",
      "loss_f =  8021518.5\n",
      "loss_f =  8017494.5\n",
      "loss_f =  8013989.5\n",
      "loss_f =  8010926.0\n",
      "loss_f =  8008187.0\n",
      "loss_f =  8005653.5\n",
      "loss_f =  8003233.0\n",
      "loss_f =  8000896.0\n",
      "loss_f =  7998637.0\n",
      "loss_f =  7996419.0\n",
      "loss_f =  7994258.0\n",
      "loss_f =  7992140.0\n",
      "loss_f =  7990067.0\n",
      "loss_f =  7988012.0\n",
      "loss_f =  7986001.5\n",
      "loss_f =  7984026.5\n",
      "loss_f =  7982089.5\n",
      "loss_f =  7980174.0\n",
      "loss_f =  7978274.5\n",
      "loss_f =  7976400.0\n",
      "loss_f =  7974428.0\n",
      "loss_f =  7972649.0\n",
      "loss_f =  7970830.5\n",
      "loss_f =  7969024.0\n",
      "loss_f =  7967273.0\n",
      "loss_f =  7965524.5\n",
      "loss_f =  7963791.0\n",
      "loss_f =  7962082.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_f =  7960390.5\n",
      "loss_f =  7958718.0\n",
      "loss_f =  7957062.5\n",
      "loss_f =  7955408.5\n",
      "loss_f =  7953788.5\n",
      "loss_f =  7952176.5\n",
      "loss_f =  7950578.5\n",
      "loss_f =  7949000.0\n",
      "thetaf step =  139.29452753067017 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11069.1953125\n",
      "Epoch  3977: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3993: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4009: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4025: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6406.73095703125\n",
      "Epoch  3200: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3216: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3232: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3248: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30090.072265625\n",
      "Epoch 10020: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10036: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10052: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10068: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.3046875\n",
      "Epoch  5060: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5076: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5092: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5108: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.9375\n",
      "Epoch  6122: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6138: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6154: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6170: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37790.0703125\n",
      "Epoch  3260: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3276: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3292: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3308: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23241.3359375\n",
      "Epoch  4147: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4163: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4179: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4195: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14119.6611328125\n",
      "Epoch 13466: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13482: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13498: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13514: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2420.60546875\n",
      "Epoch  4588: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4604: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4620: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4636: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8153.7490234375\n",
      "Epoch  3094: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3110: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3126: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3142: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19097.73828125\n",
      "Epoch  4630: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4646: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4662: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4678: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10347.7177734375\n",
      "Epoch  9618: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9634: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9650: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9666: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9313.2138671875\n",
      "Epoch  4851: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4867: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4883: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4899: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8869.4873046875\n",
      "Epoch  6058: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6074: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6090: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6106: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15644.71484375\n",
      "Epoch  3175: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3191: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3207: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3223: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.9765625\n",
      "Epoch  2779: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2795: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2811: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2827: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.149165153503418 s\n",
      "loss_c =  257.47344970703125\n",
      "loss_c =  255.81033325195312\n",
      "loss_c =  255.18563842773438\n",
      "loss_c =  254.5348663330078\n",
      "loss_c =  253.84405517578125\n",
      "loss_c =  253.10433959960938\n",
      "loss_c =  252.30609130859375\n",
      "loss_c =  251.4351806640625\n",
      "loss_c =  250.3185577392578\n",
      "loss_c =  245.9057159423828\n",
      "thetac step =  4.538448095321655 s\n",
      "step =  23\n",
      "loss z =  10764854.0\n",
      "loss z =  10758248.0\n",
      "loss z =  10753760.0\n",
      "loss z =  10750263.0\n",
      "loss z =  10747250.0\n",
      "loss z =  10744492.0\n",
      "loss z =  10741892.0\n",
      "loss z =  10739398.0\n",
      "loss z =  10736999.0\n",
      "loss z =  10734670.0\n",
      "loss z =  10732410.0\n",
      "Epoch   854: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10730206.0\n",
      "loss z =  10729774.0\n",
      "loss z =  10729344.0\n",
      "loss z =  10728916.0\n",
      "loss z =  10728492.0\n",
      "loss z =  10728065.0\n",
      "loss z =  10727645.0\n",
      "loss z =  10727224.0\n",
      "loss z =  10726392.0\n",
      "loss z =  10725977.0\n",
      "Epoch   865: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10725565.0\n",
      "loss z =  10725484.0\n",
      "loss z =  10725401.0\n",
      "loss z =  10725320.0\n",
      "loss z =  10725239.0\n",
      "loss z =  10725157.0\n",
      "loss z =  10725074.0\n",
      "loss z =  10724995.0\n",
      "loss z =  10724914.0\n",
      "loss z =  10724832.0\n",
      "loss z =  10724752.0\n",
      "Epoch   876: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10724670.0\n",
      "loss z =  10724654.0\n",
      "loss z =  10724638.0\n",
      "loss z =  10724622.0\n",
      "loss z =  10724606.0\n",
      "loss z =  10724590.0\n",
      "loss z =  10724558.0\n",
      "loss z =  10724542.0\n",
      "loss z =  10724526.0\n",
      "loss z =  10724509.0\n",
      "Epoch   887: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  25.40431571006775 s\n",
      "loss_f =  7968919.0\n",
      "loss_f =  7956265.0\n",
      "loss_f =  7949081.0\n",
      "loss_f =  7943741.5\n",
      "loss_f =  7939651.0\n",
      "loss_f =  7936351.0\n",
      "loss_f =  7933476.0\n",
      "loss_f =  7930927.5\n",
      "loss_f =  7928649.5\n",
      "loss_f =  7926544.0\n",
      "loss_f =  7924554.0\n",
      "loss_f =  7922633.0\n",
      "loss_f =  7920766.5\n",
      "loss_f =  7918939.0\n",
      "loss_f =  7917161.0\n",
      "loss_f =  7915415.0\n",
      "loss_f =  7913692.0\n",
      "loss_f =  7911988.0\n",
      "loss_f =  7910331.0\n",
      "loss_f =  7908676.0\n",
      "loss_f =  7907055.5\n",
      "loss_f =  7905448.5\n",
      "loss_f =  7903869.5\n",
      "loss_f =  7902296.0\n",
      "loss_f =  7900748.5\n",
      "loss_f =  7899213.0\n",
      "loss_f =  7897711.0\n",
      "loss_f =  7896218.0\n",
      "loss_f =  7894734.5\n",
      "loss_f =  7893273.0\n",
      "loss_f =  7891818.0\n",
      "loss_f =  7890391.0\n",
      "loss_f =  7888960.5\n",
      "loss_f =  7887556.0\n",
      "loss_f =  7886163.0\n",
      "loss_f =  7884775.5\n",
      "loss_f =  7883406.0\n",
      "loss_f =  7882049.5\n",
      "loss_f =  7880703.0\n",
      "loss_f =  7879363.5\n",
      "thetaf step =  138.55661249160767 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11070.1611328125\n",
      "Epoch  4041: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4057: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4073: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4089: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6407.76513671875\n",
      "Epoch  3264: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3280: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3296: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3312: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30090.923828125\n",
      "Epoch 10084: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10100: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10116: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10132: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.171875\n",
      "Epoch  5124: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5140: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5156: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5172: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.78515625\n",
      "Epoch  6186: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6202: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6218: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6234: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37789.890625\n",
      "Epoch  3324: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3340: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3356: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3372: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23241.76953125\n",
      "Epoch  4211: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4227: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4243: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4259: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14120.333984375\n",
      "Epoch 13530: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13546: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13562: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13578: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2420.864013671875\n",
      "Epoch  4652: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4668: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4684: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4700: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8155.19580078125\n",
      "Epoch  3158: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3174: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3190: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3206: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19098.001953125\n",
      "Epoch  4694: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4710: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4726: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4742: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10350.29296875\n",
      "Epoch  9682: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9698: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9714: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9730: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9305.7705078125\n",
      "Epoch  4916: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4932: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4948: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4964: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8870.5869140625\n",
      "Epoch  6122: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6138: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6154: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6170: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15636.1328125\n",
      "Epoch  3239: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3255: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3271: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3287: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1814.4073486328125\n",
      "Epoch  2843: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2859: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2875: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2891: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1287147998809814 s\n",
      "loss_c =  251.01304626464844\n",
      "Epoch 48522: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  249.33187866210938\n",
      "loss_c =  249.07913208007812\n",
      "loss_c =  248.8308563232422\n",
      "loss_c =  248.58226013183594\n",
      "loss_c =  248.33319091796875\n",
      "loss_c =  248.0820770263672\n",
      "loss_c =  247.8287811279297\n",
      "loss_c =  247.5719451904297\n",
      "loss_c =  247.31130981445312\n",
      "thetac step =  4.573309898376465 s\n",
      "step =  24\n",
      "loss z =  10709922.0\n",
      "loss z =  10704386.0\n",
      "loss z =  10700262.0\n",
      "loss z =  10696866.0\n",
      "loss z =  10693830.0\n",
      "loss z =  10690971.0\n",
      "loss z =  10688218.0\n",
      "loss z =  10685553.0\n",
      "loss z =  10682956.0\n",
      "loss z =  10680424.0\n",
      "loss z =  10677956.0\n",
      "Epoch   899: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10675506.0\n",
      "loss z =  10675034.0\n",
      "loss z =  10674561.0\n",
      "loss z =  10674091.0\n",
      "loss z =  10673622.0\n",
      "loss z =  10673157.0\n",
      "loss z =  10672692.0\n",
      "loss z =  10672230.0\n",
      "loss z =  10671313.0\n",
      "loss z =  10670858.0\n",
      "Epoch   910: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10670404.0\n",
      "loss z =  10670313.0\n",
      "loss z =  10670222.0\n",
      "loss z =  10670132.0\n",
      "loss z =  10670042.0\n",
      "loss z =  10669953.0\n",
      "loss z =  10669862.0\n",
      "loss z =  10669772.0\n",
      "loss z =  10669681.0\n",
      "loss z =  10669591.0\n",
      "loss z =  10669500.0\n",
      "Epoch   921: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10669408.0\n",
      "loss z =  10669390.0\n",
      "loss z =  10669373.0\n",
      "loss z =  10669354.0\n",
      "loss z =  10669338.0\n",
      "loss z =  10669320.0\n",
      "loss z =  10669285.0\n",
      "loss z =  10669266.0\n",
      "loss z =  10669250.0\n",
      "loss z =  10669232.0\n",
      "Epoch   932: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.725996494293213 s\n",
      "loss_f =  7917838.5\n",
      "loss_f =  7904133.0\n",
      "loss_f =  7896564.0\n",
      "loss_f =  7890219.0\n",
      "loss_f =  7885660.0\n",
      "loss_f =  7882026.0\n",
      "Epoch  3846: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7878808.0\n",
      "loss_f =  7877410.0\n",
      "loss_f =  7876137.0\n",
      "loss_f =  7874966.0\n",
      "loss_f =  7873873.0\n",
      "loss_f =  7872813.0\n",
      "loss_f =  7871799.5\n",
      "loss_f =  7870805.0\n",
      "loss_f =  7869840.0\n",
      "loss_f =  7868884.5\n",
      "loss_f =  7867942.0\n",
      "loss_f =  7867017.0\n",
      "loss_f =  7866102.0\n",
      "loss_f =  7865198.5\n",
      "loss_f =  7864297.0\n",
      "loss_f =  7863412.0\n",
      "loss_f =  7862529.0\n",
      "loss_f =  7861659.5\n",
      "loss_f =  7860794.0\n",
      "loss_f =  7859930.0\n",
      "loss_f =  7859078.0\n",
      "loss_f =  7858239.5\n",
      "loss_f =  7857388.5\n",
      "loss_f =  7856555.5\n",
      "loss_f =  7855732.0\n",
      "loss_f =  7854897.0\n",
      "loss_f =  7854079.0\n",
      "loss_f =  7853273.5\n",
      "loss_f =  7852463.0\n",
      "loss_f =  7851660.0\n",
      "loss_f =  7850864.5\n",
      "loss_f =  7850064.5\n",
      "loss_f =  7849275.0\n",
      "loss_f =  7848480.0\n",
      "thetaf step =  138.16653037071228 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11071.2392578125\n",
      "Epoch  4105: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4121: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4137: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4153: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6408.16455078125\n",
      "Epoch  3328: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3344: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3360: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3376: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30091.6171875\n",
      "Epoch 10148: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10164: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10180: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10196: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.15625\n",
      "Epoch  5188: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5204: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5220: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5236: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24293.083984375\n",
      "Epoch  6250: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6266: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6282: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6298: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37789.96875\n",
      "Epoch  3388: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3404: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3420: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3436: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23242.421875\n",
      "Epoch  4275: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4291: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4307: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4323: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14121.1318359375\n",
      "Epoch 13594: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13610: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13626: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13642: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2420.979736328125\n",
      "Epoch  4716: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4732: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4748: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4764: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8156.03271484375\n",
      "Epoch  3222: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3238: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3254: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3270: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19098.72265625\n",
      "Epoch  4758: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4774: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4790: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4806: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10351.2158203125\n",
      "Epoch  9746: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9762: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9778: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9794: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9305.62109375\n",
      "Epoch  4980: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4996: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5012: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5028: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8870.927734375\n",
      "Epoch  6186: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6202: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6218: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6234: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15622.1513671875\n",
      "Epoch  3303: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3319: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3335: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3351: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.604736328125\n",
      "Epoch  2907: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2923: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  2939: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  2955: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.15301513671875 s\n",
      "loss_c =  243.00064086914062\n",
      "loss_c =  241.32769775390625\n",
      "loss_c =  240.57325744628906\n",
      "loss_c =  239.47425842285156\n",
      "loss_c =  237.58462524414062\n",
      "loss_c =  234.81951904296875\n",
      "loss_c =  232.68634033203125\n",
      "loss_c =  231.47853088378906\n",
      "loss_c =  230.6976776123047\n",
      "loss_c =  230.09771728515625\n",
      "thetac step =  4.806073188781738 s\n",
      "step =  25\n",
      "loss z =  10668323.0\n",
      "loss z =  10662085.0\n",
      "loss z =  10657089.0\n",
      "loss z =  10652838.0\n",
      "loss z =  10649045.0\n",
      "loss z =  10645586.0\n",
      "loss z =  10642362.0\n",
      "loss z =  10639294.0\n",
      "loss z =  10636404.0\n",
      "loss z =  10633660.0\n",
      "loss z =  10631018.0\n",
      "Epoch   944: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10628493.0\n",
      "loss z =  10628002.0\n",
      "loss z =  10627516.0\n",
      "loss z =  10627035.0\n",
      "loss z =  10626554.0\n",
      "loss z =  10626080.0\n",
      "loss z =  10625609.0\n",
      "loss z =  10625142.0\n",
      "loss z =  10624212.0\n",
      "loss z =  10623753.0\n",
      "Epoch   955: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10623297.0\n",
      "loss z =  10623205.0\n",
      "loss z =  10623114.0\n",
      "loss z =  10623022.0\n",
      "loss z =  10622932.0\n",
      "loss z =  10622842.0\n",
      "loss z =  10622751.0\n",
      "loss z =  10622660.0\n",
      "loss z =  10622569.0\n",
      "loss z =  10622479.0\n",
      "loss z =  10622391.0\n",
      "Epoch   966: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10622301.0\n",
      "loss z =  10622283.0\n",
      "loss z =  10622265.0\n",
      "loss z =  10622247.0\n",
      "loss z =  10622230.0\n",
      "loss z =  10622211.0\n",
      "loss z =  10622176.0\n",
      "loss z =  10622158.0\n",
      "loss z =  10622140.0\n",
      "loss z =  10622122.0\n",
      "Epoch   977: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.691476345062256 s\n",
      "loss_f =  7875181.0\n",
      "loss_f =  7861942.0\n",
      "loss_f =  7854985.0\n",
      "loss_f =  7849630.0\n",
      "loss_f =  7845543.0\n",
      "loss_f =  7842051.0\n",
      "loss_f =  7839065.0\n",
      "loss_f =  7836557.0\n",
      "loss_f =  7834321.5\n",
      "loss_f =  7832288.0\n",
      "loss_f =  7830401.0\n",
      "loss_f =  7828578.5\n",
      "loss_f =  7826833.0\n",
      "loss_f =  7825147.0\n",
      "loss_f =  7823484.0\n",
      "loss_f =  7821878.0\n",
      "loss_f =  7820287.0\n",
      "loss_f =  7818733.0\n",
      "loss_f =  7817214.0\n",
      "loss_f =  7815702.5\n",
      "loss_f =  7814225.0\n",
      "loss_f =  7812772.0\n",
      "loss_f =  7811338.0\n",
      "loss_f =  7809927.0\n",
      "loss_f =  7808524.0\n",
      "loss_f =  7807149.5\n",
      "loss_f =  7805787.5\n",
      "loss_f =  7804447.5\n",
      "loss_f =  7803116.0\n",
      "loss_f =  7801802.0\n",
      "loss_f =  7800509.0\n",
      "loss_f =  7799232.5\n",
      "loss_f =  7797962.0\n",
      "loss_f =  7796708.0\n",
      "loss_f =  7795468.0\n",
      "loss_f =  7794234.0\n",
      "loss_f =  7793024.0\n",
      "loss_f =  7791814.0\n",
      "loss_f =  7790629.0\n",
      "loss_f =  7789444.0\n",
      "thetaf step =  140.01881098747253 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11073.5068359375\n",
      "Epoch  4169: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4185: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4201: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4217: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6409.7041015625\n",
      "Epoch  3392: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3408: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3424: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3440: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30094.08203125\n",
      "Epoch 10212: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10228: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10244: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10260: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.08984375\n",
      "Epoch  5252: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5268: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5284: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5300: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24293.982421875\n",
      "Epoch  6314: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6330: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6346: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6362: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37790.47265625\n",
      "Epoch  3452: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3468: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3484: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3500: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23244.123046875\n",
      "Epoch  4339: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4355: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4371: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4387: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.931640625\n",
      "Epoch 13658: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13674: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13690: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13706: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2421.563720703125\n",
      "Epoch  4780: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4796: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4812: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4828: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8158.34375\n",
      "Epoch  3286: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3302: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3318: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3334: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19101.20703125\n",
      "Epoch  4822: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4838: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4854: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4870: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10353.3984375\n",
      "Epoch  9810: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9826: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9842: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9858: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9306.3623046875\n",
      "Epoch  5044: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5060: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5076: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5092: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8872.76953125\n",
      "Epoch  6250: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6266: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6282: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6298: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15627.677734375\n",
      "Epoch  3367: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3383: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3399: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3415: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1812.939453125\n",
      "Epoch  2972: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  2988: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3004: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3020: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1854512691497803 s\n",
      "loss_c =  261.1159362792969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49523: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  258.9169006347656\n",
      "loss_c =  258.73883056640625\n",
      "loss_c =  258.5659484863281\n",
      "loss_c =  258.3948059082031\n",
      "loss_c =  258.2250061035156\n",
      "loss_c =  258.055908203125\n",
      "loss_c =  257.8868103027344\n",
      "loss_c =  257.7181396484375\n",
      "loss_c =  257.54901123046875\n",
      "thetac step =  4.991853475570679 s\n",
      "step =  26\n",
      "loss z =  10607161.0\n",
      "loss z =  10602578.0\n",
      "loss z =  10599052.0\n",
      "loss z =  10596082.0\n",
      "loss z =  10593371.0\n",
      "loss z =  10590794.0\n",
      "loss z =  10588300.0\n",
      "loss z =  10585867.0\n",
      "loss z =  10583486.0\n",
      "loss z =  10581156.0\n",
      "loss z =  10578869.0\n",
      "Epoch   989: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10576627.0\n",
      "loss z =  10576184.0\n",
      "loss z =  10575745.0\n",
      "loss z =  10575307.0\n",
      "loss z =  10574869.0\n",
      "loss z =  10574432.0\n",
      "loss z =  10574001.0\n",
      "loss z =  10573571.0\n",
      "loss z =  10572717.0\n",
      "loss z =  10572291.0\n",
      "Epoch  1000: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10571866.0\n",
      "loss z =  10571782.0\n",
      "loss z =  10571697.0\n",
      "loss z =  10571612.0\n",
      "loss z =  10571527.0\n",
      "loss z =  10571444.0\n",
      "loss z =  10571360.0\n",
      "loss z =  10571277.0\n",
      "loss z =  10571192.0\n",
      "loss z =  10571108.0\n",
      "loss z =  10571024.0\n",
      "Epoch  1011: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10570939.0\n",
      "loss z =  10570922.0\n",
      "loss z =  10570906.0\n",
      "loss z =  10570888.0\n",
      "loss z =  10570872.0\n",
      "loss z =  10570855.0\n",
      "loss z =  10570822.0\n",
      "loss z =  10570806.0\n",
      "loss z =  10570789.0\n",
      "loss z =  10570774.0\n",
      "Epoch  1022: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  26.94624352455139 s\n",
      "loss_f =  7828142.0\n",
      "loss_f =  7814566.0\n",
      "loss_f =  7807123.5\n",
      "loss_f =  7801190.0\n",
      "loss_f =  7796998.0\n",
      "loss_f =  7793326.0\n",
      "loss_f =  7790174.5\n",
      "Epoch  4247: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7788646.0\n",
      "loss_f =  7787509.0\n",
      "loss_f =  7786435.5\n",
      "loss_f =  7785424.0\n",
      "loss_f =  7784469.5\n",
      "loss_f =  7783531.0\n",
      "loss_f =  7782624.5\n",
      "loss_f =  7781732.0\n",
      "loss_f =  7780863.5\n",
      "loss_f =  7780005.0\n",
      "loss_f =  7779146.0\n",
      "loss_f =  7778305.0\n",
      "loss_f =  7777466.5\n",
      "loss_f =  7776649.5\n",
      "loss_f =  7775833.5\n",
      "loss_f =  7775032.0\n",
      "loss_f =  7774228.0\n",
      "loss_f =  7773438.0\n",
      "loss_f =  7772646.0\n",
      "loss_f =  7771876.0\n",
      "loss_f =  7771102.0\n",
      "loss_f =  7770338.0\n",
      "loss_f =  7769578.0\n",
      "loss_f =  7768813.0\n",
      "loss_f =  7768079.0\n",
      "loss_f =  7767331.0\n",
      "loss_f =  7766595.0\n",
      "loss_f =  7765865.0\n",
      "loss_f =  7765137.0\n",
      "loss_f =  7764412.0\n",
      "loss_f =  7763685.5\n",
      "loss_f =  7762986.0\n",
      "loss_f =  7762272.0\n",
      "thetaf step =  139.40174984931946 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11073.552734375\n",
      "Epoch  4233: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4249: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4265: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4281: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6409.2744140625\n",
      "Epoch  3456: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3488: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3504: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30094.076171875\n",
      "Epoch 10276: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10292: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10308: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10324: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.046875\n",
      "Epoch  5316: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5332: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5348: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5364: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24294.02734375\n",
      "Epoch  6378: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6394: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6410: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6426: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37790.48828125\n",
      "Epoch  3516: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3532: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3548: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3564: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23244.228515625\n",
      "Epoch  4403: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4419: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4435: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4451: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.876953125\n",
      "Epoch 13722: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13738: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13754: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13770: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2421.478759765625\n",
      "Epoch  4844: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4860: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4876: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4892: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8158.0791015625\n",
      "Epoch  3350: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3366: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3382: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3398: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19101.18359375\n",
      "Epoch  4886: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4902: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4918: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4934: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10352.8330078125\n",
      "Epoch  9874: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9890: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9906: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9922: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9306.1015625\n",
      "Epoch  5108: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5124: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5140: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5156: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8872.3125\n",
      "Epoch  6314: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6330: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6346: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6362: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15623.150390625\n",
      "Epoch  3431: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3447: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3463: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3479: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1812.7532958984375\n",
      "Epoch  3037: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3053: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3069: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3085: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1433601379394531 s\n",
      "loss_c =  254.42208862304688\n",
      "loss_c =  252.69277954101562\n",
      "loss_c =  252.215576171875\n",
      "loss_c =  251.76580810546875\n",
      "loss_c =  251.32473754882812\n",
      "loss_c =  250.09490966796875\n",
      "loss_c =  246.57470703125\n",
      "loss_c =  245.400634765625\n",
      "loss_c =  244.49490356445312\n",
      "loss_c =  243.69276428222656\n",
      "thetac step =  4.9567553997039795 s\n",
      "step =  27\n",
      "loss z =  10561274.0\n",
      "loss z =  10557387.0\n",
      "loss z =  10554241.0\n",
      "loss z =  10551476.0\n",
      "loss z =  10548866.0\n",
      "loss z =  10546363.0\n",
      "loss z =  10543906.0\n",
      "loss z =  10541449.0\n",
      "loss z =  10539053.0\n",
      "loss z =  10536718.0\n",
      "loss z =  10534443.0\n",
      "Epoch  1034: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10532216.0\n",
      "loss z =  10531780.0\n",
      "loss z =  10531342.0\n",
      "loss z =  10530909.0\n",
      "loss z =  10530478.0\n",
      "loss z =  10530049.0\n",
      "loss z =  10529621.0\n",
      "loss z =  10529195.0\n",
      "loss z =  10528343.0\n",
      "loss z =  10527921.0\n",
      "Epoch  1045: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10527502.0\n",
      "loss z =  10527420.0\n",
      "loss z =  10527336.0\n",
      "loss z =  10527253.0\n",
      "loss z =  10527169.0\n",
      "loss z =  10527086.0\n",
      "loss z =  10527003.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss z =  10526920.0\n",
      "loss z =  10526836.0\n",
      "loss z =  10526753.0\n",
      "loss z =  10526669.0\n",
      "Epoch  1056: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10526586.0\n",
      "loss z =  10526568.0\n",
      "loss z =  10526552.0\n",
      "loss z =  10526536.0\n",
      "loss z =  10526518.0\n",
      "loss z =  10526503.0\n",
      "loss z =  10526468.0\n",
      "loss z =  10526453.0\n",
      "loss z =  10526436.0\n",
      "loss z =  10526421.0\n",
      "Epoch  1067: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.822913885116577 s\n",
      "loss_f =  7788205.0\n",
      "loss_f =  7776305.0\n",
      "loss_f =  7769732.0\n",
      "loss_f =  7764244.0\n",
      "loss_f =  7760338.0\n",
      "loss_f =  7757125.0\n",
      "loss_f =  7754357.0\n",
      "loss_f =  7752077.0\n",
      "loss_f =  7750088.0\n",
      "loss_f =  7748246.5\n",
      "loss_f =  7746536.0\n",
      "loss_f =  7744890.0\n",
      "loss_f =  7743312.0\n",
      "loss_f =  7741788.5\n",
      "loss_f =  7740299.5\n",
      "loss_f =  7738843.5\n",
      "loss_f =  7737422.0\n",
      "loss_f =  7736022.0\n",
      "loss_f =  7734661.5\n",
      "loss_f =  7733319.0\n",
      "loss_f =  7731997.0\n",
      "loss_f =  7730697.0\n",
      "loss_f =  7729419.5\n",
      "loss_f =  7728157.5\n",
      "loss_f =  7726918.0\n",
      "loss_f =  7725694.0\n",
      "loss_f =  7724489.5\n",
      "loss_f =  7723298.0\n",
      "loss_f =  7722120.0\n",
      "loss_f =  7720965.0\n",
      "loss_f =  7719826.0\n",
      "loss_f =  7718695.0\n",
      "loss_f =  7717575.5\n",
      "loss_f =  7716466.0\n",
      "loss_f =  7715382.0\n",
      "loss_f =  7714303.0\n",
      "loss_f =  7713239.5\n",
      "loss_f =  7712186.0\n",
      "loss_f =  7711137.0\n",
      "loss_f =  7710112.0\n",
      "thetaf step =  139.74343514442444 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11073.873046875\n",
      "Epoch  4297: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4313: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4329: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4345: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6410.4453125\n",
      "Epoch  3520: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3536: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3552: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3568: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30094.84375\n",
      "Epoch 10340: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10356: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10372: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10388: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.27734375\n",
      "Epoch  5380: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5396: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5412: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5428: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24294.185546875\n",
      "Epoch  6442: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6458: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6474: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6490: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37790.6796875\n",
      "Epoch  3580: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3596: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3612: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3628: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23244.626953125\n",
      "Epoch  4467: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4483: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4499: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4515: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.8388671875\n",
      "Epoch 13786: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13802: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13818: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13834: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2421.900634765625\n",
      "Epoch  4908: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4924: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4940: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4956: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8152.5625\n",
      "Epoch  3414: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3430: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3446: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3462: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19101.701171875\n",
      "Epoch  4950: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4966: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4982: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4998: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10354.2490234375\n",
      "Epoch  9938: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9954: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9970: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9986: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9305.84375\n",
      "Epoch  5172: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5188: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5204: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5220: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8874.095703125\n",
      "Epoch  6378: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6394: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6410: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6426: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15624.28125\n",
      "Epoch  3495: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3511: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3527: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3543: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1812.8599853515625\n",
      "Epoch  3101: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3117: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3133: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3149: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1530728340148926 s\n",
      "loss_c =  257.2217102050781\n",
      "Epoch 50524: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  256.2244873046875\n",
      "loss_c =  256.10986328125\n",
      "loss_c =  255.99668884277344\n",
      "loss_c =  255.8835906982422\n",
      "loss_c =  255.76963806152344\n",
      "loss_c =  255.65493774414062\n",
      "loss_c =  255.53887939453125\n",
      "loss_c =  255.42178344726562\n",
      "loss_c =  255.30307006835938\n",
      "thetac step =  5.257880449295044 s\n",
      "step =  28\n",
      "loss z =  10523356.0\n",
      "loss z =  10518196.0\n",
      "loss z =  10514001.0\n",
      "loss z =  10510349.0\n",
      "loss z =  10507031.0\n",
      "loss z =  10503903.0\n",
      "loss z =  10500912.0\n",
      "loss z =  10498032.0\n",
      "loss z =  10495244.0\n",
      "loss z =  10492481.0\n",
      "loss z =  10489798.0\n",
      "Epoch  1079: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10487205.0\n",
      "loss z =  10486697.0\n",
      "loss z =  10486194.0\n",
      "loss z =  10485693.0\n",
      "loss z =  10485194.0\n",
      "loss z =  10484699.0\n",
      "loss z =  10484206.0\n",
      "loss z =  10483716.0\n",
      "loss z =  10482745.0\n",
      "loss z =  10482263.0\n",
      "Epoch  1090: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10481782.0\n",
      "loss z =  10481686.0\n",
      "loss z =  10481591.0\n",
      "loss z =  10481496.0\n",
      "loss z =  10481401.0\n",
      "loss z =  10481307.0\n",
      "loss z =  10481212.0\n",
      "loss z =  10481117.0\n",
      "loss z =  10481022.0\n",
      "loss z =  10480926.0\n",
      "loss z =  10480833.0\n",
      "Epoch  1101: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10480738.0\n",
      "loss z =  10480719.0\n",
      "loss z =  10480700.0\n",
      "loss z =  10480682.0\n",
      "loss z =  10480662.0\n",
      "loss z =  10480644.0\n",
      "loss z =  10480606.0\n",
      "loss z =  10480587.0\n",
      "loss z =  10480568.0\n",
      "loss z =  10480550.0\n",
      "Epoch  1112: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  25.217615604400635 s\n",
      "loss_f =  7746609.0\n",
      "loss_f =  7733383.0\n",
      "loss_f =  7726555.0\n",
      "loss_f =  7720939.0\n",
      "loss_f =  7716614.5\n",
      "loss_f =  7712944.0\n",
      "Epoch  4646: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7710067.0\n",
      "loss_f =  7708879.0\n",
      "loss_f =  7707766.0\n",
      "loss_f =  7706750.0\n",
      "loss_f =  7705809.5\n",
      "loss_f =  7704905.0\n",
      "loss_f =  7704039.0\n",
      "loss_f =  7703194.5\n",
      "loss_f =  7702365.5\n",
      "loss_f =  7701560.0\n",
      "loss_f =  7700762.0\n",
      "loss_f =  7699969.0\n",
      "loss_f =  7699197.0\n",
      "loss_f =  7698430.5\n",
      "loss_f =  7697671.0\n",
      "loss_f =  7696935.0\n",
      "loss_f =  7696189.0\n",
      "loss_f =  7695457.0\n",
      "loss_f =  7694729.0\n",
      "loss_f =  7694020.5\n",
      "loss_f =  7693310.5\n",
      "loss_f =  7692604.0\n",
      "loss_f =  7691909.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_f =  7691216.0\n",
      "loss_f =  7690532.0\n",
      "loss_f =  7689856.0\n",
      "loss_f =  7689185.0\n",
      "loss_f =  7688522.0\n",
      "loss_f =  7687863.0\n",
      "loss_f =  7687205.0\n",
      "loss_f =  7686550.0\n",
      "loss_f =  7685910.0\n",
      "loss_f =  7685268.5\n",
      "loss_f =  7684633.5\n",
      "thetaf step =  140.00559329986572 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11073.9228515625\n",
      "Epoch  4361: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4377: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4393: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4409: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6410.36474609375\n",
      "Epoch  3584: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3600: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3616: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3632: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30094.87890625\n",
      "Epoch 10404: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10420: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10436: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10452: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.26953125\n",
      "Epoch  5444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5460: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5476: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5492: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24293.9921875\n",
      "Epoch  6506: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6522: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6538: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6554: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37790.68359375\n",
      "Epoch  3644: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3660: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3676: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3692: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23244.705078125\n",
      "Epoch  4531: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4547: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4563: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4579: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.888671875\n",
      "Epoch 13850: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13866: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13882: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13898: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2421.89892578125\n",
      "Epoch  4972: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4988: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5004: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5020: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8151.947265625\n",
      "Epoch  3478: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3494: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3510: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3526: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19101.744140625\n",
      "Epoch  5014: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5030: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5046: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5062: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10354.1318359375\n",
      "Epoch 10002: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10018: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10034: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10050: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9305.697265625\n",
      "Epoch  5236: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5252: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5268: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5284: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8874.0390625\n",
      "Epoch  6442: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6458: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6474: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6490: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15624.255859375\n",
      "Epoch  3559: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3575: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3591: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3607: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1812.8388671875\n",
      "Epoch  3165: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3181: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3197: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3213: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1719145774841309 s\n",
      "loss_c =  256.22052001953125\n",
      "loss_c =  254.62994384765625\n",
      "loss_c =  254.11500549316406\n",
      "loss_c =  253.6531982421875\n",
      "loss_c =  253.19027709960938\n",
      "loss_c =  252.72409057617188\n",
      "loss_c =  252.25343322753906\n",
      "loss_c =  251.77755737304688\n",
      "loss_c =  251.2954864501953\n",
      "loss_c =  250.8064422607422\n",
      "thetac step =  5.393234729766846 s\n",
      "step =  29\n",
      "loss z =  10497952.0\n",
      "loss z =  10493239.0\n",
      "loss z =  10489274.0\n",
      "loss z =  10485800.0\n",
      "loss z =  10482600.0\n",
      "loss z =  10479567.0\n",
      "loss z =  10476666.0\n",
      "loss z =  10473861.0\n",
      "loss z =  10471154.0\n",
      "loss z =  10468537.0\n",
      "loss z =  10465993.0\n",
      "Epoch  1124: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10463518.0\n",
      "loss z =  10463034.0\n",
      "loss z =  10462550.0\n",
      "loss z =  10462071.0\n",
      "loss z =  10461596.0\n",
      "loss z =  10461121.0\n",
      "loss z =  10460647.0\n",
      "loss z =  10460178.0\n",
      "loss z =  10459244.0\n",
      "loss z =  10458782.0\n",
      "Epoch  1135: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10458320.0\n",
      "loss z =  10458228.0\n",
      "loss z =  10458136.0\n",
      "loss z =  10458044.0\n",
      "loss z =  10457952.0\n",
      "loss z =  10457861.0\n",
      "loss z =  10457769.0\n",
      "loss z =  10457680.0\n",
      "loss z =  10457589.0\n",
      "loss z =  10457497.0\n",
      "loss z =  10457407.0\n",
      "Epoch  1146: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10457315.0\n",
      "loss z =  10457296.0\n",
      "loss z =  10457278.0\n",
      "loss z =  10457260.0\n",
      "loss z =  10457242.0\n",
      "loss z =  10457223.0\n",
      "loss z =  10457188.0\n",
      "loss z =  10457168.0\n",
      "loss z =  10457150.0\n",
      "loss z =  10457132.0\n",
      "Epoch  1157: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.946882486343384 s\n",
      "loss_f =  7727310.0\n",
      "loss_f =  7714038.5\n",
      "loss_f =  7706485.0\n",
      "loss_f =  7700501.5\n",
      "loss_f =  7696052.0\n",
      "loss_f =  7692335.5\n",
      "Epoch  4846: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7689402.0\n",
      "loss_f =  7688124.5\n",
      "loss_f =  7686963.0\n",
      "loss_f =  7685939.0\n",
      "loss_f =  7684980.0\n",
      "loss_f =  7684061.0\n",
      "loss_f =  7683174.0\n",
      "loss_f =  7682317.5\n",
      "loss_f =  7681491.0\n",
      "loss_f =  7680673.0\n",
      "loss_f =  7679863.0\n",
      "loss_f =  7679075.0\n",
      "loss_f =  7678296.0\n",
      "loss_f =  7677523.0\n",
      "loss_f =  7676775.0\n",
      "loss_f =  7676014.0\n",
      "loss_f =  7675276.0\n",
      "loss_f =  7674544.0\n",
      "loss_f =  7673820.0\n",
      "loss_f =  7673097.0\n",
      "loss_f =  7672389.0\n",
      "loss_f =  7671677.5\n",
      "loss_f =  7670977.5\n",
      "loss_f =  7670297.0\n",
      "loss_f =  7669604.5\n",
      "loss_f =  7668925.5\n",
      "loss_f =  7668255.0\n",
      "loss_f =  7667589.0\n",
      "loss_f =  7666918.5\n",
      "loss_f =  7666272.0\n",
      "loss_f =  7665620.0\n",
      "loss_f =  7664974.0\n",
      "loss_f =  7664331.0\n",
      "loss_f =  7663698.0\n",
      "thetaf step =  140.66738748550415 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11074.0703125\n",
      "Epoch  4425: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4441: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4457: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4473: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6410.52197265625\n",
      "Epoch  3648: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3664: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3680: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3696: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30095.193359375\n",
      "Epoch 10468: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10484: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10500: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10516: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.3515625\n",
      "Epoch  5508: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5524: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5540: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5556: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24293.828125\n",
      "Epoch  6570: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6586: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6602: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6618: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37790.7109375\n",
      "Epoch  3708: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3724: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3740: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3756: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23244.955078125\n",
      "Epoch  4595: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4611: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4627: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4643: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.826171875\n",
      "Epoch 13914: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13930: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13946: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13962: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.008056640625\n",
      "Epoch  5036: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5052: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5068: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5084: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8152.25048828125\n",
      "Epoch  3542: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3558: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3574: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3590: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19102.04296875\n",
      "Epoch  5078: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5094: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5110: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5126: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10354.458984375\n",
      "Epoch 10066: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10082: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10098: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10114: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9305.2353515625\n",
      "Epoch  5300: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5316: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5332: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5348: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8874.26171875\n",
      "Epoch  6506: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6522: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6538: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6554: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15623.8134765625\n",
      "Epoch  3623: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3639: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3655: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3671: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1812.88134765625\n",
      "Epoch  3229: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3245: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3261: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3277: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1396148204803467 s\n",
      "loss_c =  257.36846923828125\n",
      "Epoch 51525: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  256.31927490234375\n",
      "loss_c =  256.1826171875\n",
      "loss_c =  256.0513916015625\n",
      "loss_c =  255.92190551757812\n",
      "loss_c =  255.7936553955078\n",
      "loss_c =  255.66552734375\n",
      "loss_c =  255.53651428222656\n",
      "loss_c =  255.4065399169922\n",
      "loss_c =  255.27557373046875\n",
      "thetac step =  5.568827390670776 s\n",
      "step =  30\n",
      "loss z =  10458663.0\n",
      "loss z =  10455106.0\n",
      "loss z =  10451815.0\n",
      "loss z =  10448716.0\n",
      "loss z =  10445749.0\n",
      "loss z =  10442883.0\n",
      "loss z =  10440087.0\n",
      "loss z =  10437376.0\n",
      "loss z =  10434742.0\n",
      "loss z =  10432190.0\n",
      "loss z =  10429707.0\n",
      "Epoch  1169: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10427292.0\n",
      "loss z =  10426822.0\n",
      "loss z =  10426350.0\n",
      "loss z =  10425881.0\n",
      "loss z =  10425415.0\n",
      "loss z =  10424952.0\n",
      "loss z =  10424490.0\n",
      "loss z =  10424033.0\n",
      "loss z =  10423120.0\n",
      "loss z =  10422668.0\n",
      "Epoch  1180: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10422217.0\n",
      "loss z =  10422129.0\n",
      "loss z =  10422038.0\n",
      "loss z =  10421948.0\n",
      "loss z =  10421861.0\n",
      "loss z =  10421773.0\n",
      "loss z =  10421684.0\n",
      "loss z =  10421595.0\n",
      "loss z =  10421506.0\n",
      "loss z =  10421418.0\n",
      "loss z =  10421329.0\n",
      "Epoch  1191: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10421239.0\n",
      "loss z =  10421222.0\n",
      "loss z =  10421203.0\n",
      "loss z =  10421186.0\n",
      "loss z =  10421168.0\n",
      "loss z =  10421150.0\n",
      "loss z =  10421114.0\n",
      "loss z =  10421097.0\n",
      "loss z =  10421080.0\n",
      "loss z =  10421062.0\n",
      "Epoch  1202: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  25.41608500480652 s\n",
      "loss_f =  7695460.0\n",
      "loss_f =  7682424.5\n",
      "loss_f =  7675581.0\n",
      "loss_f =  7669488.5\n",
      "loss_f =  7664896.0\n",
      "loss_f =  7661247.0\n",
      "loss_f =  7658366.0\n",
      "loss_f =  7656000.5\n",
      "loss_f =  7653958.5\n",
      "loss_f =  7652081.0\n",
      "loss_f =  7650325.5\n",
      "loss_f =  7648679.0\n",
      "loss_f =  7647120.5\n",
      "loss_f =  7645617.0\n",
      "loss_f =  7644181.5\n",
      "loss_f =  7642782.0\n",
      "loss_f =  7641417.0\n",
      "loss_f =  7640080.0\n",
      "loss_f =  7638780.0\n",
      "loss_f =  7637507.0\n",
      "loss_f =  7636253.0\n",
      "loss_f =  7635034.5\n",
      "loss_f =  7633832.0\n",
      "loss_f =  7632657.0\n",
      "loss_f =  7631515.0\n",
      "loss_f =  7630375.0\n",
      "loss_f =  7629259.5\n",
      "loss_f =  7628165.0\n",
      "loss_f =  7627092.0\n",
      "loss_f =  7626020.0\n",
      "loss_f =  7624972.0\n",
      "loss_f =  7623958.0\n",
      "loss_f =  7622938.0\n",
      "loss_f =  7621937.0\n",
      "loss_f =  7620953.5\n",
      "loss_f =  7619980.5\n",
      "loss_f =  7619011.5\n",
      "loss_f =  7618055.5\n",
      "loss_f =  7617120.5\n",
      "loss_f =  7616200.0\n",
      "thetaf step =  140.7255551815033 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11074.154296875\n",
      "Epoch  4489: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4505: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4521: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4537: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6410.419921875\n",
      "Epoch  3712: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3728: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3744: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3760: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30095.251953125\n",
      "Epoch 10532: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10548: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10564: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10580: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.3203125\n",
      "Epoch  5572: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5588: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5604: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5620: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24293.765625\n",
      "Epoch  6634: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6650: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6666: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6682: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37790.7265625\n",
      "Epoch  3772: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3788: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3804: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3820: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23245.078125\n",
      "Epoch  4659: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4675: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4691: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4707: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.8203125\n",
      "Epoch 13978: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13994: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 14010: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 14026: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.005126953125\n",
      "Epoch  5100: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5116: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5132: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5148: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8150.59521484375\n",
      "Epoch  3606: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3622: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3638: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3654: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19102.18359375\n",
      "Epoch  5142: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5158: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5174: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5190: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10354.291015625\n",
      "Epoch 10130: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10146: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10162: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10178: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9305.13671875\n",
      "Epoch  5364: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5380: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5396: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5412: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8874.1025390625\n",
      "Epoch  6570: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6586: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6602: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6618: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15624.9619140625\n",
      "Epoch  3687: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3703: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3719: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3735: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1812.8995361328125\n",
      "Epoch  3293: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3309: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3325: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3341: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1476936340332031 s\n",
      "loss_c =  256.66937255859375\n",
      "loss_c =  254.9095001220703\n",
      "loss_c =  254.32733154296875\n",
      "loss_c =  253.83926391601562\n",
      "loss_c =  253.3467559814453\n",
      "loss_c =  252.84725952148438\n",
      "loss_c =  252.3394775390625\n",
      "loss_c =  251.82257080078125\n",
      "loss_c =  251.29530334472656\n",
      "loss_c =  250.75701904296875\n",
      "thetac step =  4.933608293533325 s\n",
      "step =  31\n",
      "loss z =  10418549.0\n",
      "loss z =  10413631.0\n",
      "loss z =  10409492.0\n",
      "loss z =  10405855.0\n",
      "loss z =  10402535.0\n",
      "loss z =  10399432.0\n",
      "loss z =  10396494.0\n",
      "loss z =  10393658.0\n",
      "loss z =  10390920.0\n",
      "loss z =  10388266.0\n",
      "loss z =  10385690.0\n",
      "Epoch  1214: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10383192.0\n",
      "loss z =  10382700.0\n",
      "loss z =  10382213.0\n",
      "loss z =  10381729.0\n",
      "loss z =  10381245.0\n",
      "loss z =  10380765.0\n",
      "loss z =  10380288.0\n",
      "loss z =  10379813.0\n",
      "loss z =  10378863.0\n",
      "loss z =  10378392.0\n",
      "Epoch  1225: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10377924.0\n",
      "loss z =  10377830.0\n",
      "loss z =  10377736.0\n",
      "loss z =  10377646.0\n",
      "loss z =  10377553.0\n",
      "loss z =  10377460.0\n",
      "loss z =  10377368.0\n",
      "loss z =  10377274.0\n",
      "loss z =  10377180.0\n",
      "loss z =  10377088.0\n",
      "loss z =  10376996.0\n",
      "Epoch  1236: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10376904.0\n",
      "loss z =  10376886.0\n",
      "loss z =  10376867.0\n",
      "loss z =  10376849.0\n",
      "loss z =  10376829.0\n",
      "loss z =  10376811.0\n",
      "loss z =  10376774.0\n",
      "loss z =  10376756.0\n",
      "loss z =  10376738.0\n",
      "loss z =  10376720.0\n",
      "Epoch  1247: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.822444915771484 s\n",
      "loss_f =  7655279.0\n",
      "loss_f =  7642243.5\n",
      "loss_f =  7636623.5\n",
      "loss_f =  7631147.0\n",
      "loss_f =  7626255.0\n",
      "loss_f =  7622661.0\n",
      "Epoch  5243: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7620693.0\n",
      "loss_f =  7619482.5\n",
      "loss_f =  7618416.0\n",
      "loss_f =  7617444.0\n",
      "loss_f =  7616550.0\n",
      "loss_f =  7615701.0\n",
      "loss_f =  7614889.0\n",
      "loss_f =  7614104.0\n",
      "loss_f =  7613343.0\n",
      "loss_f =  7612588.0\n",
      "loss_f =  7611864.0\n",
      "loss_f =  7611144.0\n",
      "loss_f =  7610438.5\n",
      "loss_f =  7609737.0\n",
      "loss_f =  7609057.0\n",
      "loss_f =  7608383.0\n",
      "loss_f =  7607707.0\n",
      "loss_f =  7607044.0\n",
      "loss_f =  7606386.5\n",
      "loss_f =  7605739.5\n",
      "loss_f =  7605105.0\n",
      "loss_f =  7604460.5\n",
      "loss_f =  7603840.0\n",
      "loss_f =  7603219.5\n",
      "loss_f =  7602599.0\n",
      "loss_f =  7601994.0\n",
      "loss_f =  7601389.0\n",
      "loss_f =  7600797.0\n",
      "loss_f =  7600199.5\n",
      "loss_f =  7599613.0\n",
      "loss_f =  7599039.0\n",
      "loss_f =  7598458.0\n",
      "loss_f =  7597889.0\n",
      "loss_f =  7597316.0\n",
      "thetaf step =  139.60561680793762 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11074.3642578125\n",
      "Epoch  4553: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4569: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4585: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4601: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6410.55615234375\n",
      "Epoch  3776: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3792: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3808: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3824: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30095.552734375\n",
      "Epoch 10596: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10612: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10628: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10644: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.3515625\n",
      "Epoch  5636: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5652: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5668: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5684: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.25390625\n",
      "Epoch  6698: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6714: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6730: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6746: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37790.76953125\n",
      "Epoch  3836: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3852: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3868: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3884: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23245.36328125\n",
      "Epoch  4723: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4739: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4755: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4771: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.83203125\n",
      "Epoch 14042: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 14058: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 14074: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 14090: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.11279296875\n",
      "Epoch  5164: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5180: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5196: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5212: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8150.619140625\n",
      "Epoch  3670: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3686: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3702: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3718: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19102.46484375\n",
      "Epoch  5206: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5222: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5238: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5254: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10354.720703125\n",
      "Epoch 10194: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10210: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10226: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10242: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9304.7138671875\n",
      "Epoch  5429: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5445: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5461: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5477: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8874.37109375\n",
      "Epoch  6634: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6650: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6666: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6682: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15624.1748046875\n",
      "Epoch  3751: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3767: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3783: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3799: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1812.937255859375\n",
      "Epoch  3357: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3373: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3389: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3405: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1797993183135986 s\n",
      "loss_c =  256.115966796875\n",
      "Epoch 52526: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  255.08331298828125\n",
      "loss_c =  254.93875122070312\n",
      "loss_c =  254.81008911132812\n",
      "loss_c =  254.6889190673828\n",
      "loss_c =  254.57159423828125\n",
      "loss_c =  254.4554901123047\n",
      "loss_c =  254.33981323242188\n",
      "loss_c =  254.2234649658203\n",
      "loss_c =  254.10650634765625\n",
      "thetac step =  4.995190143585205 s\n",
      "step =  32\n",
      "loss z =  10438103.0\n",
      "loss z =  10426348.0\n",
      "loss z =  10415740.0\n",
      "loss z =  10406076.0\n",
      "loss z =  10397310.0\n",
      "loss z =  10389307.0\n",
      "loss z =  10382890.0\n",
      "loss z =  10378070.0\n",
      "loss z =  10374428.0\n",
      "loss z =  10371363.0\n",
      "loss z =  10368523.0\n",
      "Epoch  1259: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10365630.0\n",
      "loss z =  10365067.0\n",
      "loss z =  10364509.0\n",
      "loss z =  10363952.0\n",
      "loss z =  10363398.0\n",
      "loss z =  10362848.0\n",
      "loss z =  10362300.0\n",
      "loss z =  10361757.0\n",
      "loss z =  10360679.0\n",
      "loss z =  10360149.0\n",
      "Epoch  1270: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10359617.0\n",
      "loss z =  10359511.0\n",
      "loss z =  10359405.0\n",
      "loss z =  10359301.0\n",
      "loss z =  10359195.0\n",
      "loss z =  10359089.0\n",
      "loss z =  10358984.0\n",
      "loss z =  10358879.0\n",
      "loss z =  10358774.0\n",
      "loss z =  10358670.0\n",
      "loss z =  10358565.0\n",
      "Epoch  1281: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10358460.0\n",
      "loss z =  10358439.0\n",
      "loss z =  10358419.0\n",
      "loss z =  10358398.0\n",
      "loss z =  10358377.0\n",
      "loss z =  10358357.0\n",
      "loss z =  10358315.0\n",
      "loss z =  10358295.0\n",
      "loss z =  10358274.0\n",
      "loss z =  10358253.0\n",
      "Epoch  1292: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.893548727035522 s\n",
      "loss_f =  7640696.0\n",
      "loss_f =  7627977.0\n",
      "loss_f =  7621989.0\n",
      "loss_f =  7614989.5\n",
      "loss_f =  7609673.0\n",
      "loss_f =  7605892.5\n",
      "Epoch  5443: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7603667.0\n",
      "loss_f =  7602278.5\n",
      "loss_f =  7601066.0\n",
      "loss_f =  7599991.0\n",
      "loss_f =  7599002.0\n",
      "loss_f =  7598073.0\n",
      "Epoch  5474: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7597359.5\n",
      "loss_f =  7596932.0\n",
      "loss_f =  7596508.0\n",
      "loss_f =  7596097.0\n",
      "loss_f =  7595681.0\n",
      "loss_f =  7595270.0\n",
      "loss_f =  7594880.0\n",
      "loss_f =  7594483.0\n",
      "loss_f =  7594091.0\n",
      "loss_f =  7593699.5\n",
      "loss_f =  7593321.5\n",
      "loss_f =  7592927.0\n",
      "loss_f =  7592542.5\n",
      "loss_f =  7592164.0\n",
      "loss_f =  7591791.0\n",
      "loss_f =  7591416.0\n",
      "loss_f =  7591040.5\n",
      "loss_f =  7590669.0\n",
      "loss_f =  7590302.5\n",
      "loss_f =  7589938.5\n",
      "loss_f =  7589572.0\n",
      "loss_f =  7589202.0\n",
      "loss_f =  7588844.5\n",
      "loss_f =  7588487.5\n",
      "loss_f =  7588128.0\n",
      "loss_f =  7587775.0\n",
      "loss_f =  7587413.0\n",
      "loss_f =  7587068.5\n",
      "thetaf step =  140.19039630889893 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11074.876953125\n",
      "Epoch  4617: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4633: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4649: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4665: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6410.37548828125\n",
      "Epoch  3840: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3856: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3872: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3888: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30095.751953125\n",
      "Epoch 10660: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10676: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10692: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10708: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.3125\n",
      "Epoch  5700: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5716: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5732: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5748: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.267578125\n",
      "Epoch  6762: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6778: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6794: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6810: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37790.88671875\n",
      "Epoch  3900: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3916: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3932: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3948: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23245.7890625\n",
      "Epoch  4787: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4803: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4819: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4835: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.9189453125\n",
      "Epoch 14106: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 14122: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 14138: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 14154: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.11474609375\n",
      "Epoch  5228: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5244: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5260: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5276: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8149.51171875\n",
      "Epoch  3734: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3750: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3766: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3782: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19102.8984375\n",
      "Epoch  5270: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5286: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5302: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5318: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10354.47265625\n",
      "Epoch 10258: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10274: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10290: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10306: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9304.8134765625\n",
      "Epoch  5493: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5509: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5525: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5541: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8874.0302734375\n",
      "Epoch  6698: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6714: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6730: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6746: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15623.8056640625\n",
      "Epoch  3815: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3831: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3847: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3863: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.04150390625\n",
      "Epoch  3421: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3437: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3453: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3469: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1559648513793945 s\n",
      "loss_c =  255.82766723632812\n",
      "loss_c =  254.4998779296875\n",
      "loss_c =  254.0593719482422\n",
      "loss_c =  253.65597534179688\n",
      "loss_c =  253.25830078125\n",
      "loss_c =  252.86343383789062\n",
      "loss_c =  252.46884155273438\n",
      "loss_c =  252.0726318359375\n",
      "loss_c =  251.67327880859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_c =  251.26968383789062\n",
      "thetac step =  5.850454807281494 s\n",
      "step =  33\n",
      "loss z =  10440106.0\n",
      "loss z =  10425001.0\n",
      "loss z =  10411490.0\n",
      "loss z =  10399506.0\n",
      "loss z =  10389057.0\n",
      "loss z =  10380322.0\n",
      "loss z =  10373503.0\n",
      "loss z =  10368129.0\n",
      "loss z =  10363580.0\n",
      "loss z =  10359480.0\n",
      "loss z =  10355696.0\n",
      "Epoch  1304: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10352136.0\n",
      "loss z =  10351450.0\n",
      "loss z =  10350778.0\n",
      "loss z =  10350108.0\n",
      "loss z =  10349446.0\n",
      "loss z =  10348791.0\n",
      "loss z =  10348142.0\n",
      "loss z =  10347498.0\n",
      "loss z =  10346236.0\n",
      "loss z =  10345612.0\n",
      "Epoch  1315: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10344993.0\n",
      "loss z =  10344870.0\n",
      "loss z =  10344749.0\n",
      "loss z =  10344625.0\n",
      "loss z =  10344501.0\n",
      "loss z =  10344379.0\n",
      "loss z =  10344257.0\n",
      "loss z =  10344136.0\n",
      "loss z =  10344016.0\n",
      "loss z =  10343893.0\n",
      "loss z =  10343772.0\n",
      "Epoch  1326: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10343651.0\n",
      "loss z =  10343627.0\n",
      "loss z =  10343603.0\n",
      "loss z =  10343578.0\n",
      "loss z =  10343554.0\n",
      "loss z =  10343530.0\n",
      "loss z =  10343482.0\n",
      "loss z =  10343458.0\n",
      "loss z =  10343433.0\n",
      "loss z =  10343410.0\n",
      "Epoch  1337: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.84684944152832 s\n",
      "loss_f =  7629507.5\n",
      "loss_f =  7614710.0\n",
      "loss_f =  7607531.0\n",
      "loss_f =  7600137.5\n",
      "loss_f =  7593682.0\n",
      "loss_f =  7588955.0\n",
      "Epoch  5645: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7585597.0\n",
      "loss_f =  7584086.0\n",
      "loss_f =  7582772.0\n",
      "loss_f =  7581567.0\n",
      "loss_f =  7580478.0\n",
      "loss_f =  7579441.0\n",
      "loss_f =  7578478.5\n",
      "loss_f =  7577541.5\n",
      "loss_f =  7576660.0\n",
      "loss_f =  7575789.0\n",
      "loss_f =  7574940.0\n",
      "loss_f =  7574118.0\n",
      "loss_f =  7573298.0\n",
      "loss_f =  7572503.0\n",
      "loss_f =  7571722.0\n",
      "loss_f =  7570941.0\n",
      "loss_f =  7570182.5\n",
      "loss_f =  7569429.0\n",
      "loss_f =  7568695.0\n",
      "loss_f =  7567965.5\n",
      "loss_f =  7567240.5\n",
      "loss_f =  7566526.0\n",
      "loss_f =  7565824.0\n",
      "loss_f =  7565136.5\n",
      "loss_f =  7564442.0\n",
      "loss_f =  7563768.5\n",
      "loss_f =  7563091.0\n",
      "loss_f =  7562426.5\n",
      "loss_f =  7561763.5\n",
      "loss_f =  7561110.0\n",
      "loss_f =  7560463.5\n",
      "loss_f =  7559825.5\n",
      "loss_f =  7559190.0\n",
      "loss_f =  7558557.0\n",
      "thetaf step =  139.7707347869873 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11075.171875\n",
      "Epoch  4681: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4697: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4713: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4729: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6410.373046875\n",
      "Epoch  3904: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3920: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3936: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3952: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30096.076171875\n",
      "Epoch 10724: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10740: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10756: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10772: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.29296875\n",
      "Epoch  5764: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5780: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5796: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5812: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.6875\n",
      "Epoch  6826: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6842: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6858: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6874: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37790.96484375\n",
      "Epoch  3964: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3980: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3996: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4012: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23246.1875\n",
      "Epoch  4851: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4867: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4883: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4899: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.8251953125\n",
      "Epoch 14170: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 14186: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 14202: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 14218: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.22216796875\n",
      "Epoch  5292: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5308: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5324: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5340: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8149.220703125\n",
      "Epoch  3798: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3814: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3830: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3846: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19103.3515625\n",
      "Epoch  5334: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5350: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5366: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5382: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10354.541015625\n",
      "Epoch 10322: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10338: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10354: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10370: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9304.4482421875\n",
      "Epoch  5557: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5573: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5589: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5605: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8873.98046875\n",
      "Epoch  6762: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6778: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6794: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6810: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15623.7509765625\n",
      "Epoch  3879: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3895: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3911: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3927: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.0919189453125\n",
      "Epoch  3485: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3501: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3517: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3533: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.166335105895996 s\n",
      "loss_c =  257.145263671875\n",
      "Epoch 53527: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  256.05230712890625\n",
      "loss_c =  255.9128875732422\n",
      "loss_c =  255.794677734375\n",
      "loss_c =  255.68792724609375\n",
      "loss_c =  255.58639526367188\n",
      "loss_c =  255.48764038085938\n",
      "loss_c =  255.39012145996094\n",
      "loss_c =  255.29290771484375\n",
      "loss_c =  255.19557189941406\n",
      "thetac step =  5.435652017593384 s\n",
      "step =  34\n",
      "loss z =  10430218.0\n",
      "loss z =  10416053.0\n",
      "loss z =  10403102.0\n",
      "loss z =  10391074.0\n",
      "loss z =  10379872.0\n",
      "loss z =  10369484.0\n",
      "loss z =  10359972.0\n",
      "loss z =  10351380.0\n",
      "loss z =  10343872.0\n",
      "loss z =  10338015.0\n",
      "loss z =  10333577.0\n",
      "Epoch  1349: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10330000.0\n",
      "loss z =  10329349.0\n",
      "loss z =  10328704.0\n",
      "loss z =  10328073.0\n",
      "loss z =  10327443.0\n",
      "loss z =  10326821.0\n",
      "loss z =  10326204.0\n",
      "loss z =  10325592.0\n",
      "loss z =  10324380.0\n",
      "loss z =  10323781.0\n",
      "Epoch  1360: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10323186.0\n",
      "loss z =  10323067.0\n",
      "loss z =  10322949.0\n",
      "loss z =  10322832.0\n",
      "loss z =  10322715.0\n",
      "loss z =  10322598.0\n",
      "loss z =  10322480.0\n",
      "loss z =  10322362.0\n",
      "loss z =  10322244.0\n",
      "loss z =  10322127.0\n",
      "loss z =  10322011.0\n",
      "Epoch  1371: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10321895.0\n",
      "loss z =  10321872.0\n",
      "loss z =  10321849.0\n",
      "loss z =  10321825.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss z =  10321802.0\n",
      "loss z =  10321780.0\n",
      "loss z =  10321732.0\n",
      "loss z =  10321710.0\n",
      "loss z =  10321686.0\n",
      "loss z =  10321662.0\n",
      "Epoch  1382: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.87155508995056 s\n",
      "loss_f =  7611435.0\n",
      "loss_f =  7596435.0\n",
      "loss_f =  7588597.5\n",
      "loss_f =  7580639.0\n",
      "loss_f =  7574542.5\n",
      "loss_f =  7569843.0\n",
      "Epoch  5846: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7566273.0\n",
      "loss_f =  7564764.0\n",
      "loss_f =  7563439.0\n",
      "loss_f =  7562232.0\n",
      "loss_f =  7561117.0\n",
      "loss_f =  7560073.5\n",
      "loss_f =  7559082.0\n",
      "Epoch  5877: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7558509.0\n",
      "loss_f =  7558044.0\n",
      "loss_f =  7557589.0\n",
      "loss_f =  7557134.5\n",
      "loss_f =  7556681.5\n",
      "loss_f =  7556238.5\n",
      "loss_f =  7555806.0\n",
      "loss_f =  7555369.0\n",
      "loss_f =  7554941.0\n",
      "loss_f =  7554519.5\n",
      "loss_f =  7554097.0\n",
      "loss_f =  7553670.5\n",
      "loss_f =  7553252.0\n",
      "loss_f =  7552840.5\n",
      "loss_f =  7552418.0\n",
      "loss_f =  7552016.0\n",
      "loss_f =  7551620.5\n",
      "loss_f =  7551205.0\n",
      "loss_f =  7550805.0\n",
      "loss_f =  7550409.0\n",
      "loss_f =  7550012.5\n",
      "loss_f =  7549610.0\n",
      "loss_f =  7549217.5\n",
      "loss_f =  7548826.5\n",
      "loss_f =  7548440.0\n",
      "loss_f =  7548051.0\n",
      "loss_f =  7547654.5\n",
      "thetaf step =  140.52801728248596 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11075.314453125\n",
      "Epoch  4745: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4761: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4777: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4793: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6410.25390625\n",
      "Epoch  3968: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3984: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4000: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4016: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30096.16015625\n",
      "Epoch 10788: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10804: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10820: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10836: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.265625\n",
      "Epoch  5828: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5844: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5860: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5876: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.529296875\n",
      "Epoch  6890: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6906: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6922: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6938: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.0\n",
      "Epoch  4028: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4044: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4060: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4076: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23246.37109375\n",
      "Epoch  4915: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4931: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4947: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4963: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.744140625\n",
      "Epoch 14234: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 14250: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 14266: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 14282: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.231201171875\n",
      "Epoch  5356: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5372: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5388: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5404: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8148.484375\n",
      "Epoch  3862: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3878: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3894: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3910: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19103.521484375\n",
      "Epoch  5398: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5414: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5430: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5446: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10354.36328125\n",
      "Epoch 10386: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10402: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10418: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10434: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9304.390625\n",
      "Epoch  5621: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5637: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5653: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5669: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8873.76171875\n",
      "Epoch  6826: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6842: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6858: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6874: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15623.7548828125\n",
      "Epoch  3943: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3959: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3975: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3991: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.3516845703125\n",
      "Epoch  3549: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3565: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3581: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3597: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.164527416229248 s\n",
      "loss_c =  256.46038818359375\n",
      "loss_c =  255.051513671875\n",
      "loss_c =  254.61688232421875\n",
      "loss_c =  254.20159912109375\n",
      "loss_c =  253.79632568359375\n",
      "loss_c =  253.39718627929688\n",
      "loss_c =  253.00088500976562\n",
      "loss_c =  252.60455322265625\n",
      "loss_c =  252.20623779296875\n",
      "loss_c =  251.80450439453125\n",
      "thetac step =  6.077385663986206 s\n",
      "step =  35\n",
      "loss z =  10396352.0\n",
      "loss z =  10382003.0\n",
      "loss z =  10369078.0\n",
      "loss z =  10357647.0\n",
      "loss z =  10347829.0\n",
      "loss z =  10339746.0\n",
      "loss z =  10333318.0\n",
      "loss z =  10327958.0\n",
      "loss z =  10323194.0\n",
      "loss z =  10318820.0\n",
      "loss z =  10314750.0\n",
      "Epoch  1394: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10310954.0\n",
      "loss z =  10310234.0\n",
      "loss z =  10309524.0\n",
      "loss z =  10308820.0\n",
      "loss z =  10308125.0\n",
      "loss z =  10307437.0\n",
      "loss z =  10306757.0\n",
      "loss z =  10306085.0\n",
      "loss z =  10304762.0\n",
      "loss z =  10304110.0\n",
      "Epoch  1405: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10303464.0\n",
      "loss z =  10303338.0\n",
      "loss z =  10303210.0\n",
      "loss z =  10303083.0\n",
      "loss z =  10302956.0\n",
      "loss z =  10302827.0\n",
      "loss z =  10302699.0\n",
      "loss z =  10302571.0\n",
      "loss z =  10302442.0\n",
      "loss z =  10302316.0\n",
      "loss z =  10302190.0\n",
      "Epoch  1416: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10302064.0\n",
      "loss z =  10302038.0\n",
      "loss z =  10302014.0\n",
      "loss z =  10301986.0\n",
      "loss z =  10301962.0\n",
      "loss z =  10301936.0\n",
      "loss z =  10301888.0\n",
      "loss z =  10301863.0\n",
      "loss z =  10301838.0\n",
      "loss z =  10301814.0\n",
      "Epoch  1427: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.815211534500122 s\n",
      "loss_f =  7594994.0\n",
      "loss_f =  7579306.5\n",
      "loss_f =  7571381.5\n",
      "loss_f =  7563932.0\n",
      "loss_f =  7557984.0\n",
      "Epoch  6040: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7553736.0\n",
      "loss_f =  7551674.5\n",
      "loss_f =  7549936.0\n",
      "loss_f =  7548500.0\n",
      "loss_f =  7547208.5\n",
      "loss_f =  7546034.5\n",
      "loss_f =  7544946.5\n",
      "loss_f =  7543911.5\n",
      "loss_f =  7542931.0\n",
      "loss_f =  7541983.0\n",
      "loss_f =  7541063.0\n",
      "loss_f =  7540168.5\n",
      "loss_f =  7539291.0\n",
      "loss_f =  7538436.0\n",
      "loss_f =  7537602.0\n",
      "loss_f =  7536766.5\n",
      "loss_f =  7535954.0\n",
      "loss_f =  7535158.0\n",
      "loss_f =  7534364.0\n",
      "loss_f =  7533590.0\n",
      "loss_f =  7532826.5\n",
      "loss_f =  7532074.5\n",
      "loss_f =  7531323.5\n",
      "loss_f =  7530586.5\n",
      "loss_f =  7529866.0\n",
      "loss_f =  7529136.0\n",
      "loss_f =  7528432.0\n",
      "loss_f =  7527723.5\n",
      "loss_f =  7527026.5\n",
      "loss_f =  7526342.5\n",
      "loss_f =  7525662.0\n",
      "loss_f =  7524991.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_f =  7524322.5\n",
      "loss_f =  7523649.0\n",
      "loss_f =  7523009.5\n",
      "thetaf step =  139.82229089736938 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11075.2255859375\n",
      "Epoch  4809: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4825: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4841: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4857: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6410.3857421875\n",
      "Epoch  4032: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4048: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4064: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4080: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30096.36328125\n",
      "Epoch 10852: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10868: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10884: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10900: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.34375\n",
      "Epoch  5892: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5908: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5924: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5940: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24293.404296875\n",
      "Epoch  6954: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6970: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6986: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7002: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.01171875\n",
      "Epoch  4092: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4108: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4124: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4140: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23246.431640625\n",
      "Epoch  4979: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4995: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5027: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.62890625\n",
      "Epoch 14298: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 14314: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 14330: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 14346: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.3583984375\n",
      "Epoch  5420: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5436: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5452: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5468: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8148.0830078125\n",
      "Epoch  3926: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3942: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3958: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3974: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19103.677734375\n",
      "Epoch  5462: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5478: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5494: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5510: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10354.6767578125\n",
      "Epoch 10450: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10466: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10482: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10498: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9303.83203125\n",
      "Epoch  5685: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5701: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5717: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5733: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8874.083984375\n",
      "Epoch  6890: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6906: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6922: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6938: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15623.73828125\n",
      "Epoch  4007: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4023: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4039: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4055: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.203857421875\n",
      "Epoch  3613: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3629: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3645: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3661: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.139066219329834 s\n",
      "loss_c =  257.4081726074219\n",
      "Epoch 54528: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  256.0852355957031\n",
      "loss_c =  255.9425506591797\n",
      "loss_c =  255.82077026367188\n",
      "loss_c =  255.70993041992188\n",
      "loss_c =  255.6050262451172\n",
      "loss_c =  255.5028076171875\n",
      "loss_c =  255.40223693847656\n",
      "loss_c =  255.30197143554688\n",
      "loss_c =  255.20211791992188\n",
      "thetac step =  6.252959489822388 s\n",
      "step =  36\n",
      "loss z =  10360685.0\n",
      "loss z =  10349164.0\n",
      "loss z =  10338483.0\n",
      "loss z =  10328623.0\n",
      "loss z =  10319667.0\n",
      "loss z =  10311640.0\n",
      "loss z =  10304676.0\n",
      "loss z =  10299264.0\n",
      "loss z =  10295074.0\n",
      "loss z =  10291552.0\n",
      "loss z =  10288315.0\n",
      "Epoch  1439: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10285208.0\n",
      "loss z =  10284601.0\n",
      "loss z =  10283996.0\n",
      "loss z =  10283393.0\n",
      "loss z =  10282794.0\n",
      "loss z =  10282197.0\n",
      "loss z =  10281605.0\n",
      "loss z =  10281019.0\n",
      "loss z =  10279848.0\n",
      "loss z =  10279268.0\n",
      "Epoch  1450: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10278690.0\n",
      "loss z =  10278575.0\n",
      "loss z =  10278460.0\n",
      "loss z =  10278345.0\n",
      "loss z =  10278230.0\n",
      "loss z =  10278117.0\n",
      "loss z =  10278002.0\n",
      "loss z =  10277888.0\n",
      "loss z =  10277774.0\n",
      "loss z =  10277658.0\n",
      "loss z =  10277546.0\n",
      "Epoch  1461: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10277434.0\n",
      "loss z =  10277412.0\n",
      "loss z =  10277388.0\n",
      "loss z =  10277366.0\n",
      "loss z =  10277343.0\n",
      "loss z =  10277320.0\n",
      "loss z =  10277276.0\n",
      "loss z =  10277254.0\n",
      "loss z =  10277230.0\n",
      "loss z =  10277208.0\n",
      "Epoch  1472: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.957438468933105 s\n",
      "loss_f =  7573865.0\n",
      "loss_f =  7560334.0\n",
      "loss_f =  7551431.0\n",
      "loss_f =  7543320.0\n",
      "loss_f =  7537727.5\n",
      "loss_f =  7532956.0\n",
      "Epoch  6245: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7529535.0\n",
      "loss_f =  7527988.5\n",
      "loss_f =  7526609.0\n",
      "loss_f =  7525360.0\n",
      "loss_f =  7524241.5\n",
      "loss_f =  7523202.5\n",
      "Epoch  6276: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7522198.0\n",
      "loss_f =  7521723.0\n",
      "loss_f =  7521247.0\n",
      "loss_f =  7520764.5\n",
      "loss_f =  7520321.5\n",
      "loss_f =  7519870.5\n",
      "loss_f =  7519428.0\n",
      "loss_f =  7518978.0\n",
      "loss_f =  7518542.5\n",
      "loss_f =  7518112.5\n",
      "loss_f =  7517667.0\n",
      "loss_f =  7517245.0\n",
      "loss_f =  7516821.0\n",
      "loss_f =  7516404.0\n",
      "loss_f =  7515982.0\n",
      "loss_f =  7515573.0\n",
      "loss_f =  7515162.0\n",
      "loss_f =  7514743.0\n",
      "loss_f =  7514333.5\n",
      "loss_f =  7513931.0\n",
      "loss_f =  7513520.0\n",
      "loss_f =  7513123.5\n",
      "loss_f =  7512725.0\n",
      "loss_f =  7512320.5\n",
      "loss_f =  7511925.5\n",
      "loss_f =  7511531.0\n",
      "loss_f =  7511143.0\n",
      "loss_f =  7510751.5\n",
      "thetaf step =  141.21791648864746 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11075.439453125\n",
      "Epoch  4873: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4889: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4905: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4921: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6410.21484375\n",
      "Epoch  4096: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4112: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4128: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4144: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30096.4921875\n",
      "Epoch 10916: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10932: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10948: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10964: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.328125\n",
      "Epoch  5956: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5972: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5988: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6004: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.548828125\n",
      "Epoch  7018: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7034: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7050: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7066: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.0703125\n",
      "Epoch  4156: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4172: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4188: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4204: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23246.685546875\n",
      "Epoch  5043: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5059: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5075: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5091: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.615234375\n",
      "Epoch 14362: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 14378: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 14394: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 14410: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.3779296875\n",
      "Epoch  5484: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5500: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5516: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5532: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8147.78271484375\n",
      "Epoch  3990: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4022: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4038: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19103.92578125\n",
      "Epoch  5526: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5542: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5558: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5574: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10354.3994140625\n",
      "Epoch 10514: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10530: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10546: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10562: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9303.8046875\n",
      "Epoch  5749: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5765: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5781: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5797: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8873.7548828125\n",
      "Epoch  6954: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6970: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6986: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7002: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15623.8916015625\n",
      "Epoch  4071: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4087: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4103: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4119: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.2955322265625\n",
      "Epoch  3677: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3693: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3709: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3725: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.2766637802124023 s\n",
      "loss_c =  256.58453369140625\n",
      "loss_c =  255.16053771972656\n",
      "loss_c =  254.73684692382812\n",
      "loss_c =  254.34007263183594\n",
      "loss_c =  253.9505615234375\n",
      "loss_c =  253.56365966796875\n",
      "loss_c =  253.17649841308594\n",
      "loss_c =  252.78744506835938\n",
      "loss_c =  252.39517211914062\n",
      "loss_c =  251.99905395507812\n",
      "thetac step =  6.660425901412964 s\n",
      "step =  37\n",
      "loss z =  10319467.0\n",
      "loss z =  10308714.0\n",
      "loss z =  10299455.0\n",
      "loss z =  10291853.0\n",
      "loss z =  10285937.0\n",
      "loss z =  10281338.0\n",
      "loss z =  10277460.0\n",
      "loss z =  10273889.0\n",
      "loss z =  10270473.0\n",
      "loss z =  10267179.0\n",
      "loss z =  10263988.0\n",
      "Epoch  1484: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10260896.0\n",
      "loss z =  10260290.0\n",
      "loss z =  10259690.0\n",
      "loss z =  10259090.0\n",
      "loss z =  10258493.0\n",
      "loss z =  10257900.0\n",
      "loss z =  10257312.0\n",
      "loss z =  10256728.0\n",
      "loss z =  10255566.0\n",
      "loss z =  10254990.0\n",
      "Epoch  1495: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10254419.0\n",
      "loss z =  10254306.0\n",
      "loss z =  10254191.0\n",
      "loss z =  10254079.0\n",
      "loss z =  10253964.0\n",
      "loss z =  10253850.0\n",
      "loss z =  10253736.0\n",
      "loss z =  10253624.0\n",
      "loss z =  10253510.0\n",
      "loss z =  10253396.0\n",
      "loss z =  10253283.0\n",
      "Epoch  1506: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10253171.0\n",
      "loss z =  10253149.0\n",
      "loss z =  10253127.0\n",
      "loss z =  10253104.0\n",
      "loss z =  10253082.0\n",
      "loss z =  10253060.0\n",
      "loss z =  10253015.0\n",
      "loss z =  10252992.0\n",
      "loss z =  10252970.0\n",
      "loss z =  10252948.0\n",
      "Epoch  1517: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  26.98620295524597 s\n",
      "loss_f =  7552928.0\n",
      "loss_f =  7541446.0\n",
      "loss_f =  7533991.0\n",
      "loss_f =  7526493.0\n",
      "loss_f =  7519242.0\n",
      "loss_f =  7514209.0\n",
      "Epoch  6442: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7511785.0\n",
      "loss_f =  7510076.0\n",
      "loss_f =  7508696.0\n",
      "loss_f =  7507452.0\n",
      "loss_f =  7506328.0\n",
      "loss_f =  7505308.0\n",
      "loss_f =  7504337.0\n",
      "loss_f =  7503415.5\n",
      "loss_f =  7502522.0\n",
      "loss_f =  7501663.0\n",
      "loss_f =  7500823.5\n",
      "loss_f =  7499992.0\n",
      "loss_f =  7499196.0\n",
      "loss_f =  7498395.0\n",
      "loss_f =  7497610.0\n",
      "loss_f =  7496837.0\n",
      "loss_f =  7496083.0\n",
      "loss_f =  7495332.0\n",
      "loss_f =  7494601.0\n",
      "loss_f =  7493867.0\n",
      "loss_f =  7493142.0\n",
      "loss_f =  7492429.0\n",
      "loss_f =  7491724.0\n",
      "loss_f =  7491037.0\n",
      "loss_f =  7490339.0\n",
      "loss_f =  7489665.0\n",
      "loss_f =  7488982.5\n",
      "loss_f =  7488331.0\n",
      "loss_f =  7487659.5\n",
      "loss_f =  7487011.5\n",
      "loss_f =  7486368.5\n",
      "loss_f =  7485728.0\n",
      "loss_f =  7485086.5\n",
      "loss_f =  7484464.0\n",
      "thetaf step =  141.06869530677795 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11075.28515625\n",
      "Epoch  4937: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4953: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4969: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4985: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6410.3701171875\n",
      "Epoch  4160: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4176: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4192: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4208: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30096.66015625\n",
      "Epoch 10980: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10996: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11012: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11028: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.38671875\n",
      "Epoch  6020: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6036: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6052: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6068: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.5859375\n",
      "Epoch  7082: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7098: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7114: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7130: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.05078125\n",
      "Epoch  4220: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4236: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4252: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4268: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23246.677734375\n",
      "Epoch  5107: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5123: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5139: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5155: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.478515625\n",
      "Epoch 14426: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 14442: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 14458: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 14474: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.50390625\n",
      "Epoch  5548: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5564: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5580: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5596: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8147.033203125\n",
      "Epoch  4054: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4070: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4086: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4102: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19103.9609375\n",
      "Epoch  5590: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5606: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5622: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5638: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10354.87109375\n",
      "Epoch 10578: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10594: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10610: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10626: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9303.220703125\n",
      "Epoch  5814: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5830: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5846: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5862: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8874.162109375\n",
      "Epoch  7018: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7034: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7050: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7066: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15624.0\n",
      "Epoch  4135: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4151: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4167: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4183: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.226318359375\n",
      "Epoch  3741: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3757: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3773: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3789: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1473753452301025 s\n",
      "loss_c =  256.4339599609375\n",
      "Epoch 55529: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  255.66358947753906\n",
      "loss_c =  255.5027313232422\n",
      "loss_c =  255.38494873046875\n",
      "loss_c =  255.28890991210938\n",
      "loss_c =  255.20376586914062\n",
      "loss_c =  255.1231231689453\n",
      "loss_c =  255.04403686523438\n",
      "loss_c =  254.965087890625\n",
      "loss_c =  254.88510131835938\n",
      "thetac step =  6.600181341171265 s\n",
      "step =  38\n",
      "loss z =  10303442.0\n",
      "loss z =  10291514.0\n",
      "loss z =  10281796.0\n",
      "loss z =  10273978.0\n",
      "loss z =  10267863.0\n",
      "loss z =  10263148.0\n",
      "loss z =  10259162.0\n",
      "loss z =  10255490.0\n",
      "loss z =  10251991.0\n",
      "loss z =  10248620.0\n",
      "loss z =  10245351.0\n",
      "Epoch  1529: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10242177.0\n",
      "loss z =  10241548.0\n",
      "loss z =  10240922.0\n",
      "loss z =  10240305.0\n",
      "loss z =  10239691.0\n",
      "loss z =  10239077.0\n",
      "loss z =  10238467.0\n",
      "loss z =  10237861.0\n",
      "loss z =  10236660.0\n",
      "loss z =  10236062.0\n",
      "Epoch  1540: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10235468.0\n",
      "loss z =  10235351.0\n",
      "loss z =  10235231.0\n",
      "loss z =  10235114.0\n",
      "loss z =  10234996.0\n",
      "loss z =  10234878.0\n",
      "loss z =  10234760.0\n",
      "loss z =  10234640.0\n",
      "loss z =  10234524.0\n",
      "loss z =  10234406.0\n",
      "loss z =  10234288.0\n",
      "Epoch  1551: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10234170.0\n",
      "loss z =  10234147.0\n",
      "loss z =  10234124.0\n",
      "loss z =  10234099.0\n",
      "loss z =  10234074.0\n",
      "loss z =  10234052.0\n",
      "loss z =  10234005.0\n",
      "loss z =  10233982.0\n",
      "loss z =  10233960.0\n",
      "loss z =  10233936.0\n",
      "Epoch  1562: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.778366327285767 s\n",
      "loss_f =  7537385.0\n",
      "loss_f =  7524893.0\n",
      "loss_f =  7517374.5\n",
      "loss_f =  7508536.5\n",
      "loss_f =  7502121.0\n",
      "loss_f =  7497580.0\n",
      "Epoch  6643: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7495033.0\n",
      "loss_f =  7493243.5\n",
      "loss_f =  7491785.5\n",
      "loss_f =  7490562.0\n",
      "loss_f =  7489436.0\n",
      "loss_f =  7488369.0\n",
      "Epoch  6674: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7487565.5\n",
      "loss_f =  7487068.0\n",
      "loss_f =  7486591.0\n",
      "loss_f =  7486119.5\n",
      "loss_f =  7485664.0\n",
      "loss_f =  7485210.5\n",
      "Epoch  6705: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  7484799.5\n",
      "loss_f =  7484578.0\n",
      "loss_f =  7484355.0\n",
      "loss_f =  7484132.5\n",
      "loss_f =  7483915.5\n",
      "loss_f =  7483700.0\n",
      "loss_f =  7483481.0\n",
      "loss_f =  7483256.0\n",
      "loss_f =  7483036.5\n",
      "loss_f =  7482818.5\n",
      "loss_f =  7482609.5\n",
      "loss_f =  7482391.0\n",
      "loss_f =  7482172.0\n",
      "loss_f =  7481956.0\n",
      "loss_f =  7481735.0\n",
      "loss_f =  7481535.5\n",
      "loss_f =  7481321.0\n",
      "loss_f =  7481100.0\n",
      "loss_f =  7480898.0\n",
      "loss_f =  7480680.0\n",
      "loss_f =  7480466.0\n",
      "loss_f =  7480258.0\n",
      "thetaf step =  140.59617042541504 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11075.373046875\n",
      "Epoch  5001: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5017: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5033: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5049: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6410.3115234375\n",
      "Epoch  4224: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4240: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4256: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4272: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30096.755859375\n",
      "Epoch 11044: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11060: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11076: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11092: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.3671875\n",
      "Epoch  6084: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6100: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6116: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6132: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.572265625\n",
      "Epoch  7146: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7162: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7178: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7194: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.06640625\n",
      "Epoch  4284: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4300: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4316: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4332: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23246.802734375\n",
      "Epoch  5171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5187: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5203: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5219: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.494140625\n",
      "Epoch 14490: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 14506: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 14522: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 14538: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.537109375\n",
      "Epoch  5612: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5628: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5644: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5660: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8146.80322265625\n",
      "Epoch  4118: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4134: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4150: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4166: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19104.07421875\n",
      "Epoch  5654: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5670: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5686: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5702: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10354.8203125\n",
      "Epoch 10642: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10658: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10674: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10690: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9303.115234375\n",
      "Epoch  5878: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5894: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5910: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5926: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8874.087890625\n",
      "Epoch  7082: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7098: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7114: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7130: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15623.82421875\n",
      "Epoch  4199: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4215: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4231: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4247: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.203857421875\n",
      "Epoch  3805: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3821: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3837: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3853: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.171358585357666 s\n",
      "loss_c =  256.16668701171875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_c =  255.15489196777344\n",
      "loss_c =  254.80857849121094\n",
      "loss_c =  254.46376037597656\n",
      "loss_c =  254.11526489257812\n",
      "loss_c =  253.7621612548828\n",
      "loss_c =  253.40426635742188\n",
      "loss_c =  253.04107666015625\n",
      "loss_c =  252.67242431640625\n",
      "loss_c =  252.29827880859375\n",
      "thetac step =  5.927362680435181 s\n",
      "step =  39\n",
      "loss z =  10311402.0\n",
      "loss z =  10296637.0\n",
      "loss z =  10283661.0\n",
      "loss z =  10272344.0\n",
      "loss z =  10262645.0\n",
      "loss z =  10254684.0\n",
      "loss z =  10248373.0\n",
      "loss z =  10243394.0\n",
      "loss z =  10239230.0\n",
      "loss z =  10235398.0\n",
      "loss z =  10231808.0\n",
      "Epoch  1574: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10228402.0\n",
      "loss z =  10227746.0\n",
      "loss z =  10227095.0\n",
      "loss z =  10226446.0\n",
      "loss z =  10225805.0\n",
      "loss z =  10225171.0\n",
      "loss z =  10224538.0\n",
      "loss z =  10223912.0\n",
      "loss z =  10222641.0\n",
      "loss z =  10222002.0\n",
      "Epoch  1585: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10221364.0\n",
      "loss z =  10221239.0\n",
      "loss z =  10221114.0\n",
      "loss z =  10220988.0\n",
      "loss z =  10220863.0\n",
      "loss z =  10220738.0\n",
      "loss z =  10220611.0\n",
      "loss z =  10220487.0\n",
      "loss z =  10220363.0\n",
      "loss z =  10220238.0\n",
      "loss z =  10220114.0\n",
      "Epoch  1596: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10219990.0\n",
      "loss z =  10219964.0\n",
      "loss z =  10219940.0\n",
      "loss z =  10219915.0\n",
      "loss z =  10219890.0\n",
      "loss z =  10219866.0\n",
      "loss z =  10219816.0\n",
      "loss z =  10219790.0\n",
      "loss z =  10219767.0\n",
      "loss z =  10219743.0\n",
      "Epoch  1607: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  25.599079847335815 s\n",
      "loss_f =  7526298.0\n",
      "loss_f =  7516039.0\n",
      "loss_f =  7508179.0\n",
      "loss_f =  7500260.0\n",
      "Epoch  6834: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7494281.5\n",
      "loss_f =  7490651.0\n",
      "loss_f =  7487923.0\n",
      "loss_f =  7485808.0\n",
      "loss_f =  7484044.5\n",
      "loss_f =  7482564.0\n",
      "Epoch  6865: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7481411.5\n",
      "loss_f =  7480820.5\n",
      "loss_f =  7480245.5\n",
      "loss_f =  7479685.0\n",
      "loss_f =  7479165.0\n",
      "loss_f =  7478646.0\n",
      "loss_f =  7478140.0\n",
      "loss_f =  7477649.0\n",
      "loss_f =  7477156.0\n",
      "loss_f =  7476687.0\n",
      "loss_f =  7476214.5\n",
      "loss_f =  7475750.0\n",
      "loss_f =  7475285.5\n",
      "loss_f =  7474825.0\n",
      "loss_f =  7474372.0\n",
      "loss_f =  7473931.0\n",
      "loss_f =  7473475.0\n",
      "loss_f =  7473029.0\n",
      "loss_f =  7472604.0\n",
      "loss_f =  7472162.0\n",
      "loss_f =  7471738.5\n",
      "loss_f =  7471318.0\n",
      "loss_f =  7470887.0\n",
      "loss_f =  7470463.5\n",
      "loss_f =  7470043.0\n",
      "loss_f =  7469622.5\n",
      "loss_f =  7469214.0\n",
      "loss_f =  7468809.5\n",
      "loss_f =  7468385.0\n",
      "loss_f =  7467989.0\n",
      "thetaf step =  143.14501452445984 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11075.306640625\n",
      "Epoch  5065: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5081: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5097: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5113: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6410.38525390625\n",
      "Epoch  4288: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4304: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4320: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4336: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30096.921875\n",
      "Epoch 11108: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11124: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11140: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11156: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.4140625\n",
      "Epoch  6148: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6164: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6180: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6196: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.685546875\n",
      "Epoch  7210: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7226: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7242: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7258: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.0625\n",
      "Epoch  4348: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4364: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4380: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4396: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23246.904296875\n",
      "Epoch  5235: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5251: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5267: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5283: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.326171875\n",
      "Epoch 14554: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 14570: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 14586: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 14602: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.6552734375\n",
      "Epoch  5676: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5692: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5708: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5724: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8146.267578125\n",
      "Epoch  4182: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4198: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4214: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4230: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19104.1796875\n",
      "Epoch  5718: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5734: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5750: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5766: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10355.146484375\n",
      "Epoch 10706: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10722: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10738: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10754: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9302.6162109375\n",
      "Epoch  5942: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5958: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5974: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5990: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8874.27734375\n",
      "Epoch  7146: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7162: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7178: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7194: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15624.197265625\n",
      "Epoch  4263: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4279: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4295: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4311: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.2509765625\n",
      "Epoch  3869: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3885: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3901: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3917: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1682212352752686 s\n",
      "loss_c =  256.5557861328125\n",
      "Epoch 56530: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  255.7662353515625\n",
      "loss_c =  255.60528564453125\n",
      "loss_c =  255.49530029296875\n",
      "loss_c =  255.40557861328125\n",
      "loss_c =  255.32354736328125\n",
      "loss_c =  255.24429321289062\n",
      "loss_c =  255.16494750976562\n",
      "loss_c =  255.08538818359375\n",
      "loss_c =  255.00460815429688\n",
      "thetac step =  6.651107311248779 s\n",
      "Saving model...\n",
      "...saving done.\n",
      "step =  40\n",
      "loss z =  10344435.0\n",
      "loss z =  10326632.0\n",
      "loss z =  10310678.0\n",
      "loss z =  10296128.0\n",
      "loss z =  10282774.0\n",
      "loss z =  10270466.0\n",
      "loss z =  10259204.0\n",
      "loss z =  10249060.0\n",
      "loss z =  10240138.0\n",
      "loss z =  10232451.0\n",
      "loss z =  10226479.0\n",
      "Epoch  1619: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10221828.0\n",
      "loss z =  10221023.0\n",
      "loss z =  10220232.0\n",
      "loss z =  10219459.0\n",
      "loss z =  10218696.0\n",
      "loss z =  10217948.0\n",
      "loss z =  10217208.0\n",
      "loss z =  10216480.0\n",
      "loss z =  10215046.0\n",
      "loss z =  10214340.0\n",
      "Epoch  1630: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10213641.0\n",
      "loss z =  10213502.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss z =  10213362.0\n",
      "loss z =  10213225.0\n",
      "loss z =  10213088.0\n",
      "loss z =  10212948.0\n",
      "loss z =  10212811.0\n",
      "loss z =  10212674.0\n",
      "loss z =  10212537.0\n",
      "loss z =  10212400.0\n",
      "loss z =  10212263.0\n",
      "Epoch  1641: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10212127.0\n",
      "loss z =  10212099.0\n",
      "loss z =  10212072.0\n",
      "loss z =  10212045.0\n",
      "loss z =  10212018.0\n",
      "loss z =  10211990.0\n",
      "loss z =  10211937.0\n",
      "loss z =  10211910.0\n",
      "loss z =  10211883.0\n",
      "loss z =  10211855.0\n",
      "Epoch  1652: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  25.630122661590576 s\n",
      "loss_f =  7521719.5\n",
      "loss_f =  7509704.5\n",
      "loss_f =  7499644.5\n",
      "loss_f =  7491442.5\n",
      "loss_f =  7483431.0\n",
      "loss_f =  7477002.5\n",
      "Epoch  7045: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7473225.5\n",
      "loss_f =  7471284.0\n",
      "loss_f =  7469565.0\n",
      "loss_f =  7468148.5\n",
      "loss_f =  7466851.0\n",
      "loss_f =  7465636.0\n",
      "loss_f =  7464506.5\n",
      "loss_f =  7463420.0\n",
      "loss_f =  7462382.5\n",
      "loss_f =  7461379.0\n",
      "loss_f =  7460406.0\n",
      "loss_f =  7459461.0\n",
      "loss_f =  7458522.0\n",
      "loss_f =  7457603.0\n",
      "loss_f =  7456707.0\n",
      "loss_f =  7455823.0\n",
      "loss_f =  7454957.0\n",
      "loss_f =  7454106.5\n",
      "loss_f =  7453258.0\n",
      "loss_f =  7452417.0\n",
      "loss_f =  7451596.0\n",
      "loss_f =  7450792.0\n",
      "loss_f =  7449975.0\n",
      "loss_f =  7449189.5\n",
      "loss_f =  7448398.0\n",
      "loss_f =  7447625.0\n",
      "loss_f =  7446858.0\n",
      "loss_f =  7446102.5\n",
      "loss_f =  7445354.0\n",
      "loss_f =  7444619.5\n",
      "loss_f =  7443878.0\n",
      "loss_f =  7443154.0\n",
      "loss_f =  7442437.5\n",
      "loss_f =  7441726.0\n",
      "thetaf step =  140.9642572402954 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11075.1904296875\n",
      "Epoch  5129: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5145: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5161: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5177: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6410.4521484375\n",
      "Epoch  4352: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4368: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4384: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4400: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30096.93359375\n",
      "Epoch 11172: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11188: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11204: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11220: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.43359375\n",
      "Epoch  6212: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6228: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6244: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6260: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.72265625\n",
      "Epoch  7274: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7290: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7306: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7322: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.046875\n",
      "Epoch  4412: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4428: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4444: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4460: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23246.853515625\n",
      "Epoch  5299: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5315: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5331: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5347: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.2783203125\n",
      "Epoch 14618: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 14634: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 14650: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 14666: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.6923828125\n",
      "Epoch  5740: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5756: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5772: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5788: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8146.29296875\n",
      "Epoch  4246: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4262: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4278: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4294: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19104.12109375\n",
      "Epoch  5782: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5798: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5814: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5830: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10355.3193359375\n",
      "Epoch 10770: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10786: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10802: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10818: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9302.44140625\n",
      "Epoch  6006: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6022: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6038: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6054: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8874.419921875\n",
      "Epoch  7210: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7226: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7242: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7258: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15623.9765625\n",
      "Epoch  4327: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4343: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4359: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4375: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.2269287109375\n",
      "Epoch  3933: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  3949: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  3965: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  3981: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1730422973632812 s\n",
      "loss_c =  256.3629455566406\n",
      "loss_c =  255.25306701660156\n",
      "loss_c =  254.899169921875\n",
      "loss_c =  254.553466796875\n",
      "loss_c =  254.2074737548828\n",
      "loss_c =  253.85899353027344\n",
      "loss_c =  253.50701904296875\n",
      "loss_c =  253.15087890625\n",
      "loss_c =  252.7900848388672\n",
      "loss_c =  252.4244842529297\n",
      "thetac step =  6.998544454574585 s\n",
      "step =  41\n",
      "loss z =  10284686.0\n",
      "loss z =  10270939.0\n",
      "loss z =  10258402.0\n",
      "loss z =  10247037.0\n",
      "loss z =  10236850.0\n",
      "loss z =  10227918.0\n",
      "loss z =  10220252.0\n",
      "loss z =  10214197.0\n",
      "loss z =  10209452.0\n",
      "loss z =  10205480.0\n",
      "loss z =  10201898.0\n",
      "Epoch  1664: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10198527.0\n",
      "loss z =  10197874.0\n",
      "loss z =  10197227.0\n",
      "loss z =  10196586.0\n",
      "loss z =  10195951.0\n",
      "loss z =  10195321.0\n",
      "loss z =  10194695.0\n",
      "loss z =  10194076.0\n",
      "loss z =  10192845.0\n",
      "loss z =  10192237.0\n",
      "Epoch  1675: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10191632.0\n",
      "loss z =  10191512.0\n",
      "loss z =  10191389.0\n",
      "loss z =  10191270.0\n",
      "loss z =  10191149.0\n",
      "loss z =  10191030.0\n",
      "loss z =  10190911.0\n",
      "loss z =  10190792.0\n",
      "loss z =  10190674.0\n",
      "loss z =  10190555.0\n",
      "loss z =  10190434.0\n",
      "Epoch  1686: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10190315.0\n",
      "loss z =  10190292.0\n",
      "loss z =  10190268.0\n",
      "loss z =  10190244.0\n",
      "loss z =  10190220.0\n",
      "loss z =  10190196.0\n",
      "loss z =  10190148.0\n",
      "loss z =  10190124.0\n",
      "loss z =  10190101.0\n",
      "loss z =  10190078.0\n",
      "Epoch  1697: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  25.4361355304718 s\n",
      "loss_f =  7503002.0\n",
      "loss_f =  7488625.5\n",
      "loss_f =  7479553.5\n",
      "loss_f =  7471730.0\n",
      "loss_f =  7464814.0\n",
      "loss_f =  7459297.0\n",
      "Epoch  7244: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7455899.0\n",
      "loss_f =  7454088.0\n",
      "loss_f =  7452487.0\n",
      "loss_f =  7451100.0\n",
      "loss_f =  7449854.0\n",
      "loss_f =  7448661.0\n",
      "Epoch  7275: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7447663.5\n",
      "loss_f =  7447139.0\n",
      "loss_f =  7446616.0\n",
      "loss_f =  7446104.0\n",
      "loss_f =  7445605.5\n",
      "loss_f =  7445105.0\n",
      "Epoch  7306: reducing learning rate of group 0 to 6.2500e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_f =  7444620.0\n",
      "loss_f =  7444372.0\n",
      "loss_f =  7444137.0\n",
      "loss_f =  7443898.0\n",
      "loss_f =  7443659.0\n",
      "loss_f =  7443422.5\n",
      "loss_f =  7443185.5\n",
      "Epoch  7337: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  7443047.5\n",
      "loss_f =  7442928.0\n",
      "loss_f =  7442814.0\n",
      "loss_f =  7442692.0\n",
      "loss_f =  7442570.0\n",
      "loss_f =  7442458.0\n",
      "Epoch  7368: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  7442378.5\n",
      "loss_f =  7442312.0\n",
      "loss_f =  7442249.5\n",
      "loss_f =  7442204.0\n",
      "loss_f =  7442133.0\n",
      "loss_f =  7442086.0\n",
      "Epoch  7399: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  7442038.0\n",
      "loss_f =  7442006.0\n",
      "loss_f =  7441976.0\n",
      "thetaf step =  140.5460696220398 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11075.2939453125\n",
      "Epoch  5193: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5209: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5225: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5241: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6410.390625\n",
      "Epoch  4416: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4432: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4448: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4464: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30097.1640625\n",
      "Epoch 11236: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11252: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11268: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11284: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.484375\n",
      "Epoch  6276: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6292: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6308: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6324: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.76171875\n",
      "Epoch  7338: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7354: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7370: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7386: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.10546875\n",
      "Epoch  4476: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4492: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4508: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4524: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23247.115234375\n",
      "Epoch  5363: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5379: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5395: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5411: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.1513671875\n",
      "Epoch 14682: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 14698: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 14714: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 14730: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.820068359375\n",
      "Epoch  5804: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5820: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5836: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5852: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8146.00634765625\n",
      "Epoch  4310: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4326: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4342: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4358: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19104.37109375\n",
      "Epoch  5846: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5862: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5878: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5894: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10355.3935546875\n",
      "Epoch 10834: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10850: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10866: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10882: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9302.02734375\n",
      "Epoch  6071: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6087: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6103: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6119: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8874.3583984375\n",
      "Epoch  7274: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7290: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7306: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7322: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15624.0810546875\n",
      "Epoch  4391: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4407: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4423: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4439: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.2939453125\n",
      "Epoch  3997: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4013: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4029: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4045: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1682438850402832 s\n",
      "loss_c =  256.7799072265625\n",
      "Epoch 57531: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  255.9073944091797\n",
      "loss_c =  255.75303649902344\n",
      "loss_c =  255.65045166015625\n",
      "loss_c =  255.56231689453125\n",
      "loss_c =  255.47775268554688\n",
      "loss_c =  255.39407348632812\n",
      "loss_c =  255.30999755859375\n",
      "loss_c =  255.22531127929688\n",
      "loss_c =  255.1403045654297\n",
      "thetac step =  6.370927333831787 s\n",
      "step =  42\n",
      "loss z =  10253688.0\n",
      "loss z =  10239159.0\n",
      "loss z =  10226246.0\n",
      "loss z =  10214917.0\n",
      "loss z =  10205234.0\n",
      "loss z =  10197249.0\n",
      "loss z =  10190784.0\n",
      "loss z =  10185831.0\n",
      "loss z =  10181781.0\n",
      "loss z =  10178175.0\n",
      "loss z =  10174824.0\n",
      "Epoch  1709: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10171648.0\n",
      "loss z =  10171034.0\n",
      "loss z =  10170426.0\n",
      "loss z =  10169827.0\n",
      "loss z =  10169229.0\n",
      "loss z =  10168637.0\n",
      "loss z =  10168049.0\n",
      "loss z =  10167468.0\n",
      "loss z =  10166305.0\n",
      "loss z =  10165730.0\n",
      "Epoch  1720: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10165160.0\n",
      "loss z =  10165048.0\n",
      "loss z =  10164933.0\n",
      "loss z =  10164820.0\n",
      "loss z =  10164707.0\n",
      "loss z =  10164594.0\n",
      "loss z =  10164480.0\n",
      "loss z =  10164368.0\n",
      "loss z =  10164254.0\n",
      "loss z =  10164144.0\n",
      "loss z =  10164031.0\n",
      "Epoch  1731: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10163920.0\n",
      "loss z =  10163898.0\n",
      "loss z =  10163876.0\n",
      "loss z =  10163853.0\n",
      "loss z =  10163831.0\n",
      "loss z =  10163808.0\n",
      "loss z =  10163764.0\n",
      "loss z =  10163742.0\n",
      "loss z =  10163720.0\n",
      "loss z =  10163698.0\n",
      "Epoch  1742: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  25.364466667175293 s\n",
      "loss_f =  7479934.0\n",
      "loss_f =  7470707.0\n",
      "loss_f =  7461201.0\n",
      "Epoch  7430: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7454542.5\n",
      "loss_f =  7450318.0\n",
      "loss_f =  7446514.5\n",
      "loss_f =  7443834.0\n",
      "loss_f =  7441675.0\n",
      "loss_f =  7439881.0\n",
      "loss_f =  7438364.5\n",
      "loss_f =  7437039.5\n",
      "loss_f =  7435838.5\n",
      "loss_f =  7434699.0\n",
      "loss_f =  7433628.0\n",
      "loss_f =  7432610.5\n",
      "loss_f =  7431607.5\n",
      "loss_f =  7430651.0\n",
      "loss_f =  7429699.0\n",
      "loss_f =  7428756.0\n",
      "loss_f =  7427819.5\n",
      "loss_f =  7426909.0\n",
      "loss_f =  7425990.5\n",
      "loss_f =  7425119.0\n",
      "loss_f =  7424233.0\n",
      "loss_f =  7423363.0\n",
      "loss_f =  7422511.0\n",
      "loss_f =  7421657.0\n",
      "loss_f =  7420829.0\n",
      "loss_f =  7420010.5\n",
      "loss_f =  7419204.5\n",
      "loss_f =  7418399.0\n",
      "loss_f =  7417607.0\n",
      "loss_f =  7416819.5\n",
      "loss_f =  7416062.0\n",
      "loss_f =  7415292.5\n",
      "loss_f =  7414546.5\n",
      "loss_f =  7413793.0\n",
      "loss_f =  7413063.0\n",
      "loss_f =  7412339.0\n",
      "loss_f =  7411608.5\n",
      "thetaf step =  142.07057905197144 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11075.23828125\n",
      "Epoch  5257: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5273: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5289: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5305: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6410.39453125\n",
      "Epoch  4480: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4496: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4512: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4528: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30097.189453125\n",
      "Epoch 11300: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11316: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11332: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11348: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.5\n",
      "Epoch  6340: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6356: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6372: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6388: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.798828125\n",
      "Epoch  7402: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7418: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7434: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7450: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.11328125\n",
      "Epoch  4540: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4556: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4572: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4588: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23247.119140625\n",
      "Epoch  5427: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5443: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5459: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5475: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.0830078125\n",
      "Epoch 14746: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 14762: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 14778: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 14794: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.850341796875\n",
      "Epoch  5868: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5884: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5900: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5916: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8145.7890625\n",
      "Epoch  4374: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4390: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4406: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4422: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19104.38671875\n",
      "Epoch  5910: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5926: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5942: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5958: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10355.447265625\n",
      "Epoch 10898: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10914: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10930: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 10946: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9301.87890625\n",
      "Epoch  6135: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6151: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6167: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6183: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8874.3857421875\n",
      "Epoch  7338: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7354: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7370: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7386: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15624.669921875\n",
      "Epoch  4455: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4471: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4487: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4503: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.277587890625\n",
      "Epoch  4061: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4077: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4093: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4109: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1686336994171143 s\n",
      "loss_c =  257.0831298828125\n",
      "loss_c =  255.32220458984375\n",
      "loss_c =  254.85263061523438\n",
      "loss_c =  254.48622131347656\n",
      "loss_c =  254.13287353515625\n",
      "loss_c =  253.78305053710938\n",
      "loss_c =  253.4331817626953\n",
      "loss_c =  253.0809783935547\n",
      "loss_c =  252.72463989257812\n",
      "loss_c =  252.3626251220703\n",
      "thetac step =  6.920786619186401 s\n",
      "step =  43\n",
      "loss z =  10218638.0\n",
      "loss z =  10205872.0\n",
      "loss z =  10194906.0\n",
      "loss z =  10185778.0\n",
      "loss z =  10178364.0\n",
      "loss z =  10172380.0\n",
      "loss z =  10167496.0\n",
      "loss z =  10163178.0\n",
      "loss z =  10159196.0\n",
      "loss z =  10155444.0\n",
      "loss z =  10151867.0\n",
      "Epoch  1754: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10148437.0\n",
      "loss z =  10147772.0\n",
      "loss z =  10147113.0\n",
      "loss z =  10146458.0\n",
      "loss z =  10145810.0\n",
      "loss z =  10145164.0\n",
      "loss z =  10144521.0\n",
      "loss z =  10143882.0\n",
      "loss z =  10142616.0\n",
      "loss z =  10141991.0\n",
      "Epoch  1765: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10141370.0\n",
      "loss z =  10141245.0\n",
      "loss z =  10141122.0\n",
      "loss z =  10140998.0\n",
      "loss z =  10140874.0\n",
      "loss z =  10140751.0\n",
      "loss z =  10140628.0\n",
      "loss z =  10140504.0\n",
      "loss z =  10140380.0\n",
      "loss z =  10140259.0\n",
      "loss z =  10140137.0\n",
      "Epoch  1776: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10140016.0\n",
      "loss z =  10139992.0\n",
      "loss z =  10139968.0\n",
      "loss z =  10139943.0\n",
      "loss z =  10139918.0\n",
      "loss z =  10139894.0\n",
      "loss z =  10139845.0\n",
      "loss z =  10139820.0\n",
      "loss z =  10139796.0\n",
      "loss z =  10139770.0\n",
      "Epoch  1787: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  25.70949125289917 s\n",
      "loss_f =  7459231.0\n",
      "loss_f =  7447093.0\n",
      "loss_f =  7440255.0\n",
      "loss_f =  7432271.0\n",
      "loss_f =  7425606.0\n",
      "loss_f =  7420456.0\n",
      "Epoch  7645: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7417375.0\n",
      "loss_f =  7415578.0\n",
      "loss_f =  7414153.0\n",
      "loss_f =  7412816.0\n",
      "loss_f =  7411698.0\n",
      "loss_f =  7410600.0\n",
      "loss_f =  7409572.0\n",
      "loss_f =  7408605.0\n",
      "loss_f =  7407656.0\n",
      "loss_f =  7406750.0\n",
      "loss_f =  7405844.0\n",
      "loss_f =  7404971.0\n",
      "loss_f =  7404118.0\n",
      "loss_f =  7403274.5\n",
      "loss_f =  7402450.0\n",
      "loss_f =  7401634.0\n",
      "loss_f =  7400826.5\n",
      "loss_f =  7400033.5\n",
      "loss_f =  7399243.5\n",
      "loss_f =  7398482.0\n",
      "loss_f =  7397717.0\n",
      "loss_f =  7396960.0\n",
      "loss_f =  7396219.0\n",
      "loss_f =  7395481.0\n",
      "loss_f =  7394749.0\n",
      "loss_f =  7394024.5\n",
      "loss_f =  7393316.5\n",
      "loss_f =  7392607.0\n",
      "loss_f =  7391898.0\n",
      "loss_f =  7391196.5\n",
      "loss_f =  7390520.0\n",
      "loss_f =  7389830.0\n",
      "loss_f =  7389155.0\n",
      "loss_f =  7388489.0\n",
      "thetaf step =  142.8528323173523 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11075.328125\n",
      "Epoch  5321: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5337: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5353: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5369: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6410.3515625\n",
      "Epoch  4544: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4560: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4576: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4592: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30097.4140625\n",
      "Epoch 11364: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11380: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11396: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11412: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.55859375\n",
      "Epoch  6404: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6420: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6436: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6452: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.87109375\n",
      "Epoch  7466: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7482: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7498: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7514: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.1875\n",
      "Epoch  4604: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4620: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4636: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4652: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23247.33984375\n",
      "Epoch  5491: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5507: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5523: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5539: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14113.9814453125\n",
      "Epoch 14810: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 14826: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 14842: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 14858: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.978515625\n",
      "Epoch  5932: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5948: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5964: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5980: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8145.00244140625\n",
      "Epoch  4438: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4454: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4470: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4486: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19104.552734375\n",
      "Epoch  5974: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5990: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6006: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6022: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10355.634765625\n",
      "Epoch 10962: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10978: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 10994: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11010: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9301.4599609375\n",
      "Epoch  6199: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6215: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6231: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6247: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8874.404296875\n",
      "Epoch  7402: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7418: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7434: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7450: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15624.3134765625\n",
      "Epoch  4519: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4535: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4551: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4567: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.30322265625\n",
      "Epoch  4125: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4141: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4157: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4173: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.145578384399414 s\n",
      "loss_c =  256.6385803222656\n",
      "Epoch 58532: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  255.70750427246094\n",
      "loss_c =  255.56825256347656\n",
      "loss_c =  255.46871948242188\n",
      "loss_c =  255.3838653564453\n",
      "loss_c =  255.3042755126953\n",
      "loss_c =  255.22610473632812\n",
      "loss_c =  255.14791870117188\n",
      "loss_c =  255.06903076171875\n",
      "loss_c =  254.9893035888672\n",
      "thetac step =  7.287069082260132 s\n",
      "step =  44\n",
      "loss z =  10195531.0\n",
      "loss z =  10183423.0\n",
      "loss z =  10173518.0\n",
      "loss z =  10165674.0\n",
      "loss z =  10159410.0\n",
      "loss z =  10154128.0\n",
      "loss z =  10149437.0\n",
      "loss z =  10145150.0\n",
      "loss z =  10141165.0\n",
      "loss z =  10137424.0\n",
      "loss z =  10133884.0\n",
      "Epoch  1799: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10130514.0\n",
      "loss z =  10129860.0\n",
      "loss z =  10129216.0\n",
      "loss z =  10128573.0\n",
      "loss z =  10127936.0\n",
      "loss z =  10127306.0\n",
      "loss z =  10126679.0\n",
      "loss z =  10126058.0\n",
      "loss z =  10124827.0\n",
      "loss z =  10124217.0\n",
      "Epoch  1810: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10123614.0\n",
      "loss z =  10123493.0\n",
      "loss z =  10123372.0\n",
      "loss z =  10123251.0\n",
      "loss z =  10123132.0\n",
      "loss z =  10123011.0\n",
      "loss z =  10122892.0\n",
      "loss z =  10122774.0\n",
      "loss z =  10122654.0\n",
      "loss z =  10122536.0\n",
      "loss z =  10122416.0\n",
      "Epoch  1821: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10122299.0\n",
      "loss z =  10122275.0\n",
      "loss z =  10122252.0\n",
      "loss z =  10122228.0\n",
      "loss z =  10122204.0\n",
      "loss z =  10122180.0\n",
      "loss z =  10122133.0\n",
      "loss z =  10122110.0\n",
      "loss z =  10122086.0\n",
      "loss z =  10122062.0\n",
      "Epoch  1832: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  25.76599144935608 s\n",
      "loss_f =  7444518.0\n",
      "loss_f =  7433536.0\n",
      "loss_f =  7425080.0\n",
      "loss_f =  7418926.0\n",
      "loss_f =  7411892.5\n",
      "loss_f =  7407016.0\n",
      "Epoch  7845: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7403541.0\n",
      "loss_f =  7401961.5\n",
      "loss_f =  7400590.0\n",
      "loss_f =  7399336.0\n",
      "loss_f =  7398181.0\n",
      "loss_f =  7397122.5\n",
      "Epoch  7876: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7396116.0\n",
      "loss_f =  7395640.0\n",
      "loss_f =  7395159.0\n",
      "loss_f =  7394696.0\n",
      "loss_f =  7394241.0\n",
      "loss_f =  7393788.5\n",
      "loss_f =  7393344.5\n",
      "Epoch  7907: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  7393078.5\n",
      "loss_f =  7392868.5\n",
      "loss_f =  7392649.0\n",
      "loss_f =  7392422.0\n",
      "loss_f =  7392213.0\n",
      "loss_f =  7391994.0\n",
      "Epoch  7938: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  7391845.0\n",
      "loss_f =  7391735.0\n",
      "loss_f =  7391616.0\n",
      "loss_f =  7391512.0\n",
      "loss_f =  7391409.0\n",
      "loss_f =  7391302.0\n",
      "Epoch  7969: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  7391208.5\n",
      "loss_f =  7391167.0\n",
      "loss_f =  7391103.5\n",
      "loss_f =  7391058.0\n",
      "loss_f =  7391000.0\n",
      "loss_f =  7390943.0\n",
      "Epoch  8000: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  7390895.5\n",
      "loss_f =  7390869.5\n",
      "loss_f =  7390849.0\n",
      "thetaf step =  143.13104820251465 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11075.408203125\n",
      "Epoch  5385: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5401: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5417: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5433: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6410.27099609375\n",
      "Epoch  4608: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4624: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4640: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4656: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30097.44921875\n",
      "Epoch 11428: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11444: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11460: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11476: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.56640625\n",
      "Epoch  6468: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6484: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6500: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6516: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.966796875\n",
      "Epoch  7530: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7546: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7562: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7578: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.2421875\n",
      "Epoch  4668: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4684: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4700: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4716: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23247.458984375\n",
      "Epoch  5555: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5571: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5587: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5603: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14113.962890625\n",
      "Epoch 14874: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 14890: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 14906: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 14922: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.997314453125\n",
      "Epoch  5996: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6012: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6028: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6044: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8144.6025390625\n",
      "Epoch  4502: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4518: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4534: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4550: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19104.67578125\n",
      "Epoch  6038: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6054: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6070: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6086: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10355.611328125\n",
      "Epoch 11026: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11042: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11058: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11074: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9301.322265625\n",
      "Epoch  6263: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6279: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6295: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6311: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8874.322265625\n",
      "Epoch  7466: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7482: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7498: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7514: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15624.767578125\n",
      "Epoch  4583: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4599: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4615: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4631: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.322998046875\n",
      "Epoch  4189: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4205: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4221: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4237: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1494312286376953 s\n",
      "loss_c =  256.90509033203125\n",
      "loss_c =  255.26382446289062\n",
      "loss_c =  254.81375122070312\n",
      "loss_c =  254.49224853515625\n",
      "loss_c =  254.17210388183594\n",
      "loss_c =  253.84950256347656\n",
      "loss_c =  253.52291870117188\n",
      "loss_c =  253.1861572265625\n",
      "loss_c =  252.8494110107422\n",
      "loss_c =  252.5076141357422\n",
      "thetac step =  7.577383995056152 s\n",
      "step =  45\n",
      "loss z =  10222413.0\n",
      "loss z =  10205545.0\n",
      "loss z =  10190232.0\n",
      "loss z =  10176498.0\n",
      "loss z =  10164466.0\n",
      "loss z =  10154354.0\n",
      "loss z =  10146230.0\n",
      "loss z =  10139776.0\n",
      "loss z =  10134383.0\n",
      "loss z =  10129614.0\n",
      "Epoch  1843: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10125258.0\n",
      "loss z =  10124442.0\n",
      "loss z =  10123634.0\n",
      "loss z =  10122842.0\n",
      "loss z =  10122062.0\n",
      "loss z =  10121288.0\n",
      "loss z =  10120524.0\n",
      "loss z =  10119771.0\n",
      "loss z =  10119026.0\n",
      "loss z =  10117566.0\n",
      "Epoch  1854: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10116845.0\n",
      "loss z =  10116700.0\n",
      "loss z =  10116557.0\n",
      "loss z =  10116416.0\n",
      "loss z =  10116274.0\n",
      "loss z =  10116132.0\n",
      "loss z =  10115990.0\n",
      "loss z =  10115848.0\n",
      "loss z =  10115708.0\n",
      "loss z =  10115567.0\n",
      "loss z =  10115428.0\n",
      "Epoch  1865: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10115290.0\n",
      "loss z =  10115262.0\n",
      "loss z =  10115233.0\n",
      "loss z =  10115205.0\n",
      "loss z =  10115178.0\n",
      "loss z =  10115150.0\n",
      "loss z =  10115122.0\n",
      "loss z =  10115067.0\n",
      "loss z =  10115038.0\n",
      "loss z =  10115011.0\n",
      "Epoch  1876: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  25.093626022338867 s\n",
      "loss_f =  7439880.0\n",
      "loss_f =  7426540.5\n",
      "loss_f =  7419828.5\n",
      "Epoch  8031: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7412375.0\n",
      "loss_f =  7408987.0\n",
      "loss_f =  7405595.0\n",
      "loss_f =  7403030.0\n",
      "loss_f =  7400927.0\n",
      "loss_f =  7399146.0\n",
      "loss_f =  7397669.5\n",
      "Epoch  8062: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7396852.0\n",
      "loss_f =  7396210.0\n",
      "loss_f =  7395604.0\n",
      "loss_f =  7395029.0\n",
      "loss_f =  7394465.0\n",
      "loss_f =  7393904.0\n",
      "Epoch  8093: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  7393531.0\n",
      "loss_f =  7393267.5\n",
      "loss_f =  7393010.0\n",
      "loss_f =  7392751.0\n",
      "loss_f =  7392473.0\n",
      "loss_f =  7392227.5\n",
      "Epoch  8124: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  7392022.0\n",
      "loss_f =  7391893.0\n",
      "loss_f =  7391767.5\n",
      "loss_f =  7391640.0\n",
      "loss_f =  7391518.0\n",
      "loss_f =  7391388.0\n",
      "Epoch  8155: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  7391274.0\n",
      "loss_f =  7391211.0\n",
      "loss_f =  7391153.0\n",
      "loss_f =  7391093.0\n",
      "loss_f =  7391020.5\n",
      "loss_f =  7390960.5\n",
      "Epoch  8186: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  7390900.0\n",
      "loss_f =  7390865.0\n",
      "loss_f =  7390832.5\n",
      "loss_f =  7390801.0\n",
      "loss_f =  7390766.0\n",
      "loss_f =  7390734.0\n",
      "thetaf step =  144.96067714691162 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11075.4462890625\n",
      "Epoch  5449: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5465: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5481: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5497: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6410.2333984375\n",
      "Epoch  4672: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4688: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4704: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4720: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30097.666015625\n",
      "Epoch 11492: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11508: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11524: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11540: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.59375\n",
      "Epoch  6532: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6548: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6564: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6580: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24293.015625\n",
      "Epoch  7594: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7610: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7626: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7642: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.29296875\n",
      "Epoch  4732: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4748: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4764: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4780: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23247.650390625\n",
      "Epoch  5619: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5635: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5651: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5667: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14113.916015625\n",
      "Epoch 14938: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 14954: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 14970: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 14986: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2423.1171875\n",
      "Epoch  6060: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6076: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6092: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6108: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8144.21337890625\n",
      "Epoch  4566: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4582: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4598: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4614: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19104.833984375\n",
      "Epoch  6102: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6118: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6134: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6150: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10355.8388671875\n",
      "Epoch 11090: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11106: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11122: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11138: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9300.876953125\n",
      "Epoch  6328: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6344: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6360: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6376: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8874.31640625\n",
      "Epoch  7530: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7546: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7562: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7578: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15626.0322265625\n",
      "Epoch  4647: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4663: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4679: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4695: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.3216552734375\n",
      "Epoch  4253: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4269: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4285: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4301: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.2236456871032715 s\n",
      "loss_c =  258.32403564453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59533: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  256.20855712890625\n",
      "loss_c =  255.97409057617188\n",
      "loss_c =  255.8062286376953\n",
      "loss_c =  255.67306518554688\n",
      "loss_c =  255.55804443359375\n",
      "loss_c =  255.45281982421875\n",
      "loss_c =  255.35305786132812\n",
      "loss_c =  255.25689697265625\n",
      "loss_c =  255.16253662109375\n",
      "thetac step =  7.96532940864563 s\n",
      "step =  46\n",
      "loss z =  10247017.0\n",
      "loss z =  10227393.0\n",
      "loss z =  10209922.0\n",
      "loss z =  10194306.0\n",
      "loss z =  10180287.0\n",
      "loss z =  10167791.0\n",
      "loss z =  10156794.0\n",
      "loss z =  10147367.0\n",
      "loss z =  10139554.0\n",
      "loss z =  10133137.0\n",
      "loss z =  10127848.0\n",
      "Epoch  1888: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10123391.0\n",
      "loss z =  10122581.0\n",
      "loss z =  10121781.0\n",
      "loss z =  10120994.0\n",
      "loss z =  10120219.0\n",
      "loss z =  10119455.0\n",
      "loss z =  10118698.0\n",
      "loss z =  10117949.0\n",
      "loss z =  10116476.0\n",
      "loss z =  10115752.0\n",
      "Epoch  1899: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10115036.0\n",
      "loss z =  10114893.0\n",
      "loss z =  10114750.0\n",
      "loss z =  10114608.0\n",
      "loss z =  10114465.0\n",
      "loss z =  10114324.0\n",
      "loss z =  10114184.0\n",
      "loss z =  10114043.0\n",
      "loss z =  10113904.0\n",
      "loss z =  10113764.0\n",
      "loss z =  10113623.0\n",
      "Epoch  1910: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10113484.0\n",
      "loss z =  10113456.0\n",
      "loss z =  10113428.0\n",
      "loss z =  10113400.0\n",
      "loss z =  10113372.0\n",
      "loss z =  10113344.0\n",
      "loss z =  10113288.0\n",
      "loss z =  10113260.0\n",
      "loss z =  10113234.0\n",
      "loss z =  10113204.0\n",
      "Epoch  1921: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  27.296518564224243 s\n",
      "loss_f =  7440842.0\n",
      "Epoch  8217: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7430113.0\n",
      "loss_f =  7425172.5\n",
      "loss_f =  7419894.0\n",
      "loss_f =  7416226.5\n",
      "loss_f =  7412907.5\n",
      "loss_f =  7410127.0\n",
      "Epoch  8248: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7408493.5\n",
      "loss_f =  7407424.0\n",
      "loss_f =  7406457.5\n",
      "loss_f =  7405586.0\n",
      "loss_f =  7404767.5\n",
      "loss_f =  7403995.0\n",
      "Epoch  8279: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  7403393.0\n",
      "loss_f =  7403038.0\n",
      "loss_f =  7402684.0\n",
      "loss_f =  7402324.0\n",
      "loss_f =  7401980.0\n",
      "loss_f =  7401630.5\n",
      "Epoch  8310: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  7401320.0\n",
      "loss_f =  7401149.0\n",
      "loss_f =  7400987.0\n",
      "loss_f =  7400816.5\n",
      "loss_f =  7400641.5\n",
      "loss_f =  7400477.0\n",
      "Epoch  8341: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  7400312.0\n",
      "loss_f =  7400226.5\n",
      "loss_f =  7400139.0\n",
      "loss_f =  7400064.0\n",
      "loss_f =  7399976.0\n",
      "loss_f =  7399898.0\n",
      "loss_f =  7399811.0\n",
      "Epoch  8372: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  7399766.0\n",
      "loss_f =  7399718.0\n",
      "loss_f =  7399682.0\n",
      "loss_f =  7399638.0\n",
      "loss_f =  7399591.5\n",
      "loss_f =  7399545.5\n",
      "Epoch  8403: reducing learning rate of group 0 to 3.9063e-06.\n",
      "thetaf step =  138.05711889266968 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11075.60546875\n",
      "Epoch  5513: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5529: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5545: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5561: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6409.9873046875\n",
      "Epoch  4736: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4752: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4768: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4784: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30097.6953125\n",
      "Epoch 11556: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11572: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11588: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11604: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.60546875\n",
      "Epoch  6596: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6612: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6628: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6644: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24293.01953125\n",
      "Epoch  7658: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7674: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7690: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7706: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.3984375\n",
      "Epoch  4796: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4812: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4828: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4844: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23247.833984375\n",
      "Epoch  5683: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5699: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5715: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5731: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.02734375\n",
      "Epoch 15002: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 15018: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 15034: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 15050: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2423.10986328125\n",
      "Epoch  6124: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6140: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6156: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6172: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8143.7314453125\n",
      "Epoch  4630: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4646: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4662: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4678: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19104.943359375\n",
      "Epoch  6166: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6182: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6198: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6214: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10355.80859375\n",
      "Epoch 11154: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11170: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11186: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11202: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9300.7021484375\n",
      "Epoch  6392: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6408: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6424: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6440: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8874.1611328125\n",
      "Epoch  7594: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7610: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7626: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7642: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15624.0341796875\n",
      "Epoch  4711: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4727: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4743: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4759: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.269287109375\n",
      "Epoch  4317: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4333: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4349: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4365: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1363024711608887 s\n",
      "loss_c =  255.8919677734375\n",
      "loss_c =  255.03646850585938\n",
      "loss_c =  254.7257843017578\n",
      "loss_c =  254.40965270996094\n",
      "loss_c =  254.058349609375\n",
      "loss_c =  253.7196807861328\n",
      "loss_c =  253.37704467773438\n",
      "loss_c =  253.02767944335938\n",
      "loss_c =  252.6706085205078\n",
      "loss_c =  252.3048095703125\n",
      "thetac step =  7.680439710617065 s\n",
      "step =  47\n",
      "loss z =  10198480.0\n",
      "loss z =  10181892.0\n",
      "loss z =  10166944.0\n",
      "loss z =  10153611.0\n",
      "loss z =  10141912.0\n",
      "loss z =  10131896.0\n",
      "loss z =  10123717.0\n",
      "loss z =  10117430.0\n",
      "loss z =  10112678.0\n",
      "loss z =  10108768.0\n",
      "Epoch  1932: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10105256.0\n",
      "loss z =  10104594.0\n",
      "loss z =  10103943.0\n",
      "loss z =  10103298.0\n",
      "loss z =  10102659.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss z =  10102026.0\n",
      "loss z =  10101393.0\n",
      "loss z =  10100770.0\n",
      "loss z =  10100156.0\n",
      "loss z =  10098937.0\n",
      "Epoch  1943: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10098336.0\n",
      "loss z =  10098216.0\n",
      "loss z =  10098098.0\n",
      "loss z =  10097978.0\n",
      "loss z =  10097860.0\n",
      "loss z =  10097742.0\n",
      "loss z =  10097622.0\n",
      "loss z =  10097502.0\n",
      "loss z =  10097382.0\n",
      "loss z =  10097263.0\n",
      "loss z =  10097143.0\n",
      "Epoch  1954: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10097024.0\n",
      "loss z =  10096999.0\n",
      "loss z =  10096976.0\n",
      "loss z =  10096952.0\n",
      "loss z =  10096929.0\n",
      "loss z =  10096905.0\n",
      "loss z =  10096882.0\n",
      "loss z =  10096834.0\n",
      "loss z =  10096811.0\n",
      "loss z =  10096786.0\n",
      "Epoch  1965: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.85142207145691 s\n",
      "loss_f =  7427132.0\n",
      "loss_f =  7417933.0\n",
      "loss_f =  7407749.5\n",
      "loss_f =  7400704.5\n",
      "loss_f =  7394081.5\n",
      "loss_f =  7388622.5\n",
      "loss_f =  7384705.0\n",
      "loss_f =  7381476.0\n",
      "loss_f =  7378898.0\n",
      "loss_f =  7376518.0\n",
      "loss_f =  7374440.0\n",
      "loss_f =  7372448.0\n",
      "loss_f =  7370609.0\n",
      "loss_f =  7368840.0\n",
      "loss_f =  7367146.0\n",
      "loss_f =  7365515.0\n",
      "loss_f =  7363925.0\n",
      "loss_f =  7362366.5\n",
      "loss_f =  7360856.5\n",
      "loss_f =  7359372.0\n",
      "loss_f =  7357933.0\n",
      "loss_f =  7356517.5\n",
      "loss_f =  7355121.0\n",
      "loss_f =  7353749.5\n",
      "loss_f =  7352413.0\n",
      "loss_f =  7351093.5\n",
      "loss_f =  7349806.5\n",
      "loss_f =  7348525.0\n",
      "loss_f =  7347282.0\n",
      "loss_f =  7346053.0\n",
      "loss_f =  7344844.0\n",
      "loss_f =  7343648.0\n",
      "loss_f =  7342479.0\n",
      "loss_f =  7341317.0\n",
      "loss_f =  7340175.0\n",
      "loss_f =  7339065.0\n",
      "loss_f =  7337950.0\n",
      "loss_f =  7336854.0\n",
      "loss_f =  7335789.0\n",
      "loss_f =  7334716.0\n",
      "thetaf step =  141.48861503601074 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11075.490234375\n",
      "Epoch  5577: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5593: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5609: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5625: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6410.17626953125\n",
      "Epoch  4800: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4816: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4832: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4848: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30098.01953125\n",
      "Epoch 11620: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11636: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11652: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11668: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.58203125\n",
      "Epoch  6660: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6676: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6692: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6708: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24293.078125\n",
      "Epoch  7722: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7738: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7754: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7770: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.37109375\n",
      "Epoch  4860: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4876: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4892: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4908: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23247.779296875\n",
      "Epoch  5747: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5763: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5779: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5795: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14113.9990234375\n",
      "Epoch 15066: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 15082: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 15098: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 15114: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2423.219970703125\n",
      "Epoch  6188: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6204: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6220: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6236: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8143.4462890625\n",
      "Epoch  4694: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4710: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4726: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4742: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19105.05078125\n",
      "Epoch  6230: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6246: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6262: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6278: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10356.03125\n",
      "Epoch 11218: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11234: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11250: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11266: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9300.3876953125\n",
      "Epoch  6456: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6472: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6488: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6504: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8874.294921875\n",
      "Epoch  7658: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7674: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7690: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7706: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15624.234375\n",
      "Epoch  4775: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4791: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4807: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4823: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.2557373046875\n",
      "Epoch  4381: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4397: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4413: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4429: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.247459888458252 s\n",
      "loss_c =  256.37506103515625\n",
      "Epoch 60534: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  255.59512329101562\n",
      "loss_c =  255.5170440673828\n",
      "loss_c =  255.44264221191406\n",
      "loss_c =  255.36891174316406\n",
      "loss_c =  255.29481506347656\n",
      "loss_c =  255.22015380859375\n",
      "loss_c =  255.14413452148438\n",
      "loss_c =  255.06695556640625\n",
      "loss_c =  254.9882049560547\n",
      "thetac step =  8.753661632537842 s\n",
      "step =  48\n",
      "loss z =  10137815.0\n",
      "loss z =  10123470.0\n",
      "loss z =  10111185.0\n",
      "loss z =  10100982.0\n",
      "loss z =  10092981.0\n",
      "loss z =  10087039.0\n",
      "loss z =  10082486.0\n",
      "loss z =  10078570.0\n",
      "loss z =  10074936.0\n",
      "loss z =  10071467.0\n",
      "loss z =  10068108.0\n",
      "Epoch  1977: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10064872.0\n",
      "loss z =  10064240.0\n",
      "loss z =  10063614.0\n",
      "loss z =  10062990.0\n",
      "loss z =  10062372.0\n",
      "loss z =  10061756.0\n",
      "loss z =  10061140.0\n",
      "loss z =  10060531.0\n",
      "loss z =  10059320.0\n",
      "loss z =  10058720.0\n",
      "Epoch  1988: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10058125.0\n",
      "loss z =  10058006.0\n",
      "loss z =  10057886.0\n",
      "loss z =  10057767.0\n",
      "loss z =  10057647.0\n",
      "loss z =  10057530.0\n",
      "loss z =  10057412.0\n",
      "loss z =  10057295.0\n",
      "loss z =  10057176.0\n",
      "loss z =  10057058.0\n",
      "loss z =  10056941.0\n",
      "Epoch  1999: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10056824.0\n",
      "loss z =  10056801.0\n",
      "loss z =  10056779.0\n",
      "loss z =  10056754.0\n",
      "loss z =  10056732.0\n",
      "loss z =  10056708.0\n",
      "loss z =  10056661.0\n",
      "loss z =  10056638.0\n",
      "loss z =  10056615.0\n",
      "loss z =  10056592.0\n",
      "Epoch  2010: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  27.941672801971436 s\n",
      "loss_f =  7390110.5\n",
      "loss_f =  7380716.0\n",
      "loss_f =  7369424.0\n",
      "loss_f =  7362823.5\n",
      "loss_f =  7356548.5\n",
      "loss_f =  7352069.0\n",
      "loss_f =  7348242.5\n",
      "Epoch  8634: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7346487.5\n",
      "loss_f =  7345096.5\n",
      "loss_f =  7343846.5\n",
      "loss_f =  7342733.0\n",
      "loss_f =  7341656.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_f =  7340644.0\n",
      "Epoch  8665: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7339955.0\n",
      "loss_f =  7339482.5\n",
      "loss_f =  7339010.5\n",
      "loss_f =  7338543.0\n",
      "loss_f =  7338092.5\n",
      "loss_f =  7337639.0\n",
      "Epoch  8696: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  7337281.0\n",
      "loss_f =  7337055.5\n",
      "loss_f =  7336834.0\n",
      "loss_f =  7336611.5\n",
      "loss_f =  7336400.5\n",
      "loss_f =  7336180.0\n",
      "Epoch  8727: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  7335986.0\n",
      "loss_f =  7335874.5\n",
      "loss_f =  7335768.0\n",
      "loss_f =  7335658.0\n",
      "loss_f =  7335542.0\n",
      "loss_f =  7335438.0\n",
      "Epoch  8758: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  7335326.5\n",
      "loss_f =  7335272.0\n",
      "loss_f =  7335216.0\n",
      "loss_f =  7335166.5\n",
      "loss_f =  7335112.0\n",
      "loss_f =  7335059.0\n",
      "loss_f =  7335005.5\n",
      "Epoch  8789: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  7334962.0\n",
      "loss_f =  7334939.0\n",
      "thetaf step =  150.2099404335022 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11075.5546875\n",
      "Epoch  5641: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5657: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5673: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5689: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6410.10986328125\n",
      "Epoch  4864: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4880: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4896: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4912: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30098.111328125\n",
      "Epoch 11684: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11700: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11716: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11732: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.57421875\n",
      "Epoch  6724: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6740: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6756: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6772: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24293.068359375\n",
      "Epoch  7786: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7802: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7818: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7834: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.40234375\n",
      "Epoch  4924: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4940: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4956: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4972: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23247.892578125\n",
      "Epoch  5811: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5827: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5843: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5859: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.0263671875\n",
      "Epoch 15130: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 15146: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 15162: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 15178: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2423.204833984375\n",
      "Epoch  6252: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6268: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6284: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6300: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8143.21044921875\n",
      "Epoch  4758: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4774: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4790: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4806: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19105.173828125\n",
      "Epoch  6294: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6310: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6326: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6342: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10356.0068359375\n",
      "Epoch 11282: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11298: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11314: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11330: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9300.2607421875\n",
      "Epoch  6520: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6536: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6552: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6568: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8874.1806640625\n",
      "Epoch  7722: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7738: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7754: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7770: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15624.4130859375\n",
      "Epoch  4839: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4855: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4871: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4887: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.27880859375\n",
      "Epoch  4445: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4461: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4477: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4493: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.367786169052124 s\n",
      "loss_c =  256.42388916015625\n",
      "loss_c =  255.2410430908203\n",
      "loss_c =  254.86386108398438\n",
      "loss_c =  254.52474975585938\n",
      "loss_c =  254.18368530273438\n",
      "loss_c =  253.837158203125\n",
      "loss_c =  253.48358154296875\n",
      "loss_c =  253.12228393554688\n",
      "loss_c =  252.75230407714844\n",
      "loss_c =  252.37301635742188\n",
      "thetac step =  9.30546236038208 s\n",
      "step =  49\n",
      "loss z =  10138664.0\n",
      "loss z =  10121783.0\n",
      "loss z =  10107220.0\n",
      "loss z =  10095140.0\n",
      "loss z =  10085593.0\n",
      "loss z =  10078396.0\n",
      "loss z =  10072782.0\n",
      "loss z =  10068043.0\n",
      "loss z =  10063809.0\n",
      "loss z =  10059894.0\n",
      "loss z =  10056262.0\n",
      "Epoch  2022: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10052846.0\n",
      "loss z =  10052195.0\n",
      "loss z =  10051548.0\n",
      "loss z =  10050910.0\n",
      "loss z =  10050277.0\n",
      "loss z =  10049651.0\n",
      "loss z =  10049030.0\n",
      "loss z =  10048419.0\n",
      "loss z =  10047207.0\n",
      "loss z =  10046610.0\n",
      "Epoch  2033: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10046017.0\n",
      "loss z =  10045898.0\n",
      "loss z =  10045782.0\n",
      "loss z =  10045665.0\n",
      "loss z =  10045545.0\n",
      "loss z =  10045426.0\n",
      "loss z =  10045310.0\n",
      "loss z =  10045194.0\n",
      "loss z =  10045079.0\n",
      "loss z =  10044963.0\n",
      "loss z =  10044845.0\n",
      "Epoch  2044: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10044730.0\n",
      "loss z =  10044706.0\n",
      "loss z =  10044684.0\n",
      "loss z =  10044661.0\n",
      "loss z =  10044639.0\n",
      "loss z =  10044616.0\n",
      "loss z =  10044570.0\n",
      "loss z =  10044547.0\n",
      "loss z =  10044524.0\n",
      "loss z =  10044500.0\n",
      "Epoch  2055: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  27.918593883514404 s\n",
      "loss_f =  7380586.0\n",
      "loss_f =  7370381.0\n",
      "loss_f =  7361742.0\n",
      "loss_f =  7355285.0\n",
      "Epoch  8820: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7350910.5\n",
      "loss_f =  7347724.5\n",
      "loss_f =  7345358.5\n",
      "loss_f =  7343441.0\n",
      "loss_f =  7341799.0\n",
      "loss_f =  7340426.0\n",
      "Epoch  8851: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7339437.0\n",
      "loss_f =  7338857.5\n",
      "loss_f =  7338314.0\n",
      "loss_f =  7337779.0\n",
      "loss_f =  7337269.0\n",
      "loss_f =  7336780.0\n",
      "Epoch  8882: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  7336341.0\n",
      "loss_f =  7336099.5\n",
      "loss_f =  7335876.0\n",
      "loss_f =  7335636.0\n",
      "loss_f =  7335397.0\n",
      "loss_f =  7335170.0\n",
      "Epoch  8913: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  7334946.0\n",
      "loss_f =  7334825.0\n",
      "loss_f =  7334718.0\n",
      "loss_f =  7334594.0\n",
      "loss_f =  7334489.5\n",
      "loss_f =  7334379.5\n",
      "loss_f =  7334262.0\n",
      "Epoch  8944: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  7334195.5\n",
      "loss_f =  7334131.0\n",
      "loss_f =  7334081.5\n",
      "loss_f =  7334024.0\n",
      "loss_f =  7333969.0\n",
      "loss_f =  7333908.0\n",
      "Epoch  8975: reducing learning rate of group 0 to 7.8125e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_f =  7333868.0\n",
      "loss_f =  7333849.0\n",
      "loss_f =  7333812.0\n",
      "loss_f =  7333791.0\n",
      "loss_f =  7333751.0\n",
      "thetaf step =  144.97485828399658 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11075.6552734375\n",
      "Epoch  5705: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5721: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5737: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5753: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6410.0380859375\n",
      "Epoch  4928: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4944: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4960: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4976: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30098.44140625\n",
      "Epoch 11748: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11764: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11780: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11796: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.5546875\n",
      "Epoch  6788: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6804: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6820: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6836: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24293.15625\n",
      "Epoch  7850: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7866: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7882: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7898: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.45703125\n",
      "Epoch  4988: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5004: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5020: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5036: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23248.083984375\n",
      "Epoch  5875: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5891: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5907: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5923: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.107421875\n",
      "Epoch 15194: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 15210: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 15226: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 15242: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2423.066650390625\n",
      "Epoch  6316: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6332: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6348: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6364: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8142.5380859375\n",
      "Epoch  4822: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4838: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4854: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4870: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19105.43359375\n",
      "Epoch  6358: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6374: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6390: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6406: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10356.138671875\n",
      "Epoch 11346: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11362: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11378: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11394: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9299.9052734375\n",
      "Epoch  6585: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6601: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6617: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6633: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8874.068359375\n",
      "Epoch  7786: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7802: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7818: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7834: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15624.4775390625\n",
      "Epoch  4903: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4919: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4935: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4951: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.280517578125\n",
      "Epoch  4509: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4525: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4541: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4557: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.2548480033874512 s\n",
      "loss_c =  256.361083984375\n",
      "Epoch 61535: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  255.45855712890625\n",
      "loss_c =  255.35513305664062\n",
      "loss_c =  255.27420043945312\n",
      "loss_c =  255.20001220703125\n",
      "loss_c =  255.12701416015625\n",
      "loss_c =  255.05361938476562\n",
      "loss_c =  254.97901916503906\n",
      "loss_c =  254.90309143066406\n",
      "loss_c =  254.82546997070312\n",
      "thetac step =  8.830963373184204 s\n",
      "step =  50\n",
      "loss z =  10125224.0\n",
      "loss z =  10107334.0\n",
      "loss z =  10092378.0\n",
      "loss z =  10080257.0\n",
      "loss z =  10070810.0\n",
      "loss z =  10063462.0\n",
      "loss z =  10057562.0\n",
      "loss z =  10052533.0\n",
      "loss z =  10048094.0\n",
      "loss z =  10044087.0\n",
      "loss z =  10040404.0\n",
      "Epoch  2067: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10036984.0\n",
      "loss z =  10036336.0\n",
      "loss z =  10035692.0\n",
      "loss z =  10035055.0\n",
      "loss z =  10034429.0\n",
      "loss z =  10033807.0\n",
      "loss z =  10033191.0\n",
      "loss z =  10032581.0\n",
      "loss z =  10031378.0\n",
      "loss z =  10030784.0\n",
      "Epoch  2078: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10030196.0\n",
      "loss z =  10030078.0\n",
      "loss z =  10029960.0\n",
      "loss z =  10029844.0\n",
      "loss z =  10029728.0\n",
      "loss z =  10029612.0\n",
      "loss z =  10029497.0\n",
      "loss z =  10029383.0\n",
      "loss z =  10029266.0\n",
      "loss z =  10029152.0\n",
      "loss z =  10029037.0\n",
      "Epoch  2089: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10028920.0\n",
      "loss z =  10028897.0\n",
      "loss z =  10028876.0\n",
      "loss z =  10028852.0\n",
      "loss z =  10028828.0\n",
      "loss z =  10028807.0\n",
      "loss z =  10028761.0\n",
      "loss z =  10028738.0\n",
      "loss z =  10028715.0\n",
      "loss z =  10028693.0\n",
      "Epoch  2100: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  27.456603288650513 s\n",
      "loss_f =  7367947.0\n",
      "Epoch  9006: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7359587.5\n",
      "loss_f =  7353144.0\n",
      "loss_f =  7347733.0\n",
      "loss_f =  7344377.0\n",
      "loss_f =  7341683.5\n",
      "loss_f =  7339371.0\n",
      "Epoch  9037: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7337525.5\n",
      "loss_f =  7336642.5\n",
      "loss_f =  7335845.5\n",
      "loss_f =  7335114.5\n",
      "loss_f =  7334438.0\n",
      "loss_f =  7333778.0\n",
      "Epoch  9068: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  7333140.0\n",
      "loss_f =  7332843.0\n",
      "loss_f =  7332537.0\n",
      "loss_f =  7332237.5\n",
      "loss_f =  7331945.0\n",
      "loss_f =  7331648.0\n",
      "loss_f =  7331358.5\n",
      "loss_f =  7331067.0\n",
      "loss_f =  7330784.0\n",
      "loss_f =  7330498.0\n",
      "loss_f =  7330203.0\n",
      "loss_f =  7329929.5\n",
      "loss_f =  7329648.0\n",
      "loss_f =  7329372.0\n",
      "loss_f =  7329097.5\n",
      "loss_f =  7328816.0\n",
      "loss_f =  7328535.0\n",
      "loss_f =  7328265.0\n",
      "loss_f =  7327992.5\n",
      "loss_f =  7327714.5\n",
      "loss_f =  7327448.0\n",
      "loss_f =  7327175.0\n",
      "loss_f =  7326910.0\n",
      "loss_f =  7326636.0\n",
      "loss_f =  7326362.5\n",
      "loss_f =  7326102.0\n",
      "loss_f =  7325839.5\n",
      "thetaf step =  141.07115244865417 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11075.7099609375\n",
      "Epoch  5769: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5785: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5801: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5817: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6409.9873046875\n",
      "Epoch  4992: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5008: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5024: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5040: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30098.533203125\n",
      "Epoch 11812: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11828: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11844: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11860: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.5703125\n",
      "Epoch  6852: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6868: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6884: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6900: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.828125\n",
      "Epoch  7914: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7930: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7946: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7962: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.4921875\n",
      "Epoch  5052: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5068: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5084: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5100: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23248.17578125\n",
      "Epoch  5939: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5955: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5971: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5987: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.126953125\n",
      "Epoch 15258: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 15274: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 15290: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 15306: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.982666015625\n",
      "Epoch  6380: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6396: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6412: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6428: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8142.87744140625\n",
      "Epoch  4886: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4902: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4918: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4934: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19105.560546875\n",
      "Epoch  6422: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6438: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6454: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6470: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10356.1689453125\n",
      "Epoch 11410: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11426: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11442: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11458: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9299.783203125\n",
      "Epoch  6649: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6665: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6681: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6697: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8874.0107421875\n",
      "Epoch  7850: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7866: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7882: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7898: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15624.6845703125\n",
      "Epoch  4967: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4983: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4999: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5015: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.294189453125\n",
      "Epoch  4573: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4589: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4605: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4621: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1774344444274902 s\n",
      "loss_c =  256.7430419921875\n",
      "loss_c =  255.36703491210938\n",
      "loss_c =  254.9705810546875\n",
      "loss_c =  254.60733032226562\n",
      "loss_c =  254.24497985839844\n",
      "loss_c =  253.88040161132812\n",
      "loss_c =  253.5122528076172\n",
      "loss_c =  253.13980102539062\n",
      "loss_c =  252.7624053955078\n",
      "loss_c =  252.37982177734375\n",
      "thetac step =  7.5893309116363525 s\n",
      "step =  51\n",
      "loss z =  10099809.0\n",
      "loss z =  10084990.0\n",
      "loss z =  10073144.0\n",
      "loss z =  10064042.0\n",
      "loss z =  10056974.0\n",
      "loss z =  10051320.0\n",
      "loss z =  10046534.0\n",
      "loss z =  10042308.0\n",
      "loss z =  10038465.0\n",
      "loss z =  10034914.0\n",
      "loss z =  10031582.0\n",
      "Epoch  2112: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10028402.0\n",
      "loss z =  10027783.0\n",
      "loss z =  10027162.0\n",
      "loss z =  10026551.0\n",
      "loss z =  10025941.0\n",
      "loss z =  10025339.0\n",
      "loss z =  10024740.0\n",
      "loss z =  10024142.0\n",
      "loss z =  10022967.0\n",
      "loss z =  10022382.0\n",
      "Epoch  2123: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10021804.0\n",
      "loss z =  10021687.0\n",
      "loss z =  10021573.0\n",
      "loss z =  10021456.0\n",
      "loss z =  10021340.0\n",
      "loss z =  10021226.0\n",
      "loss z =  10021112.0\n",
      "loss z =  10020997.0\n",
      "loss z =  10020882.0\n",
      "loss z =  10020768.0\n",
      "loss z =  10020654.0\n",
      "Epoch  2134: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10020539.0\n",
      "loss z =  10020516.0\n",
      "loss z =  10020493.0\n",
      "loss z =  10020472.0\n",
      "loss z =  10020448.0\n",
      "loss z =  10020426.0\n",
      "loss z =  10020381.0\n",
      "loss z =  10020358.0\n",
      "loss z =  10020336.0\n",
      "loss z =  10020313.0\n",
      "Epoch  2145: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  25.29413628578186 s\n",
      "loss_f =  7362092.0\n",
      "loss_f =  7353370.0\n",
      "loss_f =  7345836.0\n",
      "loss_f =  7338206.5\n",
      "Epoch  9222: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7333078.5\n",
      "loss_f =  7330159.5\n",
      "loss_f =  7327799.0\n",
      "loss_f =  7325906.0\n",
      "loss_f =  7324310.0\n",
      "loss_f =  7322952.5\n",
      "loss_f =  7321741.5\n",
      "loss_f =  7320639.5\n",
      "loss_f =  7319604.0\n",
      "loss_f =  7318631.0\n",
      "loss_f =  7317683.0\n",
      "loss_f =  7316778.0\n",
      "loss_f =  7315895.0\n",
      "loss_f =  7315041.0\n",
      "loss_f =  7314184.0\n",
      "loss_f =  7313344.0\n",
      "loss_f =  7312530.5\n",
      "loss_f =  7311721.0\n",
      "loss_f =  7310918.0\n",
      "loss_f =  7310140.0\n",
      "loss_f =  7309346.0\n",
      "loss_f =  7308574.0\n",
      "loss_f =  7307798.5\n",
      "loss_f =  7307048.5\n",
      "loss_f =  7306292.0\n",
      "loss_f =  7305551.0\n",
      "loss_f =  7304817.0\n",
      "loss_f =  7304090.0\n",
      "loss_f =  7303368.5\n",
      "loss_f =  7302657.0\n",
      "loss_f =  7301945.5\n",
      "loss_f =  7301254.5\n",
      "loss_f =  7300562.0\n",
      "loss_f =  7299879.5\n",
      "loss_f =  7299200.0\n",
      "loss_f =  7298530.0\n",
      "thetaf step =  137.51735877990723 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11075.841796875\n",
      "Epoch  5833: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5849: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5865: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5881: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6409.89013671875\n",
      "Epoch  5056: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5072: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5088: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5104: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30098.880859375\n",
      "Epoch 11876: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11892: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11908: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11924: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.53125\n",
      "Epoch  6916: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6932: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6948: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6964: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.978515625\n",
      "Epoch  7978: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7994: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8010: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8026: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.55078125\n",
      "Epoch  5116: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5132: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5148: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5164: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23248.400390625\n",
      "Epoch  6003: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6019: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6035: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6051: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.2119140625\n",
      "Epoch 15322: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 15338: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 15354: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 15370: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.684326171875\n",
      "Epoch  6444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6460: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6476: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6492: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8141.97802734375\n",
      "Epoch  4950: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4966: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4982: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4998: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19105.876953125\n",
      "Epoch  6486: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6502: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6518: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6534: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10356.2060546875\n",
      "Epoch 11474: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11490: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11506: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11522: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9299.4375\n",
      "Epoch  6713: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6729: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6745: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6761: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8873.876953125\n",
      "Epoch  7914: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7930: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7946: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7962: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15625.4375\n",
      "Epoch  5031: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5047: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5063: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5079: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.34033203125\n",
      "Epoch  4637: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4653: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4669: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4685: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.146592617034912 s\n",
      "loss_c =  257.0801696777344\n",
      "Epoch 62536: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  255.80917358398438\n",
      "loss_c =  255.5565185546875\n",
      "loss_c =  255.41847229003906\n",
      "loss_c =  255.31820678710938\n",
      "loss_c =  255.23074340820312\n",
      "loss_c =  255.14759826660156\n",
      "loss_c =  255.06549072265625\n",
      "loss_c =  254.98355102539062\n",
      "loss_c =  254.90089416503906\n",
      "thetac step =  8.768376350402832 s\n",
      "step =  52\n",
      "loss z =  10120360.0\n",
      "loss z =  10103084.0\n",
      "loss z =  10088744.0\n",
      "loss z =  10076812.0\n",
      "loss z =  10066506.0\n",
      "loss z =  10057200.0\n",
      "loss z =  10048589.0\n",
      "loss z =  10040344.0\n",
      "loss z =  10032698.0\n",
      "loss z =  10025448.0\n",
      "loss z =  10018911.0\n",
      "Epoch  2157: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10013030.0\n",
      "loss z =  10011928.0\n",
      "loss z =  10010854.0\n",
      "loss z =  10009810.0\n",
      "loss z =  10008799.0\n",
      "loss z =  10007814.0\n",
      "loss z =  10006858.0\n",
      "loss z =  10005927.0\n",
      "loss z =  10004147.0\n",
      "loss z =  10003296.0\n",
      "Epoch  2168: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10002464.0\n",
      "loss z =  10002301.0\n",
      "loss z =  10002140.0\n",
      "loss z =  10001978.0\n",
      "loss z =  10001819.0\n",
      "loss z =  10001658.0\n",
      "loss z =  10001498.0\n",
      "loss z =  10001341.0\n",
      "loss z =  10001185.0\n",
      "loss z =  10001029.0\n",
      "loss z =  10000875.0\n",
      "Epoch  2179: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10000722.0\n",
      "loss z =  10000690.0\n",
      "loss z =  10000660.0\n",
      "loss z =  10000628.0\n",
      "loss z =  10000598.0\n",
      "loss z =  10000567.0\n",
      "loss z =  10000506.0\n",
      "loss z =  10000476.0\n",
      "loss z =  10000446.0\n",
      "loss z =  10000415.0\n",
      "Epoch  2190: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.80744695663452 s\n",
      "loss_f =  7345363.0\n",
      "loss_f =  7335758.5\n",
      "loss_f =  7326530.0\n",
      "loss_f =  7318930.0\n",
      "loss_f =  7312609.0\n",
      "loss_f =  7307689.0\n",
      "Epoch  9433: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7304057.0\n",
      "loss_f =  7302431.0\n",
      "loss_f =  7301068.5\n",
      "loss_f =  7299838.5\n",
      "loss_f =  7298687.0\n",
      "loss_f =  7297639.0\n",
      "loss_f =  7296649.5\n",
      "loss_f =  7295680.0\n",
      "loss_f =  7294765.5\n",
      "loss_f =  7293866.0\n",
      "loss_f =  7292983.5\n",
      "loss_f =  7292127.0\n",
      "loss_f =  7291277.5\n",
      "loss_f =  7290442.5\n",
      "loss_f =  7289622.0\n",
      "loss_f =  7288813.0\n",
      "loss_f =  7288014.5\n",
      "loss_f =  7287228.0\n",
      "loss_f =  7286451.0\n",
      "loss_f =  7285672.0\n",
      "loss_f =  7284903.0\n",
      "loss_f =  7284150.0\n",
      "loss_f =  7283397.5\n",
      "loss_f =  7282664.0\n",
      "loss_f =  7281932.0\n",
      "loss_f =  7281210.5\n",
      "loss_f =  7280483.5\n",
      "loss_f =  7279773.5\n",
      "loss_f =  7279062.5\n",
      "loss_f =  7278369.5\n",
      "loss_f =  7277681.0\n",
      "loss_f =  7276981.0\n",
      "loss_f =  7276296.0\n",
      "loss_f =  7275623.0\n",
      "thetaf step =  133.7611575126648 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11075.876953125\n",
      "Epoch  5897: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5913: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5929: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5945: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6409.83349609375\n",
      "Epoch  5120: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5136: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5152: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5168: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30098.896484375\n",
      "Epoch 11940: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11956: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11972: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11988: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.57421875\n",
      "Epoch  6980: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6996: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7012: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7028: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24293.236328125\n",
      "Epoch  8042: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8058: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8074: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8090: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.6171875\n",
      "Epoch  5180: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5196: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5212: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5228: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23248.45703125\n",
      "Epoch  6067: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6083: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6099: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6115: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.208984375\n",
      "Epoch 15386: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 15402: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 15418: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 15434: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.579833984375\n",
      "Epoch  6508: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6524: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6540: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6556: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8141.8994140625\n",
      "Epoch  5014: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5030: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5046: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5062: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19105.873046875\n",
      "Epoch  6550: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6566: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6582: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6598: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10356.353515625\n",
      "Epoch 11538: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11554: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11570: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11586: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9299.296875\n",
      "Epoch  6777: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6793: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6809: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6825: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8873.896484375\n",
      "Epoch  7978: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7994: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8010: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8026: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15624.880859375\n",
      "Epoch  5095: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5111: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5127: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5143: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.3597412109375\n",
      "Epoch  4701: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4717: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4733: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4749: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1582176685333252 s\n",
      "loss_c =  256.795166015625\n",
      "loss_c =  255.23707580566406\n",
      "loss_c =  254.7921142578125\n",
      "loss_c =  254.44189453125\n",
      "loss_c =  254.08840942382812\n",
      "loss_c =  253.72581481933594\n",
      "loss_c =  253.35174560546875\n",
      "loss_c =  252.96450805664062\n",
      "loss_c =  252.5640411376953\n",
      "loss_c =  252.15347290039062\n",
      "thetac step =  8.93932819366455 s\n",
      "step =  53\n",
      "loss z =  10069432.0\n",
      "loss z =  10057140.0\n",
      "loss z =  10046980.0\n",
      "loss z =  10039001.0\n",
      "loss z =  10032782.0\n",
      "loss z =  10027565.0\n",
      "loss z =  10022846.0\n",
      "loss z =  10018452.0\n",
      "loss z =  10014300.0\n",
      "loss z =  10010356.0\n",
      "loss z =  10006584.0\n",
      "Epoch  2202: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10002967.0\n",
      "loss z =  10002266.0\n",
      "loss z =  10001569.0\n",
      "loss z =  10000878.0\n",
      "loss z =  10000193.0\n",
      "loss z =  9999512.0\n",
      "loss z =  9998835.0\n",
      "loss z =  9998158.0\n",
      "loss z =  9996815.0\n",
      "loss z =  9996150.0\n",
      "Epoch  2213: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9995492.0\n",
      "loss z =  9995362.0\n",
      "loss z =  9995230.0\n",
      "loss z =  9995100.0\n",
      "loss z =  9994970.0\n",
      "loss z =  9994838.0\n",
      "loss z =  9994707.0\n",
      "loss z =  9994578.0\n",
      "loss z =  9994447.0\n",
      "loss z =  9994317.0\n",
      "loss z =  9994187.0\n",
      "Epoch  2224: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9994057.0\n",
      "loss z =  9994030.0\n",
      "loss z =  9994006.0\n",
      "loss z =  9993980.0\n",
      "loss z =  9993954.0\n",
      "loss z =  9993927.0\n",
      "loss z =  9993875.0\n",
      "loss z =  9993849.0\n",
      "loss z =  9993824.0\n",
      "loss z =  9993798.0\n",
      "Epoch  2235: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  25.301379203796387 s\n",
      "loss_f =  7340937.0\n",
      "loss_f =  7329574.0\n",
      "loss_f =  7322307.0\n",
      "loss_f =  7314704.5\n",
      "loss_f =  7307489.0\n",
      "loss_f =  7302109.5\n",
      "Epoch  9632: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7298664.0\n",
      "loss_f =  7296995.5\n",
      "loss_f =  7295510.0\n",
      "loss_f =  7294181.5\n",
      "loss_f =  7292991.0\n",
      "loss_f =  7291902.0\n",
      "Epoch  9663: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7290885.5\n",
      "loss_f =  7290392.5\n",
      "loss_f =  7289918.0\n",
      "loss_f =  7289446.0\n",
      "loss_f =  7288990.0\n",
      "loss_f =  7288536.0\n",
      "loss_f =  7288077.0\n",
      "Epoch  9694: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  7287814.0\n",
      "loss_f =  7287596.5\n",
      "loss_f =  7287382.0\n",
      "loss_f =  7287153.5\n",
      "loss_f =  7286939.0\n",
      "loss_f =  7286725.0\n",
      "Epoch  9725: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  7286566.5\n",
      "loss_f =  7286463.0\n",
      "loss_f =  7286358.0\n",
      "loss_f =  7286248.0\n",
      "loss_f =  7286138.0\n",
      "loss_f =  7286030.0\n",
      "Epoch  9756: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  7285947.0\n",
      "loss_f =  7285881.0\n",
      "loss_f =  7285840.0\n",
      "loss_f =  7285778.0\n",
      "loss_f =  7285725.0\n",
      "loss_f =  7285677.0\n",
      "Epoch  9787: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  7285629.0\n",
      "loss_f =  7285600.0\n",
      "loss_f =  7285569.5\n",
      "thetaf step =  137.94935250282288 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11076.00390625\n",
      "Epoch  5961: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5977: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5993: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6009: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6409.775390625\n",
      "Epoch  5184: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5200: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5216: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5232: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30099.052734375\n",
      "Epoch 12004: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12020: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12036: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12052: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.66796875\n",
      "Epoch  7044: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7060: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7076: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7092: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.44140625\n",
      "Epoch  8106: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8122: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8138: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8154: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.734375\n",
      "Epoch  5244: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5260: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5276: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5292: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23248.65234375\n",
      "Epoch  6131: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6147: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6163: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6179: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.212890625\n",
      "Epoch 15450: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 15466: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 15482: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 15498: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.3359375\n",
      "Epoch  6572: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6588: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6604: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6620: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8141.70361328125\n",
      "Epoch  5079: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5095: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5111: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5127: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19106.076171875\n",
      "Epoch  6614: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6630: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6646: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6662: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10356.587890625\n",
      "Epoch 11602: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11618: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11634: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11650: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9298.9853515625\n",
      "Epoch  6841: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6857: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6873: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6889: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8873.970703125\n",
      "Epoch  8042: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8058: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8074: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8090: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15624.998046875\n",
      "Epoch  5159: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5175: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5191: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5207: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.4290771484375\n",
      "Epoch  4765: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4781: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4797: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4813: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.174445390701294 s\n",
      "loss_c =  256.5873107910156\n",
      "Epoch 63537: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  255.58282470703125\n",
      "loss_c =  255.45480346679688\n",
      "loss_c =  255.3555908203125\n",
      "loss_c =  255.26821899414062\n",
      "loss_c =  255.1852264404297\n",
      "loss_c =  255.103515625\n",
      "loss_c =  255.02162170410156\n",
      "loss_c =  254.93902587890625\n",
      "loss_c =  254.85537719726562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thetac step =  9.174630641937256 s\n",
      "step =  54\n",
      "loss z =  10060784.0\n",
      "loss z =  10046015.0\n",
      "loss z =  10033598.0\n",
      "loss z =  10023663.0\n",
      "loss z =  10016060.0\n",
      "loss z =  10010269.0\n",
      "loss z =  10005474.0\n",
      "loss z =  10001180.0\n",
      "loss z =  9997224.0\n",
      "loss z =  9993517.0\n",
      "Epoch  2246: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9990008.0\n",
      "loss z =  9989336.0\n",
      "loss z =  9988668.0\n",
      "loss z =  9988006.0\n",
      "loss z =  9987348.0\n",
      "loss z =  9986696.0\n",
      "loss z =  9986052.0\n",
      "loss z =  9985412.0\n",
      "loss z =  9984776.0\n",
      "loss z =  9983526.0\n",
      "Epoch  2257: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9982905.0\n",
      "loss z =  9982781.0\n",
      "loss z =  9982658.0\n",
      "loss z =  9982536.0\n",
      "loss z =  9982414.0\n",
      "loss z =  9982293.0\n",
      "loss z =  9982170.0\n",
      "loss z =  9982048.0\n",
      "loss z =  9981926.0\n",
      "loss z =  9981804.0\n",
      "loss z =  9981680.0\n",
      "Epoch  2268: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9981558.0\n",
      "loss z =  9981535.0\n",
      "loss z =  9981509.0\n",
      "loss z =  9981486.0\n",
      "loss z =  9981461.0\n",
      "loss z =  9981437.0\n",
      "loss z =  9981413.0\n",
      "loss z =  9981366.0\n",
      "loss z =  9981341.0\n",
      "loss z =  9981317.0\n",
      "Epoch  2279: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.751501321792603 s\n",
      "loss_f =  7330679.0\n",
      "loss_f =  7320349.0\n",
      "loss_f =  7311296.0\n",
      "Epoch  9818: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7304988.0\n",
      "loss_f =  7301149.0\n",
      "loss_f =  7297227.0\n",
      "loss_f =  7294130.5\n",
      "loss_f =  7291844.0\n",
      "loss_f =  7290006.0\n",
      "loss_f =  7288407.0\n",
      "Epoch  9849: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7287590.0\n",
      "loss_f =  7286947.0\n",
      "loss_f =  7286342.0\n",
      "loss_f =  7285750.0\n",
      "loss_f =  7285188.5\n",
      "loss_f =  7284638.5\n",
      "Epoch  9880: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  7284272.5\n",
      "loss_f =  7284000.0\n",
      "loss_f =  7283743.0\n",
      "loss_f =  7283482.0\n",
      "loss_f =  7283229.0\n",
      "loss_f =  7282970.0\n",
      "Epoch  9911: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  7282766.5\n",
      "loss_f =  7282642.0\n",
      "loss_f =  7282524.5\n",
      "loss_f =  7282397.0\n",
      "loss_f =  7282275.0\n",
      "loss_f =  7282148.0\n",
      "Epoch  9942: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  7282034.0\n",
      "loss_f =  7281973.0\n",
      "loss_f =  7281908.5\n",
      "loss_f =  7281850.5\n",
      "loss_f =  7281781.0\n",
      "loss_f =  7281720.0\n",
      "Epoch  9973: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  7281659.5\n",
      "loss_f =  7281626.5\n",
      "loss_f =  7281587.5\n",
      "loss_f =  7281566.5\n",
      "loss_f =  7281529.0\n",
      "loss_f =  7281501.0\n",
      "thetaf step =  138.11549639701843 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11076.1328125\n",
      "Epoch  6025: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6041: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6057: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6073: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6409.64453125\n",
      "Epoch  5248: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5264: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5280: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5296: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30099.03515625\n",
      "Epoch 12068: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12084: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12100: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12116: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.6640625\n",
      "Epoch  7108: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7124: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7140: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7156: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.146484375\n",
      "Epoch  8170: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8186: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8202: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8218: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.82421875\n",
      "Epoch  5308: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5324: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5340: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5356: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23248.796875\n",
      "Epoch  6195: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6211: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6227: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6243: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.2294921875\n",
      "Epoch 15514: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 15530: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 15546: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 15562: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2422.22509765625\n",
      "Epoch  6636: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6652: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6668: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6684: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8141.431640625\n",
      "Epoch  5143: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5159: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5175: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5191: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19106.2109375\n",
      "Epoch  6678: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6694: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6710: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6726: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10356.6240234375\n",
      "Epoch 11666: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11682: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11698: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11714: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9298.865234375\n",
      "Epoch  6906: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6922: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6938: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6954: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8873.892578125\n",
      "Epoch  8106: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8122: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8138: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8154: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15625.00390625\n",
      "Epoch  5223: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5239: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5255: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5271: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.444580078125\n",
      "Epoch  4829: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4845: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4861: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4877: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1581573486328125 s\n",
      "loss_c =  256.1319885253906\n",
      "loss_c =  255.04104614257812\n",
      "loss_c =  254.6944580078125\n",
      "loss_c =  254.359619140625\n",
      "loss_c =  254.02230834960938\n",
      "loss_c =  253.6810302734375\n",
      "loss_c =  253.33526611328125\n",
      "loss_c =  252.98486328125\n",
      "loss_c =  252.62939453125\n",
      "loss_c =  252.26889038085938\n",
      "thetac step =  8.965118885040283 s\n",
      "step =  55\n",
      "loss z =  10084583.0\n",
      "loss z =  10063926.0\n",
      "loss z =  10046148.0\n",
      "loss z =  10031089.0\n",
      "loss z =  10018663.0\n",
      "loss z =  10008852.0\n",
      "loss z =  10001262.0\n",
      "loss z =  9995120.0\n",
      "loss z =  9989950.0\n",
      "loss z =  9985358.0\n",
      "Epoch  2290: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9981203.0\n",
      "loss z =  9980425.0\n",
      "loss z =  9979662.0\n",
      "loss z =  9978910.0\n",
      "loss z =  9978168.0\n",
      "loss z =  9977436.0\n",
      "loss z =  9976715.0\n",
      "loss z =  9976004.0\n",
      "loss z =  9975300.0\n",
      "loss z =  9973926.0\n",
      "Epoch  2301: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9973250.0\n",
      "loss z =  9973116.0\n",
      "loss z =  9972984.0\n",
      "loss z =  9972849.0\n",
      "loss z =  9972717.0\n",
      "loss z =  9972585.0\n",
      "loss z =  9972454.0\n",
      "loss z =  9972322.0\n",
      "loss z =  9972190.0\n",
      "loss z =  9972058.0\n",
      "loss z =  9971927.0\n",
      "Epoch  2312: reducing learning rate of group 0 to 8.0000e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss z =  9971797.0\n",
      "loss z =  9971772.0\n",
      "loss z =  9971746.0\n",
      "loss z =  9971721.0\n",
      "loss z =  9971695.0\n",
      "loss z =  9971669.0\n",
      "loss z =  9971642.0\n",
      "loss z =  9971590.0\n",
      "loss z =  9971564.0\n",
      "loss z =  9971538.0\n",
      "Epoch  2323: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.964227437973022 s\n",
      "loss_f =  7323039.5\n",
      "Epoch 10004: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7313339.0\n",
      "loss_f =  7309375.0\n",
      "loss_f =  7305088.5\n",
      "loss_f =  7301935.0\n",
      "loss_f =  7298907.0\n",
      "loss_f =  7296294.0\n",
      "Epoch 10035: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7294786.0\n",
      "loss_f =  7293802.0\n",
      "loss_f =  7292901.5\n",
      "loss_f =  7292096.5\n",
      "loss_f =  7291377.0\n",
      "loss_f =  7290684.0\n",
      "Epoch 10066: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  7290165.0\n",
      "loss_f =  7289842.0\n",
      "loss_f =  7289526.5\n",
      "loss_f =  7289217.0\n",
      "loss_f =  7288905.0\n",
      "loss_f =  7288593.0\n",
      "Epoch 10097: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  7288321.0\n",
      "loss_f =  7288183.5\n",
      "loss_f =  7288027.0\n",
      "loss_f =  7287878.0\n",
      "loss_f =  7287732.0\n",
      "loss_f =  7287581.0\n",
      "Epoch 10128: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  7287440.0\n",
      "loss_f =  7287360.0\n",
      "loss_f =  7287290.0\n",
      "loss_f =  7287215.5\n",
      "loss_f =  7287141.5\n",
      "loss_f =  7287070.5\n",
      "loss_f =  7286996.0\n",
      "Epoch 10159: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  7286947.0\n",
      "loss_f =  7286911.0\n",
      "loss_f =  7286879.0\n",
      "loss_f =  7286835.0\n",
      "loss_f =  7286799.0\n",
      "loss_f =  7286765.0\n",
      "Epoch 10190: reducing learning rate of group 0 to 3.9063e-06.\n",
      "thetaf step =  130.08856892585754 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11076.28125\n",
      "Epoch  6089: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6105: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6121: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6137: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6409.513671875\n",
      "Epoch  5312: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5328: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5344: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5360: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30099.177734375\n",
      "Epoch 12132: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12148: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12164: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12180: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.6875\n",
      "Epoch  7172: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7188: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7204: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7220: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24291.876953125\n",
      "Epoch  8234: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8250: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8266: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8282: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.92578125\n",
      "Epoch  5372: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5388: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5404: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5420: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23249.015625\n",
      "Epoch  6259: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6275: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6291: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6307: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.2734375\n",
      "Epoch 15578: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 15594: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 15610: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 15626: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2421.98583984375\n",
      "Epoch  6700: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6716: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6732: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6748: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8141.2080078125\n",
      "Epoch  5207: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5223: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5239: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5255: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19106.482421875\n",
      "Epoch  6742: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6758: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6774: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6790: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10356.765625\n",
      "Epoch 11730: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11746: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11762: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11778: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9298.55078125\n",
      "Epoch  6970: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6986: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7002: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7018: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8873.8828125\n",
      "Epoch  8170: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8186: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8202: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8218: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15626.734375\n",
      "Epoch  5287: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5303: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5319: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5335: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.491455078125\n",
      "Epoch  4893: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4909: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4925: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  4941: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.172611951828003 s\n",
      "loss_c =  257.81207275390625\n",
      "Epoch 64538: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  256.2416687011719\n",
      "loss_c =  255.8163299560547\n",
      "loss_c =  255.58853149414062\n",
      "loss_c =  255.4450225830078\n",
      "loss_c =  255.337646484375\n",
      "loss_c =  255.24591064453125\n",
      "loss_c =  255.1611328125\n",
      "loss_c =  255.07904052734375\n",
      "loss_c =  254.99786376953125\n",
      "thetac step =  9.134360551834106 s\n",
      "step =  56\n",
      "loss z =  10066180.0\n",
      "loss z =  10047170.0\n",
      "loss z =  10030010.0\n",
      "loss z =  10015077.0\n",
      "loss z =  10002239.0\n",
      "loss z =  9991282.0\n",
      "loss z =  9982172.0\n",
      "loss z =  9974971.0\n",
      "loss z =  9969640.0\n",
      "loss z =  9965716.0\n",
      "Epoch  2334: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9962492.0\n",
      "loss z =  9961906.0\n",
      "loss z =  9961333.0\n",
      "loss z =  9960764.0\n",
      "loss z =  9960199.0\n",
      "loss z =  9959642.0\n",
      "loss z =  9959095.0\n",
      "loss z =  9958548.0\n",
      "loss z =  9958009.0\n",
      "loss z =  9956947.0\n",
      "Epoch  2345: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9956420.0\n",
      "loss z =  9956317.0\n",
      "loss z =  9956213.0\n",
      "loss z =  9956107.0\n",
      "loss z =  9956004.0\n",
      "loss z =  9955899.0\n",
      "loss z =  9955797.0\n",
      "loss z =  9955694.0\n",
      "loss z =  9955590.0\n",
      "loss z =  9955488.0\n",
      "loss z =  9955384.0\n",
      "Epoch  2356: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9955281.0\n",
      "loss z =  9955261.0\n",
      "loss z =  9955240.0\n",
      "loss z =  9955219.0\n",
      "loss z =  9955199.0\n",
      "loss z =  9955178.0\n",
      "loss z =  9955158.0\n",
      "loss z =  9955117.0\n",
      "loss z =  9955096.0\n",
      "loss z =  9955074.0\n",
      "Epoch  2367: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.838117361068726 s\n",
      "loss_f =  7309572.0\n",
      "loss_f =  7304202.5\n",
      "loss_f =  7297113.0\n",
      "loss_f =  7289090.0\n",
      "loss_f =  7283787.5\n",
      "loss_f =  7278909.0\n",
      "loss_f =  7275080.0\n",
      "Epoch 10221: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7273173.0\n",
      "loss_f =  7271656.5\n",
      "loss_f =  7270390.0\n",
      "loss_f =  7269269.0\n",
      "loss_f =  7268217.0\n",
      "loss_f =  7267172.0\n",
      "loss_f =  7266167.0\n",
      "loss_f =  7265197.5\n",
      "loss_f =  7264267.0\n",
      "loss_f =  7263379.0\n",
      "loss_f =  7262525.5\n",
      "loss_f =  7261704.0\n",
      "loss_f =  7260897.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_f =  7260117.0\n",
      "loss_f =  7259343.0\n",
      "loss_f =  7258584.0\n",
      "loss_f =  7257829.5\n",
      "loss_f =  7257103.5\n",
      "loss_f =  7256371.0\n",
      "loss_f =  7255646.5\n",
      "loss_f =  7254930.5\n",
      "loss_f =  7254221.0\n",
      "loss_f =  7253524.5\n",
      "loss_f =  7252820.5\n",
      "loss_f =  7252141.5\n",
      "loss_f =  7251451.0\n",
      "loss_f =  7250774.0\n",
      "loss_f =  7250100.0\n",
      "loss_f =  7249442.0\n",
      "loss_f =  7248778.5\n",
      "loss_f =  7248122.0\n",
      "loss_f =  7247464.0\n",
      "loss_f =  7246825.0\n",
      "thetaf step =  139.18917107582092 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11076.353515625\n",
      "Epoch  6153: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6169: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6185: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6201: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6409.39453125\n",
      "Epoch  5376: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5392: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5408: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5424: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30099.1875\n",
      "Epoch 12196: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12228: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12244: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.7265625\n",
      "Epoch  7236: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7252: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7268: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7284: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24291.787109375\n",
      "Epoch  8298: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8314: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8330: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8346: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37791.98828125\n",
      "Epoch  5436: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5452: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5468: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5484: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23249.1015625\n",
      "Epoch  6323: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6339: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6355: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6371: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.314453125\n",
      "Epoch 15642: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 15658: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 15674: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 15690: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2421.90673828125\n",
      "Epoch  6764: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6780: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6796: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6812: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8141.630859375\n",
      "Epoch  5271: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5287: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5303: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5319: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19106.478515625\n",
      "Epoch  6806: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6822: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6838: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6854: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10356.826171875\n",
      "Epoch 11794: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11810: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11826: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11842: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9298.4365234375\n",
      "Epoch  7034: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7050: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7066: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7082: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8873.84765625\n",
      "Epoch  8234: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8250: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8266: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8282: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15625.9697265625\n",
      "Epoch  5351: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5367: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5383: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5399: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.535400390625\n",
      "Epoch  4957: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4973: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  4989: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5005: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1790623664855957 s\n",
      "loss_c =  257.44976806640625\n",
      "loss_c =  255.67349243164062\n",
      "loss_c =  255.20962524414062\n",
      "loss_c =  254.81289672851562\n",
      "loss_c =  254.4356231689453\n",
      "loss_c =  254.06887817382812\n",
      "loss_c =  253.7075958251953\n",
      "loss_c =  253.34841918945312\n",
      "loss_c =  252.98928833007812\n",
      "loss_c =  252.62857055664062\n",
      "thetac step =  9.947288036346436 s\n",
      "step =  57\n",
      "loss z =  10031377.0\n",
      "loss z =  10011866.0\n",
      "loss z =  9995015.0\n",
      "loss z =  9980878.0\n",
      "loss z =  9969444.0\n",
      "loss z =  9960633.0\n",
      "loss z =  9953983.0\n",
      "loss z =  9948912.0\n",
      "loss z =  9944836.0\n",
      "loss z =  9941254.0\n",
      "loss z =  9937974.0\n",
      "Epoch  2379: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9934914.0\n",
      "loss z =  9934326.0\n",
      "loss z =  9933749.0\n",
      "loss z =  9933174.0\n",
      "loss z =  9932607.0\n",
      "loss z =  9932044.0\n",
      "loss z =  9931482.0\n",
      "loss z =  9930928.0\n",
      "loss z =  9929833.0\n",
      "loss z =  9929290.0\n",
      "Epoch  2390: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9928753.0\n",
      "loss z =  9928645.0\n",
      "loss z =  9928539.0\n",
      "loss z =  9928432.0\n",
      "loss z =  9928326.0\n",
      "loss z =  9928220.0\n",
      "loss z =  9928112.0\n",
      "loss z =  9928006.0\n",
      "loss z =  9927900.0\n",
      "loss z =  9927794.0\n",
      "loss z =  9927689.0\n",
      "Epoch  2401: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9927585.0\n",
      "loss z =  9927563.0\n",
      "loss z =  9927542.0\n",
      "loss z =  9927522.0\n",
      "loss z =  9927500.0\n",
      "loss z =  9927479.0\n",
      "loss z =  9927436.0\n",
      "loss z =  9927416.0\n",
      "loss z =  9927393.0\n",
      "loss z =  9927372.0\n",
      "Epoch  2412: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  25.640222549438477 s\n",
      "loss_f =  7284890.0\n",
      "loss_f =  7279419.0\n",
      "loss_f =  7270938.0\n",
      "loss_f =  7264217.0\n",
      "loss_f =  7258696.0\n",
      "loss_f =  7253997.0\n",
      "Epoch 10418: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7251410.0\n",
      "loss_f =  7249863.0\n",
      "loss_f =  7248496.5\n",
      "loss_f =  7247326.0\n",
      "loss_f =  7246260.0\n",
      "loss_f =  7245275.0\n",
      "loss_f =  7244356.0\n",
      "loss_f =  7243461.5\n",
      "loss_f =  7242612.5\n",
      "loss_f =  7241780.0\n",
      "loss_f =  7240974.0\n",
      "loss_f =  7240182.5\n",
      "loss_f =  7239400.0\n",
      "loss_f =  7238633.0\n",
      "loss_f =  7237881.0\n",
      "loss_f =  7237137.0\n",
      "loss_f =  7236394.0\n",
      "loss_f =  7235672.5\n",
      "loss_f =  7234945.5\n",
      "loss_f =  7234240.0\n",
      "loss_f =  7233537.5\n",
      "loss_f =  7232835.0\n",
      "loss_f =  7232151.5\n",
      "loss_f =  7231473.5\n",
      "loss_f =  7230790.5\n",
      "loss_f =  7230127.0\n",
      "loss_f =  7229461.5\n",
      "loss_f =  7228800.0\n",
      "loss_f =  7228156.5\n",
      "loss_f =  7227510.0\n",
      "loss_f =  7226864.0\n",
      "loss_f =  7226233.5\n",
      "loss_f =  7225600.5\n",
      "loss_f =  7224978.0\n",
      "thetaf step =  149.65525221824646 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11076.40625\n",
      "Epoch  6217: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6233: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6249: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6265: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6409.3291015625\n",
      "Epoch  5440: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5456: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5472: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5488: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30099.38671875\n",
      "Epoch 12260: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12276: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12292: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12308: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.7890625\n",
      "Epoch  7300: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7316: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7332: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7348: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24291.681640625\n",
      "Epoch  8362: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8378: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8394: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8410: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37792.1171875\n",
      "Epoch  5500: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5516: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5532: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5548: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23249.23046875\n",
      "Epoch  6387: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6403: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6419: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6435: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.3603515625\n",
      "Epoch 15706: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 15722: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 15738: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 15754: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2421.69970703125\n",
      "Epoch  6828: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6844: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6860: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6876: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8140.78076171875\n",
      "Epoch  5336: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5352: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5368: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5384: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19106.6171875\n",
      "Epoch  6870: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6886: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6902: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6918: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10356.9375\n",
      "Epoch 11858: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11874: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11890: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11906: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9298.1220703125\n",
      "Epoch  7098: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7114: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7130: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7146: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8873.791015625\n",
      "Epoch  8298: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8314: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8330: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8346: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15625.369140625\n",
      "Epoch  5415: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5431: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5447: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5463: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.6435546875\n",
      "Epoch  5021: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5037: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5053: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5069: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1689364910125732 s\n",
      "loss_c =  256.2208251953125\n",
      "Epoch 65539: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  255.5524139404297\n",
      "loss_c =  255.43284606933594\n",
      "loss_c =  255.34664916992188\n",
      "loss_c =  255.2714385986328\n",
      "loss_c =  255.19944763183594\n",
      "loss_c =  255.12762451171875\n",
      "loss_c =  255.055419921875\n",
      "loss_c =  254.98193359375\n",
      "loss_c =  254.90719604492188\n",
      "thetac step =  10.429650068283081 s\n",
      "step =  58\n",
      "loss z =  10050413.0\n",
      "loss z =  10027964.0\n",
      "loss z =  10008325.0\n",
      "loss z =  9991510.0\n",
      "loss z =  9977053.0\n",
      "loss z =  9964812.0\n",
      "loss z =  9954646.0\n",
      "loss z =  9946550.0\n",
      "loss z =  9940412.0\n",
      "loss z =  9935684.0\n",
      "loss z =  9931727.0\n",
      "Epoch  2424: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9928171.0\n",
      "loss z =  9927498.0\n",
      "loss z =  9926832.0\n",
      "loss z =  9926172.0\n",
      "loss z =  9925521.0\n",
      "loss z =  9924876.0\n",
      "loss z =  9924237.0\n",
      "loss z =  9923602.0\n",
      "loss z =  9922351.0\n",
      "loss z =  9921733.0\n",
      "Epoch  2435: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9921123.0\n",
      "loss z =  9921003.0\n",
      "loss z =  9920882.0\n",
      "loss z =  9920762.0\n",
      "loss z =  9920640.0\n",
      "loss z =  9920520.0\n",
      "loss z =  9920400.0\n",
      "loss z =  9920280.0\n",
      "loss z =  9920158.0\n",
      "loss z =  9920040.0\n",
      "loss z =  9919918.0\n",
      "Epoch  2446: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9919798.0\n",
      "loss z =  9919774.0\n",
      "loss z =  9919750.0\n",
      "loss z =  9919726.0\n",
      "loss z =  9919702.0\n",
      "loss z =  9919680.0\n",
      "loss z =  9919633.0\n",
      "loss z =  9919609.0\n",
      "loss z =  9919586.0\n",
      "loss z =  9919562.0\n",
      "Epoch  2457: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  25.719205379486084 s\n",
      "loss_f =  7280095.5\n",
      "loss_f =  7271819.5\n",
      "loss_f =  7265495.0\n",
      "loss_f =  7258784.0\n",
      "loss_f =  7252431.0\n",
      "loss_f =  7248140.5\n",
      "Epoch 10616: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7245849.0\n",
      "loss_f =  7244062.0\n",
      "loss_f =  7242599.0\n",
      "loss_f =  7241315.0\n",
      "loss_f =  7240169.0\n",
      "loss_f =  7239105.0\n",
      "Epoch 10647: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7238393.5\n",
      "loss_f =  7237901.0\n",
      "loss_f =  7237428.0\n",
      "loss_f =  7236971.5\n",
      "loss_f =  7236528.0\n",
      "loss_f =  7236087.0\n",
      "Epoch 10678: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  7235741.5\n",
      "loss_f =  7235522.5\n",
      "loss_f =  7235305.0\n",
      "loss_f =  7235098.0\n",
      "loss_f =  7234892.0\n",
      "loss_f =  7234678.5\n",
      "Epoch 10709: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  7234486.0\n",
      "loss_f =  7234364.0\n",
      "loss_f =  7234244.5\n",
      "loss_f =  7234130.5\n",
      "loss_f =  7234020.0\n",
      "loss_f =  7233902.0\n",
      "Epoch 10740: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  7233788.5\n",
      "loss_f =  7233726.0\n",
      "loss_f =  7233667.0\n",
      "loss_f =  7233617.0\n",
      "loss_f =  7233561.0\n",
      "loss_f =  7233504.0\n",
      "loss_f =  7233445.0\n",
      "Epoch 10771: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  7233419.5\n",
      "loss_f =  7233396.0\n",
      "loss_f =  7233360.0\n",
      "thetaf step =  134.91837549209595 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11076.42578125\n",
      "Epoch  6281: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6297: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6313: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6329: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6409.2802734375\n",
      "Epoch  5504: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5520: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5536: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5552: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30099.373046875\n",
      "Epoch 12324: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12340: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12356: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12372: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.81640625\n",
      "Epoch  7364: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7380: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7396: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7412: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24291.712890625\n",
      "Epoch  8426: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8442: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8458: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8474: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37792.17578125\n",
      "Epoch  5564: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5580: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5596: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5612: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23249.28125\n",
      "Epoch  6451: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6467: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6483: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6499: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.3603515625\n",
      "Epoch 15770: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 15786: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 15802: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 15818: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2421.623779296875\n",
      "Epoch  6892: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6908: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6924: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6940: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8140.83203125\n",
      "Epoch  5400: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5416: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5432: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5448: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19106.6328125\n",
      "Epoch  6934: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6950: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6966: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6982: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10357.0439453125\n",
      "Epoch 11922: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 11938: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 11954: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 11970: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9298.017578125\n",
      "Epoch  7162: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7178: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7194: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7210: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8873.8046875\n",
      "Epoch  8362: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8378: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8394: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8410: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15625.3359375\n",
      "Epoch  5479: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5495: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5511: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5527: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.6611328125\n",
      "Epoch  5085: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5101: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5117: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5133: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1798343658447266 s\n",
      "loss_c =  256.3398132324219\n",
      "loss_c =  255.24270629882812\n",
      "loss_c =  254.8924102783203\n",
      "loss_c =  254.55807495117188\n",
      "loss_c =  254.21945190429688\n",
      "loss_c =  253.872314453125\n",
      "loss_c =  253.51223754882812\n",
      "loss_c =  253.1318359375\n",
      "loss_c =  252.7158203125\n",
      "loss_c =  252.22860717773438\n",
      "thetac step =  10.068234920501709 s\n",
      "step =  59\n",
      "loss z =  10020590.0\n",
      "loss z =  10002770.0\n",
      "loss z =  9987015.0\n",
      "loss z =  9972929.0\n",
      "loss z =  9960935.0\n",
      "loss z =  9951036.0\n",
      "loss z =  9943153.0\n",
      "loss z =  9936927.0\n",
      "loss z =  9932226.0\n",
      "loss z =  9928346.0\n",
      "Epoch  2468: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9924890.0\n",
      "loss z =  9924239.0\n",
      "loss z =  9923597.0\n",
      "loss z =  9922963.0\n",
      "loss z =  9922336.0\n",
      "loss z =  9921719.0\n",
      "loss z =  9921108.0\n",
      "loss z =  9920500.0\n",
      "loss z =  9919898.0\n",
      "loss z =  9918714.0\n",
      "Epoch  2479: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9918133.0\n",
      "loss z =  9918016.0\n",
      "loss z =  9917902.0\n",
      "loss z =  9917785.0\n",
      "loss z =  9917670.0\n",
      "loss z =  9917557.0\n",
      "loss z =  9917443.0\n",
      "loss z =  9917328.0\n",
      "loss z =  9917213.0\n",
      "loss z =  9917099.0\n",
      "loss z =  9916984.0\n",
      "Epoch  2490: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9916871.0\n",
      "loss z =  9916849.0\n",
      "loss z =  9916826.0\n",
      "loss z =  9916804.0\n",
      "loss z =  9916780.0\n",
      "loss z =  9916757.0\n",
      "loss z =  9916735.0\n",
      "loss z =  9916690.0\n",
      "loss z =  9916667.0\n",
      "loss z =  9916644.0\n",
      "Epoch  2501: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  23.309436082839966 s\n",
      "loss_f =  7279469.5\n",
      "loss_f =  7269644.0\n",
      "loss_f =  7262902.0\n",
      "Epoch 10802: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7258182.0\n",
      "loss_f =  7253866.0\n",
      "loss_f =  7250116.0\n",
      "loss_f =  7247369.0\n",
      "loss_f =  7245111.0\n",
      "loss_f =  7243264.5\n",
      "Epoch 10833: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7242030.5\n",
      "loss_f =  7241316.5\n",
      "loss_f =  7240646.0\n",
      "loss_f =  7240013.0\n",
      "loss_f =  7239415.0\n",
      "loss_f =  7238844.0\n",
      "Epoch 10864: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  7238348.0\n",
      "loss_f =  7238070.0\n",
      "loss_f =  7237793.0\n",
      "loss_f =  7237529.0\n",
      "loss_f =  7237269.0\n",
      "loss_f =  7237005.0\n",
      "Epoch 10895: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  7236751.5\n",
      "loss_f =  7236611.5\n",
      "loss_f =  7236491.0\n",
      "loss_f =  7236357.5\n",
      "loss_f =  7236236.0\n",
      "loss_f =  7236103.5\n",
      "loss_f =  7235976.5\n",
      "Epoch 10926: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  7235901.0\n",
      "loss_f =  7235838.0\n",
      "loss_f =  7235771.0\n",
      "loss_f =  7235709.5\n",
      "loss_f =  7235649.0\n",
      "loss_f =  7235590.5\n",
      "Epoch 10957: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  7235536.5\n",
      "loss_f =  7235508.0\n",
      "loss_f =  7235476.0\n",
      "loss_f =  7235444.0\n",
      "loss_f =  7235417.0\n",
      "loss_f =  7235383.0\n",
      "Epoch 10988: reducing learning rate of group 0 to 3.9063e-06.\n",
      "thetaf step =  132.25212717056274 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11076.740234375\n",
      "Epoch  6345: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6361: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6377: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6393: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6409.16455078125\n",
      "Epoch  5568: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5584: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5600: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5616: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30099.060546875\n",
      "Epoch 12388: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12404: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12420: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12436: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.90234375\n",
      "Epoch  7428: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7444: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7460: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7476: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24291.484375\n",
      "Epoch  8490: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8506: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8522: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8538: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37792.3046875\n",
      "Epoch  5628: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5644: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5660: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5676: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23249.541015625\n",
      "Epoch  6515: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6531: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6547: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6563: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.5478515625\n",
      "Epoch 15834: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 15850: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 15866: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 15882: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2421.42626953125\n",
      "Epoch  6956: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6972: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6988: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7004: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8140.4765625\n",
      "Epoch  5464: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5480: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5496: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5512: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19107.015625\n",
      "Epoch  6998: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7014: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7030: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7046: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10357.0859375\n",
      "Epoch 11986: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12002: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12018: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12034: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9297.720703125\n",
      "Epoch  7227: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7243: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7259: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7275: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8873.546875\n",
      "Epoch  8426: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8442: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8458: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8474: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15625.65625\n",
      "Epoch  5543: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5559: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5575: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5591: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.6866455078125\n",
      "Epoch  5149: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5165: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5181: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5197: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.161628007888794 s\n",
      "loss_c =  256.3206787109375\n",
      "Epoch 66540: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  255.2286376953125\n",
      "loss_c =  255.04847717285156\n",
      "loss_c =  254.9069061279297\n",
      "loss_c =  254.77496337890625\n",
      "loss_c =  254.6436767578125\n",
      "loss_c =  254.51025390625\n",
      "loss_c =  254.37384033203125\n",
      "loss_c =  254.23434448242188\n",
      "loss_c =  254.092041015625\n",
      "thetac step =  9.202294111251831 s\n",
      "Saving model...\n",
      "...saving done.\n",
      "step =  60\n",
      "loss z =  10016391.0\n",
      "loss z =  9994766.0\n",
      "loss z =  9976582.0\n",
      "loss z =  9961823.0\n",
      "loss z =  9949950.0\n",
      "loss z =  9940052.0\n",
      "loss z =  9931868.0\n",
      "loss z =  9925059.0\n",
      "loss z =  9919350.0\n",
      "loss z =  9914482.0\n",
      "Epoch  2512: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9910266.0\n",
      "loss z =  9909506.0\n",
      "loss z =  9908760.0\n",
      "loss z =  9908033.0\n",
      "loss z =  9907314.0\n",
      "loss z =  9906612.0\n",
      "loss z =  9905924.0\n",
      "loss z =  9905247.0\n",
      "loss z =  9904582.0\n",
      "loss z =  9903284.0\n",
      "Epoch  2523: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9902651.0\n",
      "loss z =  9902526.0\n",
      "loss z =  9902402.0\n",
      "loss z =  9902278.0\n",
      "loss z =  9902155.0\n",
      "loss z =  9902031.0\n",
      "loss z =  9901908.0\n",
      "loss z =  9901788.0\n",
      "loss z =  9901666.0\n",
      "loss z =  9901543.0\n",
      "loss z =  9901423.0\n",
      "Epoch  2534: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9901302.0\n",
      "loss z =  9901278.0\n",
      "loss z =  9901254.0\n",
      "loss z =  9901230.0\n",
      "loss z =  9901205.0\n",
      "loss z =  9901180.0\n",
      "loss z =  9901157.0\n",
      "loss z =  9901108.0\n",
      "loss z =  9901084.0\n",
      "loss z =  9901060.0\n",
      "Epoch  2545: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  23.50558590888977 s\n",
      "loss_f =  7266667.0\n",
      "loss_f =  7259294.0\n",
      "loss_f =  7255715.0\n",
      "loss_f =  7249681.0\n",
      "loss_f =  7241965.5\n",
      "loss_f =  7236514.5\n",
      "loss_f =  7232269.0\n",
      "Epoch 11019: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7230195.0\n",
      "loss_f =  7228669.0\n",
      "loss_f =  7227340.0\n",
      "loss_f =  7226176.0\n",
      "loss_f =  7225109.0\n",
      "loss_f =  7224107.5\n",
      "loss_f =  7223168.0\n",
      "loss_f =  7222267.0\n",
      "loss_f =  7221400.5\n",
      "loss_f =  7220559.0\n",
      "loss_f =  7219736.0\n",
      "loss_f =  7218924.0\n",
      "loss_f =  7218151.0\n",
      "loss_f =  7217371.0\n",
      "loss_f =  7216607.0\n",
      "loss_f =  7215850.0\n",
      "loss_f =  7215118.5\n",
      "loss_f =  7214381.0\n",
      "loss_f =  7213667.0\n",
      "loss_f =  7212952.5\n",
      "loss_f =  7212244.5\n",
      "loss_f =  7211540.0\n",
      "loss_f =  7210851.0\n",
      "loss_f =  7210162.0\n",
      "loss_f =  7209485.5\n",
      "loss_f =  7208805.0\n",
      "loss_f =  7208138.0\n",
      "loss_f =  7207474.5\n",
      "loss_f =  7206817.0\n",
      "loss_f =  7206166.0\n",
      "loss_f =  7205524.5\n",
      "loss_f =  7204871.5\n",
      "loss_f =  7204239.0\n",
      "thetaf step =  133.88882446289062 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11077.1845703125\n",
      "Epoch  6409: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6425: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6441: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6457: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6409.056640625\n",
      "Epoch  5632: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5648: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5664: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5680: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30098.177734375\n",
      "Epoch 12452: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12468: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12484: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12500: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.9375\n",
      "Epoch  7492: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7508: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7524: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7540: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24291.673828125\n",
      "Epoch  8554: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8570: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8586: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8602: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37792.3828125\n",
      "Epoch  5692: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5708: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5724: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5740: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23249.73046875\n",
      "Epoch  6579: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6595: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6611: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6627: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14114.7900390625\n",
      "Epoch 15898: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 15914: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 15930: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 15946: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2421.359130859375\n",
      "Epoch  7020: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7036: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7052: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7068: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8140.5634765625\n",
      "Epoch  5528: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5544: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5560: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5576: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19107.265625\n",
      "Epoch  7062: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7078: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7094: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7110: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10357.189453125\n",
      "Epoch 12050: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12066: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12082: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12098: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9297.578125\n",
      "Epoch  7291: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7307: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7323: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7339: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8873.236328125\n",
      "Epoch  8490: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8506: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8522: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8538: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15625.83203125\n",
      "Epoch  5607: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5623: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5639: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5655: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.7261962890625\n",
      "Epoch  5213: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5229: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5245: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5261: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1594839096069336 s\n",
      "loss_c =  256.64031982421875\n",
      "loss_c =  254.77774047851562\n",
      "loss_c =  254.2616729736328\n",
      "loss_c =  253.8778076171875\n",
      "loss_c =  253.51467895507812\n",
      "loss_c =  253.16305541992188\n",
      "loss_c =  252.81813049316406\n",
      "loss_c =  252.4768829345703\n",
      "loss_c =  252.13714599609375\n",
      "loss_c =  251.79762268066406\n",
      "thetac step =  10.725608348846436 s\n",
      "step =  61\n",
      "loss z =  9978391.0\n",
      "loss z =  9963605.0\n",
      "loss z =  9950674.0\n",
      "loss z =  9939373.0\n",
      "loss z =  9929522.0\n",
      "loss z =  9921006.0\n",
      "loss z =  9913692.0\n",
      "loss z =  9907458.0\n",
      "loss z =  9902158.0\n",
      "loss z =  9897621.0\n",
      "loss z =  9893660.0\n",
      "Epoch  2557: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9890136.0\n",
      "loss z =  9889486.0\n",
      "loss z =  9888844.0\n",
      "loss z =  9888217.0\n",
      "loss z =  9887595.0\n",
      "loss z =  9886983.0\n",
      "loss z =  9886380.0\n",
      "loss z =  9885786.0\n",
      "loss z =  9884624.0\n",
      "loss z =  9884050.0\n",
      "Epoch  2568: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9883485.0\n",
      "loss z =  9883374.0\n",
      "loss z =  9883261.0\n",
      "loss z =  9883151.0\n",
      "loss z =  9883040.0\n",
      "loss z =  9882929.0\n",
      "loss z =  9882820.0\n",
      "loss z =  9882709.0\n",
      "loss z =  9882600.0\n",
      "loss z =  9882489.0\n",
      "loss z =  9882378.0\n",
      "Epoch  2579: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9882268.0\n",
      "loss z =  9882248.0\n",
      "loss z =  9882224.0\n",
      "loss z =  9882203.0\n",
      "loss z =  9882181.0\n",
      "loss z =  9882160.0\n",
      "loss z =  9882116.0\n",
      "loss z =  9882094.0\n",
      "loss z =  9882072.0\n",
      "loss z =  9882051.0\n",
      "Epoch  2590: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.146470069885254 s\n",
      "loss_f =  7250723.0\n",
      "loss_f =  7242325.0\n",
      "loss_f =  7239176.0\n",
      "loss_f =  7231975.0\n",
      "loss_f =  7224577.0\n",
      "loss_f =  7219336.0\n",
      "Epoch 11214: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7216816.5\n",
      "loss_f =  7214995.5\n",
      "loss_f =  7213455.0\n",
      "loss_f =  7212106.0\n",
      "loss_f =  7210928.0\n",
      "loss_f =  7209858.0\n",
      "Epoch 11245: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7209153.5\n",
      "loss_f =  7208671.0\n",
      "loss_f =  7208210.5\n",
      "loss_f =  7207736.0\n",
      "loss_f =  7207290.0\n",
      "loss_f =  7206853.0\n",
      "Epoch 11276: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  7206499.0\n",
      "loss_f =  7206286.0\n",
      "loss_f =  7206067.5\n",
      "loss_f =  7205851.0\n",
      "loss_f =  7205642.5\n",
      "loss_f =  7205421.0\n",
      "Epoch 11307: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  7205237.0\n",
      "loss_f =  7205134.0\n",
      "loss_f =  7205025.0\n",
      "loss_f =  7204923.5\n",
      "loss_f =  7204814.0\n",
      "loss_f =  7204702.0\n",
      "Epoch 11338: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  7204601.0\n",
      "loss_f =  7204547.0\n",
      "loss_f =  7204492.0\n",
      "loss_f =  7204444.5\n",
      "loss_f =  7204390.5\n",
      "loss_f =  7204335.0\n",
      "loss_f =  7204285.5\n",
      "Epoch 11369: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  7204246.0\n",
      "loss_f =  7204232.0\n",
      "loss_f =  7204199.0\n",
      "thetaf step =  134.5974099636078 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11077.837890625\n",
      "Epoch  6473: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6489: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6505: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6521: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6408.82177734375\n",
      "Epoch  5696: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5712: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5728: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5744: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30097.65625\n",
      "Epoch 12516: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12532: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12548: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12564: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.9453125\n",
      "Epoch  7556: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7572: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7588: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7604: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24291.197265625\n",
      "Epoch  8618: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8634: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8650: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8666: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37792.546875\n",
      "Epoch  5756: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5772: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5788: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5804: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23250.08203125\n",
      "Epoch  6643: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6659: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6675: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6691: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14115.2802734375\n",
      "Epoch 15962: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 15978: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 15994: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 16010: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2421.238037109375\n",
      "Epoch  7084: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7100: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7116: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7132: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8140.03955078125\n",
      "Epoch  5592: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5608: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5624: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5640: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19107.775390625\n",
      "Epoch  7126: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7142: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7158: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7174: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10357.236328125\n",
      "Epoch 12114: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12130: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12146: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12162: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9297.26171875\n",
      "Epoch  7355: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7371: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7387: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7403: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8872.57421875\n",
      "Epoch  8554: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8570: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8586: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8602: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15626.7080078125\n",
      "Epoch  5671: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5687: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5703: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5719: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.8330078125\n",
      "Epoch  5277: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5309: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5325: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1222875118255615 s\n",
      "loss_c =  256.997314453125\n",
      "Epoch 67541: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  256.2164001464844\n",
      "loss_c =  255.79824829101562\n",
      "loss_c =  255.57516479492188\n",
      "loss_c =  255.437744140625\n",
      "loss_c =  255.3369140625\n",
      "loss_c =  255.2520751953125\n",
      "loss_c =  255.17396545410156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_c =  255.09860229492188\n",
      "loss_c =  255.02377319335938\n",
      "thetac step =  10.224644899368286 s\n",
      "step =  62\n",
      "loss z =  9960380.0\n",
      "loss z =  9942396.0\n",
      "loss z =  9928398.0\n",
      "loss z =  9917316.0\n",
      "loss z =  9908194.0\n",
      "loss z =  9900421.0\n",
      "loss z =  9893740.0\n",
      "loss z =  9887991.0\n",
      "loss z =  9882988.0\n",
      "loss z =  9878576.0\n",
      "loss z =  9874644.0\n",
      "Epoch  2602: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9871079.0\n",
      "loss z =  9870413.0\n",
      "loss z =  9869755.0\n",
      "loss z =  9869107.0\n",
      "loss z =  9868466.0\n",
      "loss z =  9867836.0\n",
      "loss z =  9867214.0\n",
      "loss z =  9866598.0\n",
      "loss z =  9865392.0\n",
      "loss z =  9864798.0\n",
      "Epoch  2613: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9864210.0\n",
      "loss z =  9864094.0\n",
      "loss z =  9863978.0\n",
      "loss z =  9863862.0\n",
      "loss z =  9863748.0\n",
      "loss z =  9863630.0\n",
      "loss z =  9863515.0\n",
      "loss z =  9863400.0\n",
      "loss z =  9863284.0\n",
      "loss z =  9863168.0\n",
      "loss z =  9863054.0\n",
      "Epoch  2624: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9862941.0\n",
      "loss z =  9862918.0\n",
      "loss z =  9862895.0\n",
      "loss z =  9862873.0\n",
      "loss z =  9862849.0\n",
      "loss z =  9862826.0\n",
      "loss z =  9862780.0\n",
      "loss z =  9862757.0\n",
      "loss z =  9862734.0\n",
      "loss z =  9862711.0\n",
      "Epoch  2635: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.632609367370605 s\n",
      "loss_f =  7234418.5\n",
      "loss_f =  7228905.5\n",
      "loss_f =  7222895.5\n",
      "Epoch 11400: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7217780.0\n",
      "loss_f =  7213394.0\n",
      "loss_f =  7209672.0\n",
      "loss_f =  7206936.5\n",
      "loss_f =  7204873.0\n",
      "loss_f =  7203239.0\n",
      "loss_f =  7201854.5\n",
      "loss_f =  7200640.0\n",
      "loss_f =  7199542.5\n",
      "loss_f =  7198520.0\n",
      "loss_f =  7197568.0\n",
      "loss_f =  7196671.0\n",
      "loss_f =  7195812.0\n",
      "loss_f =  7194979.0\n",
      "loss_f =  7194167.0\n",
      "loss_f =  7193370.0\n",
      "loss_f =  7192576.0\n",
      "loss_f =  7191815.0\n",
      "loss_f =  7191053.5\n",
      "loss_f =  7190310.0\n",
      "loss_f =  7189573.5\n",
      "loss_f =  7188844.5\n",
      "loss_f =  7188131.0\n",
      "loss_f =  7187432.0\n",
      "loss_f =  7186724.0\n",
      "loss_f =  7186026.0\n",
      "loss_f =  7185340.0\n",
      "loss_f =  7184658.0\n",
      "loss_f =  7183980.0\n",
      "loss_f =  7183318.0\n",
      "loss_f =  7182655.0\n",
      "loss_f =  7182006.5\n",
      "loss_f =  7181347.5\n",
      "loss_f =  7180710.0\n",
      "loss_f =  7180077.5\n",
      "loss_f =  7179435.0\n",
      "loss_f =  7178812.5\n",
      "thetaf step =  135.44709730148315 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11078.041015625\n",
      "Epoch  6537: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6553: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6569: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6585: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6408.67529296875\n",
      "Epoch  5760: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5776: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5792: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5808: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30097.568359375\n",
      "Epoch 12580: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12596: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12612: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12628: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.97265625\n",
      "Epoch  7620: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7636: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7652: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7668: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24291.294921875\n",
      "Epoch  8682: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8698: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8714: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8730: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37792.65234375\n",
      "Epoch  5820: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5836: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5852: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5868: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23250.14453125\n",
      "Epoch  6707: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6723: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6739: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6755: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14115.3935546875\n",
      "Epoch 16026: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 16042: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 16058: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 16074: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2421.183349609375\n",
      "Epoch  7148: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7164: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7180: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7196: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8140.29833984375\n",
      "Epoch  5656: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5672: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5688: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5704: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19107.798828125\n",
      "Epoch  7190: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7206: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7222: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7238: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10357.3701171875\n",
      "Epoch 12178: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12194: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12210: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12226: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9297.13671875\n",
      "Epoch  7419: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7435: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7451: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7467: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8872.3818359375\n",
      "Epoch  8618: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8634: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8650: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8666: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15625.9658203125\n",
      "Epoch  5735: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5751: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5767: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5783: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1813.8846435546875\n",
      "Epoch  5341: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5357: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5373: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5389: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1687018871307373 s\n",
      "loss_c =  256.7242736816406\n",
      "loss_c =  255.29881286621094\n",
      "loss_c =  254.959716796875\n",
      "loss_c =  254.64886474609375\n",
      "loss_c =  254.34298706054688\n",
      "loss_c =  254.03817749023438\n",
      "loss_c =  253.732421875\n",
      "loss_c =  253.42477416992188\n",
      "loss_c =  253.11476135253906\n",
      "loss_c =  252.8017578125\n",
      "thetac step =  10.456753730773926 s\n",
      "step =  63\n",
      "loss z =  9948583.0\n",
      "loss z =  9929102.0\n",
      "loss z =  9914028.0\n",
      "loss z =  9902626.0\n",
      "loss z =  9893998.0\n",
      "loss z =  9887219.0\n",
      "loss z =  9881643.0\n",
      "loss z =  9876890.0\n",
      "loss z =  9872726.0\n",
      "loss z =  9868995.0\n",
      "loss z =  9865593.0\n",
      "Epoch  2647: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9862430.0\n",
      "loss z =  9861830.0\n",
      "loss z =  9861235.0\n",
      "loss z =  9860645.0\n",
      "loss z =  9860065.0\n",
      "loss z =  9859490.0\n",
      "loss z =  9858916.0\n",
      "loss z =  9858349.0\n",
      "loss z =  9857237.0\n",
      "loss z =  9856684.0\n",
      "Epoch  2658: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9856139.0\n",
      "loss z =  9856031.0\n",
      "loss z =  9855922.0\n",
      "loss z =  9855816.0\n",
      "loss z =  9855709.0\n",
      "loss z =  9855600.0\n",
      "loss z =  9855492.0\n",
      "loss z =  9855384.0\n",
      "loss z =  9855276.0\n",
      "loss z =  9855168.0\n",
      "loss z =  9855061.0\n",
      "Epoch  2669: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9854955.0\n",
      "loss z =  9854934.0\n",
      "loss z =  9854912.0\n",
      "loss z =  9854891.0\n",
      "loss z =  9854870.0\n",
      "loss z =  9854848.0\n",
      "loss z =  9854805.0\n",
      "loss z =  9854784.0\n",
      "loss z =  9854763.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss z =  9854742.0\n",
      "Epoch  2680: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.397275686264038 s\n",
      "loss_f =  7229435.0\n",
      "loss_f =  7220266.5\n",
      "loss_f =  7215521.0\n",
      "loss_f =  7209175.0\n",
      "loss_f =  7202728.0\n",
      "loss_f =  7197924.0\n",
      "Epoch 11616: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7194996.5\n",
      "loss_f =  7193397.5\n",
      "loss_f =  7191980.5\n",
      "loss_f =  7190711.0\n",
      "loss_f =  7189572.0\n",
      "loss_f =  7188471.5\n",
      "Epoch 11647: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7187542.0\n",
      "loss_f =  7187027.0\n",
      "loss_f =  7186466.0\n",
      "loss_f =  7185914.0\n",
      "loss_f =  7185391.0\n",
      "loss_f =  7184893.0\n",
      "Epoch 11678: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  7184436.0\n",
      "loss_f =  7184197.5\n",
      "loss_f =  7183996.0\n",
      "loss_f =  7183769.0\n",
      "loss_f =  7183561.0\n",
      "loss_f =  7183350.0\n",
      "loss_f =  7183134.0\n",
      "Epoch 11709: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  7183009.5\n",
      "loss_f =  7182898.5\n",
      "loss_f =  7182800.0\n",
      "loss_f =  7182691.0\n",
      "loss_f =  7182583.0\n",
      "loss_f =  7182481.0\n",
      "Epoch 11740: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  7182406.0\n",
      "loss_f =  7182346.0\n",
      "loss_f =  7182308.0\n",
      "loss_f =  7182254.0\n",
      "loss_f =  7182194.5\n",
      "loss_f =  7182147.5\n",
      "Epoch 11771: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  7182101.0\n",
      "loss_f =  7182079.0\n",
      "loss_f =  7182048.0\n",
      "thetaf step =  134.82488131523132 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11078.1240234375\n",
      "Epoch  6601: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6617: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6633: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6649: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6408.55859375\n",
      "Epoch  5824: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5840: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5856: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5872: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30097.8671875\n",
      "Epoch 12644: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12660: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12676: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12692: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.98046875\n",
      "Epoch  7684: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7700: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7716: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7732: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24290.9453125\n",
      "Epoch  8746: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8762: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8778: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8794: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37792.796875\n",
      "Epoch  5884: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5900: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5916: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5932: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23250.296875\n",
      "Epoch  6771: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6787: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6803: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6819: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14115.5322265625\n",
      "Epoch 16090: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 16106: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 16122: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 16138: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2421.065673828125\n",
      "Epoch  7212: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7228: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7244: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7260: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8139.8466796875\n",
      "Epoch  5721: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5737: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5753: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5769: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19107.974609375\n",
      "Epoch  7254: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7270: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7286: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7302: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10357.2724609375\n",
      "Epoch 12242: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12258: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12274: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12290: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9296.865234375\n",
      "Epoch  7483: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7499: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7515: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7531: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8872.1708984375\n",
      "Epoch  8682: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8698: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8714: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8730: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15626.2216796875\n",
      "Epoch  5799: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5815: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5831: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5847: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1814.0126953125\n",
      "Epoch  5405: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5421: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5437: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5453: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.190136432647705 s\n",
      "loss_c =  256.4908752441406\n",
      "Epoch 68542: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  255.84890747070312\n",
      "loss_c =  255.65794372558594\n",
      "loss_c =  255.5503387451172\n",
      "loss_c =  255.4701690673828\n",
      "loss_c =  255.39915466308594\n",
      "loss_c =  255.33074951171875\n",
      "loss_c =  255.2630615234375\n",
      "loss_c =  255.1947021484375\n",
      "loss_c =  255.12564086914062\n",
      "thetac step =  11.116078853607178 s\n",
      "step =  64\n",
      "loss z =  9943681.0\n",
      "loss z =  9920945.0\n",
      "loss z =  9902793.0\n",
      "loss z =  9889092.0\n",
      "loss z =  9879086.0\n",
      "loss z =  9871630.0\n",
      "loss z =  9865728.0\n",
      "loss z =  9860860.0\n",
      "loss z =  9856636.0\n",
      "loss z =  9852901.0\n",
      "Epoch  2691: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9849538.0\n",
      "loss z =  9848916.0\n",
      "loss z =  9848300.0\n",
      "loss z =  9847695.0\n",
      "loss z =  9847096.0\n",
      "loss z =  9846508.0\n",
      "loss z =  9845929.0\n",
      "loss z =  9845359.0\n",
      "loss z =  9844792.0\n",
      "loss z =  9843684.0\n",
      "Epoch  2702: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9843140.0\n",
      "loss z =  9843032.0\n",
      "loss z =  9842923.0\n",
      "loss z =  9842818.0\n",
      "loss z =  9842712.0\n",
      "loss z =  9842604.0\n",
      "loss z =  9842499.0\n",
      "loss z =  9842392.0\n",
      "loss z =  9842287.0\n",
      "loss z =  9842180.0\n",
      "loss z =  9842074.0\n",
      "Epoch  2713: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9841968.0\n",
      "loss z =  9841947.0\n",
      "loss z =  9841926.0\n",
      "loss z =  9841904.0\n",
      "loss z =  9841884.0\n",
      "loss z =  9841862.0\n",
      "loss z =  9841840.0\n",
      "loss z =  9841798.0\n",
      "loss z =  9841778.0\n",
      "loss z =  9841757.0\n",
      "Epoch  2724: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.304928302764893 s\n",
      "loss_f =  7219060.0\n",
      "loss_f =  7210630.0\n",
      "loss_f =  7203401.0\n",
      "Epoch 11802: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7199799.5\n",
      "loss_f =  7196086.5\n",
      "loss_f =  7192981.5\n",
      "loss_f =  7190715.5\n",
      "loss_f =  7188485.0\n",
      "loss_f =  7186673.5\n",
      "Epoch 11833: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7185243.5\n",
      "loss_f =  7184572.0\n",
      "loss_f =  7183940.5\n",
      "loss_f =  7183376.5\n",
      "loss_f =  7182843.0\n",
      "loss_f =  7182328.0\n",
      "loss_f =  7181823.0\n",
      "Epoch 11864: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  7181537.5\n",
      "loss_f =  7181312.5\n",
      "loss_f =  7181059.5\n",
      "loss_f =  7180830.0\n",
      "loss_f =  7180609.0\n",
      "loss_f =  7180373.0\n",
      "Epoch 11895: reducing learning rate of group 0 to 3.1250e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_f =  7180227.0\n",
      "loss_f =  7180113.0\n",
      "loss_f =  7180001.0\n",
      "loss_f =  7179892.0\n",
      "loss_f =  7179782.0\n",
      "loss_f =  7179674.0\n",
      "Epoch 11926: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  7179572.0\n",
      "loss_f =  7179526.5\n",
      "loss_f =  7179467.0\n",
      "loss_f =  7179403.0\n",
      "loss_f =  7179355.5\n",
      "loss_f =  7179297.0\n",
      "Epoch 11957: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  7179255.5\n",
      "loss_f =  7179225.0\n",
      "loss_f =  7179194.5\n",
      "loss_f =  7179166.0\n",
      "loss_f =  7179136.0\n",
      "loss_f =  7179107.5\n",
      "Epoch 11988: reducing learning rate of group 0 to 3.9063e-06.\n",
      "thetaf step =  136.79358530044556 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11078.1513671875\n",
      "Epoch  6665: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6681: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6697: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6713: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6408.478515625\n",
      "Epoch  5888: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5904: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5920: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5936: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30097.87890625\n",
      "Epoch 12708: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12724: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12740: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12756: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43827.0\n",
      "Epoch  7748: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7764: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7780: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7796: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24291.189453125\n",
      "Epoch  8810: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8826: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8842: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8858: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37792.859375\n",
      "Epoch  5948: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5964: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5980: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5996: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23250.30859375\n",
      "Epoch  6835: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6851: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6867: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6883: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14115.5654296875\n",
      "Epoch 16154: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 16170: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 16186: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 16202: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2421.01123046875\n",
      "Epoch  7276: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7292: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7308: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7324: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8139.99658203125\n",
      "Epoch  5785: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5801: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5817: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5833: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19107.921875\n",
      "Epoch  7318: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7334: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7350: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7366: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10357.3798828125\n",
      "Epoch 12306: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12322: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12338: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12354: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9296.7578125\n",
      "Epoch  7548: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7564: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7580: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7596: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8872.1259765625\n",
      "Epoch  8746: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8762: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8778: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8794: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15625.970703125\n",
      "Epoch  5863: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5879: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5895: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5911: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1814.0284423828125\n",
      "Epoch  5469: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5485: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5501: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5517: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1525235176086426 s\n",
      "loss_c =  256.5849914550781\n",
      "loss_c =  255.3329315185547\n",
      "loss_c =  255.01007080078125\n",
      "loss_c =  254.7161102294922\n",
      "loss_c =  254.42535400390625\n",
      "loss_c =  254.13406372070312\n",
      "loss_c =  253.84109497070312\n",
      "loss_c =  253.54583740234375\n",
      "loss_c =  253.2476348876953\n",
      "loss_c =  252.9461669921875\n",
      "thetac step =  11.374342679977417 s\n",
      "step =  65\n",
      "loss z =  9955395.0\n",
      "loss z =  9929370.0\n",
      "loss z =  9907782.0\n",
      "loss z =  9890020.0\n",
      "loss z =  9875987.0\n",
      "loss z =  9864848.0\n",
      "loss z =  9855967.0\n",
      "loss z =  9848976.0\n",
      "loss z =  9843570.0\n",
      "loss z =  9839585.0\n",
      "loss z =  9836424.0\n",
      "Epoch  2736: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9833650.0\n",
      "loss z =  9833131.0\n",
      "loss z =  9832616.0\n",
      "loss z =  9832109.0\n",
      "loss z =  9831604.0\n",
      "loss z =  9831105.0\n",
      "loss z =  9830613.0\n",
      "loss z =  9830127.0\n",
      "loss z =  9829164.0\n",
      "loss z =  9828692.0\n",
      "Epoch  2747: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9828222.0\n",
      "loss z =  9828128.0\n",
      "loss z =  9828035.0\n",
      "loss z =  9827943.0\n",
      "loss z =  9827850.0\n",
      "loss z =  9827756.0\n",
      "loss z =  9827665.0\n",
      "loss z =  9827572.0\n",
      "loss z =  9827481.0\n",
      "loss z =  9827389.0\n",
      "loss z =  9827296.0\n",
      "Epoch  2758: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9827204.0\n",
      "loss z =  9827186.0\n",
      "loss z =  9827167.0\n",
      "loss z =  9827149.0\n",
      "loss z =  9827130.0\n",
      "loss z =  9827112.0\n",
      "loss z =  9827076.0\n",
      "loss z =  9827058.0\n",
      "loss z =  9827040.0\n",
      "loss z =  9827022.0\n",
      "Epoch  2769: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  25.146777868270874 s\n",
      "loss_f =  7207489.5\n",
      "loss_f =  7200539.0\n",
      "loss_f =  7195666.0\n",
      "loss_f =  7189843.5\n",
      "loss_f =  7183798.5\n",
      "loss_f =  7178885.0\n",
      "loss_f =  7175156.0\n",
      "loss_f =  7172333.0\n",
      "loss_f =  7169931.5\n",
      "loss_f =  7167960.0\n",
      "loss_f =  7166172.0\n",
      "loss_f =  7164527.5\n",
      "loss_f =  7162988.5\n",
      "loss_f =  7161512.0\n",
      "loss_f =  7160069.0\n",
      "loss_f =  7158694.0\n",
      "loss_f =  7157346.0\n",
      "loss_f =  7156035.5\n",
      "loss_f =  7154751.0\n",
      "loss_f =  7153506.0\n",
      "loss_f =  7152271.5\n",
      "loss_f =  7151056.0\n",
      "loss_f =  7149863.0\n",
      "loss_f =  7148703.0\n",
      "loss_f =  7147539.5\n",
      "loss_f =  7146401.0\n",
      "loss_f =  7145267.5\n",
      "loss_f =  7144166.0\n",
      "loss_f =  7143078.0\n",
      "loss_f =  7142004.0\n",
      "loss_f =  7140942.0\n",
      "loss_f =  7139882.0\n",
      "loss_f =  7138851.0\n",
      "loss_f =  7137821.5\n",
      "loss_f =  7136809.5\n",
      "loss_f =  7135811.0\n",
      "loss_f =  7134816.0\n",
      "loss_f =  7133845.5\n",
      "loss_f =  7132867.5\n",
      "loss_f =  7131917.0\n",
      "thetaf step =  136.1251573562622 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11078.16015625\n",
      "Epoch  6729: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6745: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6761: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6777: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6408.3564453125\n",
      "Epoch  5952: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5968: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5984: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6000: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30098.205078125\n",
      "Epoch 12772: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12788: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12804: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12820: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.9921875\n",
      "Epoch  7812: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7828: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7844: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7860: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24290.783203125\n",
      "Epoch  8874: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8890: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8906: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8922: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37792.99609375\n",
      "Epoch  6012: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6028: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6044: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6060: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23250.46875\n",
      "Epoch  6899: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6915: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6931: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6947: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14115.6611328125\n",
      "Epoch 16218: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 16234: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 16250: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 16266: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2420.892578125\n",
      "Epoch  7340: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7356: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7372: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7388: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8139.73095703125\n",
      "Epoch  5849: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5865: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5881: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5897: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19108.05859375\n",
      "Epoch  7382: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7398: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7414: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7430: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10357.275390625\n",
      "Epoch 12370: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12386: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12402: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12418: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9296.505859375\n",
      "Epoch  7612: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7628: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7644: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7660: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8871.96484375\n",
      "Epoch  8810: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8826: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8842: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8858: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15626.1572265625\n",
      "Epoch  5927: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5943: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5959: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5975: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1814.0869140625\n",
      "Epoch  5533: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5549: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5565: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5581: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1268260478973389 s\n",
      "loss_c =  256.254150390625\n",
      "Epoch 69543: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  255.590087890625\n",
      "loss_c =  255.48703002929688\n",
      "loss_c =  255.41368103027344\n",
      "loss_c =  255.34950256347656\n",
      "loss_c =  255.28829956054688\n",
      "loss_c =  255.22740173339844\n",
      "loss_c =  255.166259765625\n",
      "loss_c =  255.10406494140625\n",
      "loss_c =  255.04116821289062\n",
      "thetac step =  11.18775486946106 s\n",
      "step =  66\n",
      "loss z =  9894602.0\n",
      "loss z =  9876794.0\n",
      "loss z =  9861577.0\n",
      "loss z =  9848380.0\n",
      "loss z =  9837273.0\n",
      "loss z =  9827828.0\n",
      "loss z =  9819760.0\n",
      "loss z =  9813096.0\n",
      "loss z =  9807905.0\n",
      "loss z =  9803881.0\n",
      "loss z =  9800684.0\n",
      "Epoch  2781: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9797994.0\n",
      "loss z =  9797482.0\n",
      "loss z =  9796970.0\n",
      "loss z =  9796463.0\n",
      "loss z =  9795960.0\n",
      "loss z =  9795462.0\n",
      "loss z =  9794968.0\n",
      "loss z =  9794473.0\n",
      "loss z =  9793498.0\n",
      "loss z =  9793014.0\n",
      "Epoch  2792: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9792532.0\n",
      "loss z =  9792436.0\n",
      "loss z =  9792340.0\n",
      "loss z =  9792246.0\n",
      "loss z =  9792150.0\n",
      "loss z =  9792054.0\n",
      "loss z =  9791961.0\n",
      "loss z =  9791865.0\n",
      "loss z =  9791770.0\n",
      "loss z =  9791676.0\n",
      "loss z =  9791581.0\n",
      "Epoch  2803: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9791488.0\n",
      "loss z =  9791468.0\n",
      "loss z =  9791450.0\n",
      "loss z =  9791431.0\n",
      "loss z =  9791412.0\n",
      "loss z =  9791394.0\n",
      "loss z =  9791356.0\n",
      "loss z =  9791337.0\n",
      "loss z =  9791318.0\n",
      "loss z =  9791298.0\n",
      "Epoch  2814: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  25.4505774974823 s\n",
      "loss_f =  7175031.0\n",
      "loss_f =  7168651.0\n",
      "loss_f =  7161148.0\n",
      "loss_f =  7155920.0\n",
      "loss_f =  7150779.0\n",
      "loss_f =  7147361.0\n",
      "Epoch 12216: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7145006.0\n",
      "loss_f =  7143628.0\n",
      "loss_f =  7142538.0\n",
      "loss_f =  7141560.0\n",
      "loss_f =  7140668.5\n",
      "loss_f =  7139843.0\n",
      "Epoch 12247: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7139132.5\n",
      "loss_f =  7138739.5\n",
      "loss_f =  7138363.5\n",
      "loss_f =  7138005.0\n",
      "loss_f =  7137639.5\n",
      "loss_f =  7137284.0\n",
      "Epoch 12278: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  7136929.0\n",
      "loss_f =  7136752.5\n",
      "loss_f =  7136574.5\n",
      "loss_f =  7136404.0\n",
      "loss_f =  7136226.0\n",
      "loss_f =  7136050.5\n",
      "loss_f =  7135890.0\n",
      "Epoch 12309: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  7135779.5\n",
      "loss_f =  7135693.0\n",
      "loss_f =  7135606.0\n",
      "loss_f =  7135510.5\n",
      "loss_f =  7135431.0\n",
      "loss_f =  7135352.5\n",
      "Epoch 12340: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  7135293.0\n",
      "loss_f =  7135244.0\n",
      "loss_f =  7135205.0\n",
      "loss_f =  7135156.0\n",
      "loss_f =  7135110.5\n",
      "loss_f =  7135066.5\n",
      "Epoch 12371: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  7135030.0\n",
      "loss_f =  7135014.0\n",
      "loss_f =  7134990.0\n",
      "thetaf step =  147.24364137649536 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11078.17578125\n",
      "Epoch  6793: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6809: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6825: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6841: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6408.298828125\n",
      "Epoch  6016: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6032: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6048: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6064: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30098.306640625\n",
      "Epoch 12836: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12852: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12868: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12884: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43827.015625\n",
      "Epoch  7876: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7892: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7908: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7924: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24291.193359375\n",
      "Epoch  8938: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8954: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8970: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8986: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37793.0546875\n",
      "Epoch  6076: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6092: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6108: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6124: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23250.5\n",
      "Epoch  6963: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6979: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6995: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7011: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14115.703125\n",
      "Epoch 16282: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 16298: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 16314: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 16330: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2420.84716796875\n",
      "Epoch  7404: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7420: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7436: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7452: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8139.73876953125\n",
      "Epoch  5913: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5929: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5945: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5961: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19108.048828125\n",
      "Epoch  7446: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7462: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7478: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7494: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10357.3037109375\n",
      "Epoch 12434: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12450: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12466: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12482: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9296.412109375\n",
      "Epoch  7676: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7692: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7708: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7724: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8871.91015625\n",
      "Epoch  8874: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8890: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8906: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8922: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15626.0380859375\n",
      "Epoch  5991: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6023: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6039: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1814.10693359375\n",
      "Epoch  5597: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5613: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5629: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5645: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.2531778812408447 s\n",
      "loss_c =  256.614501953125\n",
      "loss_c =  255.40011596679688\n",
      "loss_c =  255.0819091796875\n",
      "loss_c =  254.81129455566406\n",
      "loss_c =  254.54254150390625\n",
      "loss_c =  254.2716064453125\n",
      "loss_c =  253.9977569580078\n",
      "loss_c =  253.72052001953125\n",
      "loss_c =  253.439697265625\n",
      "loss_c =  253.15512084960938\n",
      "thetac step =  11.671332120895386 s\n",
      "step =  67\n",
      "loss z =  9886622.0\n",
      "loss z =  9866069.0\n",
      "loss z =  9850216.0\n",
      "loss z =  9837489.0\n",
      "loss z =  9826534.0\n",
      "loss z =  9817566.0\n",
      "loss z =  9810137.0\n",
      "loss z =  9803818.0\n",
      "loss z =  9799044.0\n",
      "loss z =  9795549.0\n",
      "Epoch  2825: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9792808.0\n",
      "loss z =  9792314.0\n",
      "loss z =  9791831.0\n",
      "loss z =  9791351.0\n",
      "loss z =  9790877.0\n",
      "loss z =  9790408.0\n",
      "loss z =  9789947.0\n",
      "loss z =  9789484.0\n",
      "loss z =  9789027.0\n",
      "loss z =  9788124.0\n",
      "Epoch  2836: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9787679.0\n",
      "loss z =  9787590.0\n",
      "loss z =  9787501.0\n",
      "loss z =  9787412.0\n",
      "loss z =  9787324.0\n",
      "loss z =  9787237.0\n",
      "loss z =  9787150.0\n",
      "loss z =  9787062.0\n",
      "loss z =  9786972.0\n",
      "loss z =  9786884.0\n",
      "loss z =  9786795.0\n",
      "Epoch  2847: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9786709.0\n",
      "loss z =  9786691.0\n",
      "loss z =  9786674.0\n",
      "loss z =  9786656.0\n",
      "loss z =  9786639.0\n",
      "loss z =  9786622.0\n",
      "loss z =  9786605.0\n",
      "loss z =  9786570.0\n",
      "loss z =  9786552.0\n",
      "loss z =  9786534.0\n",
      "Epoch  2858: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  26.872286319732666 s\n",
      "loss_f =  7172581.5\n",
      "loss_f =  7166272.5\n",
      "loss_f =  7159949.5\n",
      "Epoch 12402: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7154645.5\n",
      "loss_f =  7151434.0\n",
      "loss_f =  7148637.0\n",
      "loss_f =  7146697.0\n",
      "loss_f =  7145020.0\n",
      "loss_f =  7143612.0\n",
      "Epoch 12433: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7142463.5\n",
      "loss_f =  7141936.0\n",
      "loss_f =  7141422.5\n",
      "loss_f =  7140938.0\n",
      "loss_f =  7140481.0\n",
      "loss_f =  7140042.5\n",
      "loss_f =  7139612.0\n",
      "Epoch 12464: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  7139360.0\n",
      "loss_f =  7139150.5\n",
      "loss_f =  7138947.5\n",
      "loss_f =  7138748.0\n",
      "loss_f =  7138539.5\n",
      "loss_f =  7138341.0\n",
      "Epoch 12495: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  7138198.0\n",
      "loss_f =  7138105.0\n",
      "loss_f =  7138006.0\n",
      "loss_f =  7137910.0\n",
      "loss_f =  7137804.0\n",
      "loss_f =  7137707.0\n",
      "Epoch 12526: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  7137625.5\n",
      "loss_f =  7137581.0\n",
      "loss_f =  7137544.0\n",
      "loss_f =  7137483.0\n",
      "loss_f =  7137442.0\n",
      "loss_f =  7137385.0\n",
      "Epoch 12557: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  7137342.0\n",
      "loss_f =  7137312.0\n",
      "loss_f =  7137289.5\n",
      "loss_f =  7137274.0\n",
      "loss_f =  7137243.5\n",
      "loss_f =  7137209.5\n",
      "Epoch 12588: reducing learning rate of group 0 to 3.9063e-06.\n",
      "thetaf step =  150.63147807121277 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11078.125\n",
      "Epoch  6857: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6873: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6889: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6905: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6408.185546875\n",
      "Epoch  6080: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6096: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6112: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6128: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30098.599609375\n",
      "Epoch 12900: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12916: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12932: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12948: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.984375\n",
      "Epoch  7940: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7956: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7972: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7988: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24290.81640625\n",
      "Epoch  9002: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9018: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9034: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9050: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37793.19921875\n",
      "Epoch  6140: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6156: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6172: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23250.625\n",
      "Epoch  7027: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7043: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7059: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7075: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14115.7802734375\n",
      "Epoch 16346: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 16362: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 16378: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 16394: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2420.744384765625\n",
      "Epoch  7468: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7484: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7500: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7516: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8139.6767578125\n",
      "Epoch  5977: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5993: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6009: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6025: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19108.119140625\n",
      "Epoch  7510: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7526: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7542: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7558: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10357.2373046875\n",
      "Epoch 12498: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12514: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12530: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12546: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9296.189453125\n",
      "Epoch  7740: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7756: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7772: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7788: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8871.7646484375\n",
      "Epoch  8938: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8954: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8970: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8986: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15626.177734375\n",
      "Epoch  6055: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6071: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6087: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6103: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1814.16943359375\n",
      "Epoch  5661: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5677: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5693: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5709: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.3501667976379395 s\n",
      "loss_c =  256.3512268066406\n",
      "Epoch 70544: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  255.80508422851562\n",
      "loss_c =  255.54933166503906\n",
      "loss_c =  255.43031311035156\n",
      "loss_c =  255.35446166992188\n",
      "loss_c =  255.29165649414062\n",
      "loss_c =  255.23318481445312\n",
      "loss_c =  255.175537109375\n",
      "loss_c =  255.11758422851562\n",
      "loss_c =  255.05889892578125\n",
      "thetac step =  12.06399393081665 s\n",
      "step =  68\n",
      "loss z =  9916016.0\n",
      "loss z =  9884385.0\n",
      "loss z =  9857879.0\n",
      "loss z =  9836671.0\n",
      "loss z =  9819976.0\n",
      "loss z =  9808338.0\n",
      "loss z =  9800842.0\n",
      "loss z =  9795982.0\n",
      "loss z =  9792450.0\n",
      "loss z =  9789565.0\n",
      "Epoch  2869: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9786954.0\n",
      "loss z =  9786455.0\n",
      "loss z =  9785964.0\n",
      "loss z =  9785474.0\n",
      "loss z =  9784992.0\n",
      "loss z =  9784510.0\n",
      "loss z =  9784036.0\n",
      "loss z =  9783563.0\n",
      "loss z =  9783094.0\n",
      "loss z =  9782165.0\n",
      "Epoch  2880: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9781706.0\n",
      "loss z =  9781614.0\n",
      "loss z =  9781524.0\n",
      "loss z =  9781433.0\n",
      "loss z =  9781342.0\n",
      "loss z =  9781250.0\n",
      "loss z =  9781161.0\n",
      "loss z =  9781070.0\n",
      "loss z =  9780979.0\n",
      "loss z =  9780890.0\n",
      "loss z =  9780799.0\n",
      "Epoch  2891: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9780710.0\n",
      "loss z =  9780692.0\n",
      "loss z =  9780674.0\n",
      "loss z =  9780656.0\n",
      "loss z =  9780638.0\n",
      "loss z =  9780620.0\n",
      "loss z =  9780603.0\n",
      "loss z =  9780565.0\n",
      "loss z =  9780548.0\n",
      "loss z =  9780529.0\n",
      "Epoch  2902: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  26.202051877975464 s\n",
      "loss_f =  7169365.5\n",
      "loss_f =  7163898.0\n",
      "loss_f =  7158144.0\n",
      "loss_f =  7151286.5\n",
      "loss_f =  7145011.0\n",
      "loss_f =  7140300.0\n",
      "loss_f =  7136777.5\n",
      "Epoch 12619: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7134916.0\n",
      "loss_f =  7133518.0\n",
      "loss_f =  7132287.0\n",
      "loss_f =  7131210.5\n",
      "loss_f =  7130228.0\n",
      "loss_f =  7129312.0\n",
      "loss_f =  7128452.0\n",
      "loss_f =  7127619.5\n",
      "loss_f =  7126828.5\n",
      "loss_f =  7126055.0\n",
      "loss_f =  7125313.0\n",
      "loss_f =  7124574.0\n",
      "loss_f =  7123850.0\n",
      "loss_f =  7123138.0\n",
      "loss_f =  7122436.5\n",
      "loss_f =  7121746.0\n",
      "loss_f =  7121068.0\n",
      "loss_f =  7120401.0\n",
      "loss_f =  7119736.0\n",
      "loss_f =  7119084.0\n",
      "loss_f =  7118442.0\n",
      "loss_f =  7117796.0\n",
      "loss_f =  7117166.0\n",
      "loss_f =  7116542.5\n",
      "loss_f =  7115925.0\n",
      "loss_f =  7115302.5\n",
      "loss_f =  7114689.0\n",
      "loss_f =  7114087.5\n",
      "loss_f =  7113481.0\n",
      "loss_f =  7112886.5\n",
      "loss_f =  7112301.0\n",
      "loss_f =  7111703.0\n",
      "loss_f =  7111118.5\n",
      "thetaf step =  148.64868474006653 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11078.130859375\n",
      "Epoch  6921: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6937: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6953: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6969: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6408.125\n",
      "Epoch  6144: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6160: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6176: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6192: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30098.6796875\n",
      "Epoch 12964: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12980: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12996: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13012: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.9765625\n",
      "Epoch  8004: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8020: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8036: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8052: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24291.158203125\n",
      "Epoch  9066: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9082: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9098: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9114: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37793.27734375\n",
      "Epoch  6204: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6220: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6236: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6252: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23250.671875\n",
      "Epoch  7091: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7107: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7123: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7139: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14115.8125\n",
      "Epoch 16410: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 16426: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 16442: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 16458: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2420.709716796875\n",
      "Epoch  7532: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7548: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7564: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7580: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8139.833984375\n",
      "Epoch  6041: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6057: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6073: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6089: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19108.146484375\n",
      "Epoch  7574: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7590: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7606: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7622: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10357.232421875\n",
      "Epoch 12562: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12578: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12594: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12610: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9296.11328125\n",
      "Epoch  7804: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7820: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7836: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7852: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8871.6953125\n",
      "Epoch  9002: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9018: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9034: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9050: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15626.2890625\n",
      "Epoch  6119: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6135: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6151: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6167: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1814.2034912109375\n",
      "Epoch  5725: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5741: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5757: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5773: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.3362598419189453 s\n",
      "loss_c =  257.0139465332031\n",
      "loss_c =  255.58389282226562\n",
      "loss_c =  255.20909118652344\n",
      "loss_c =  254.93875122070312\n",
      "loss_c =  254.67416381835938\n",
      "loss_c =  254.40892028808594\n",
      "loss_c =  254.14193725585938\n",
      "loss_c =  253.8724365234375\n",
      "loss_c =  253.59986877441406\n",
      "loss_c =  253.323974609375\n",
      "thetac step =  12.331274271011353 s\n",
      "step =  69\n",
      "loss z =  9920506.0\n",
      "loss z =  9887994.0\n",
      "loss z =  9861317.0\n",
      "loss z =  9840285.0\n",
      "loss z =  9823695.0\n",
      "loss z =  9810092.0\n",
      "loss z =  9798592.0\n",
      "loss z =  9789047.0\n",
      "loss z =  9781419.0\n",
      "loss z =  9775372.0\n",
      "loss z =  9770895.0\n",
      "Epoch  2914: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9767513.0\n",
      "loss z =  9766939.0\n",
      "loss z =  9766385.0\n",
      "loss z =  9765844.0\n",
      "loss z =  9765311.0\n",
      "loss z =  9764790.0\n",
      "loss z =  9764280.0\n",
      "loss z =  9763778.0\n",
      "loss z =  9762788.0\n",
      "loss z =  9762305.0\n",
      "Epoch  2925: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9761824.0\n",
      "loss z =  9761731.0\n",
      "loss z =  9761636.0\n",
      "loss z =  9761541.0\n",
      "loss z =  9761446.0\n",
      "loss z =  9761353.0\n",
      "loss z =  9761258.0\n",
      "loss z =  9761164.0\n",
      "loss z =  9761071.0\n",
      "loss z =  9760978.0\n",
      "loss z =  9760884.0\n",
      "Epoch  2936: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9760790.0\n",
      "loss z =  9760771.0\n",
      "loss z =  9760752.0\n",
      "loss z =  9760733.0\n",
      "loss z =  9760715.0\n",
      "loss z =  9760697.0\n",
      "loss z =  9760660.0\n",
      "loss z =  9760641.0\n",
      "loss z =  9760622.0\n",
      "loss z =  9760604.0\n",
      "Epoch  2947: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  26.96716594696045 s\n",
      "loss_f =  7152463.5\n",
      "loss_f =  7145686.5\n",
      "loss_f =  7140304.5\n",
      "loss_f =  7134218.0\n",
      "loss_f =  7128951.0\n",
      "Epoch 12813: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7125300.5\n",
      "loss_f =  7123542.0\n",
      "loss_f =  7121947.5\n",
      "loss_f =  7120639.0\n",
      "loss_f =  7119504.0\n",
      "loss_f =  7118466.5\n",
      "loss_f =  7117509.0\n",
      "Epoch 12844: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7116962.5\n",
      "loss_f =  7116532.0\n",
      "loss_f =  7116103.0\n",
      "loss_f =  7115696.5\n",
      "loss_f =  7115294.0\n",
      "loss_f =  7114895.0\n",
      "Epoch 12875: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  7114620.0\n",
      "loss_f =  7114422.5\n",
      "loss_f =  7114230.0\n",
      "loss_f =  7114036.0\n",
      "loss_f =  7113841.0\n",
      "loss_f =  7113653.0\n",
      "Epoch 12906: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  7113501.5\n",
      "loss_f =  7113405.0\n",
      "loss_f =  7113306.0\n",
      "loss_f =  7113215.0\n",
      "loss_f =  7113116.0\n",
      "loss_f =  7113022.0\n",
      "Epoch 12937: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  7112933.0\n",
      "loss_f =  7112890.0\n",
      "loss_f =  7112847.0\n",
      "loss_f =  7112802.0\n",
      "loss_f =  7112752.0\n",
      "loss_f =  7112703.5\n",
      "Epoch 12968: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  7112656.0\n",
      "loss_f =  7112634.0\n",
      "loss_f =  7112605.0\n",
      "loss_f =  7112591.5\n",
      "thetaf step =  141.6631190776825 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11078.05078125\n",
      "Epoch  6985: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7001: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7017: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7033: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6408.01806640625\n",
      "Epoch  6208: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6224: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6240: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6256: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30099.044921875\n",
      "Epoch 13028: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13044: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13060: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13076: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.93359375\n",
      "Epoch  8068: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8084: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8100: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8116: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24290.76953125\n",
      "Epoch  9130: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9146: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9162: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9178: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37793.46875\n",
      "Epoch  6268: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6284: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6300: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6316: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23250.73046875\n",
      "Epoch  7155: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7171: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7187: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7203: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14115.8837890625\n",
      "Epoch 16474: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 16490: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 16506: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 16522: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2420.6162109375\n",
      "Epoch  7596: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7612: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7628: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7644: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8139.6259765625\n",
      "Epoch  6105: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6121: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6137: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6153: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19108.115234375\n",
      "Epoch  7638: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7654: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7670: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7686: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10357.19140625\n",
      "Epoch 12626: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12642: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12658: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12674: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9295.927734375\n",
      "Epoch  7868: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7884: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7900: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7916: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8871.560546875\n",
      "Epoch  9066: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9082: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9098: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9114: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15626.4150390625\n",
      "Epoch  6183: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6199: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6215: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6231: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1814.271484375\n",
      "Epoch  5789: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5805: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5821: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5837: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1363821029663086 s\n",
      "loss_c =  256.58319091796875\n",
      "Epoch 71545: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  255.97927856445312\n",
      "loss_c =  255.6705322265625\n",
      "loss_c =  255.5186767578125\n",
      "loss_c =  255.42652893066406\n",
      "loss_c =  255.35643005371094\n",
      "loss_c =  255.29454040527344\n",
      "loss_c =  255.23519897460938\n",
      "loss_c =  255.17648315429688\n",
      "loss_c =  255.11741638183594\n",
      "thetac step =  11.566143274307251 s\n",
      "step =  70\n",
      "loss z =  10092105.0\n",
      "loss z =  10062008.0\n",
      "loss z =  10037566.0\n",
      "loss z =  10017872.0\n",
      "loss z =  10000619.0\n",
      "loss z =  9985446.0\n",
      "loss z =  9972330.0\n",
      "loss z =  9959660.0\n",
      "loss z =  9947410.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss z =  9935807.0\n",
      "loss z =  9924570.0\n",
      "Epoch  2959: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9913326.0\n",
      "loss z =  9911008.0\n",
      "loss z =  9908834.0\n",
      "loss z =  9906580.0\n",
      "loss z =  9904552.0\n",
      "loss z =  9902393.0\n",
      "loss z =  9900288.0\n",
      "loss z =  9898083.0\n",
      "loss z =  9893638.0\n",
      "loss z =  9891550.0\n",
      "Epoch  2970: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9889513.0\n",
      "loss z =  9889080.0\n",
      "loss z =  9888690.0\n",
      "loss z =  9888217.0\n",
      "loss z =  9887829.0\n",
      "loss z =  9887374.0\n",
      "loss z =  9886946.0\n",
      "loss z =  9886495.0\n",
      "loss z =  9886043.0\n",
      "loss z =  9885611.0\n",
      "loss z =  9885203.0\n",
      "Epoch  2981: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9884794.0\n",
      "loss z =  9884716.0\n",
      "loss z =  9884617.0\n",
      "loss z =  9884540.0\n",
      "loss z =  9884422.0\n",
      "loss z =  9884345.0\n",
      "loss z =  9884170.0\n",
      "loss z =  9884091.0\n",
      "loss z =  9884011.0\n",
      "loss z =  9883934.0\n",
      "Epoch  2992: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  25.053831577301025 s\n",
      "loss_f =  7278785.0\n",
      "loss_f =  7268721.0\n",
      "loss_f =  7257947.5\n",
      "Epoch 12999: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7250009.0\n",
      "loss_f =  7243441.0\n",
      "loss_f =  7238287.0\n",
      "loss_f =  7234697.0\n",
      "loss_f =  7231850.0\n",
      "loss_f =  7229448.0\n",
      "Epoch 13030: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7227934.0\n",
      "loss_f =  7226954.0\n",
      "loss_f =  7226035.0\n",
      "loss_f =  7225128.5\n",
      "loss_f =  7224280.5\n",
      "loss_f =  7223473.0\n",
      "Epoch 13061: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  7222830.0\n",
      "loss_f =  7222440.0\n",
      "loss_f =  7222056.0\n",
      "loss_f =  7221666.5\n",
      "loss_f =  7221288.0\n",
      "loss_f =  7220907.5\n",
      "Epoch 13092: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  7220568.0\n",
      "loss_f =  7220382.0\n",
      "loss_f =  7220194.5\n",
      "loss_f =  7220005.0\n",
      "loss_f =  7219819.0\n",
      "loss_f =  7219616.0\n",
      "Epoch 13123: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  7219398.0\n",
      "loss_f =  7219316.5\n",
      "loss_f =  7219219.0\n",
      "loss_f =  7219129.0\n",
      "loss_f =  7219038.0\n",
      "loss_f =  7218942.5\n",
      "loss_f =  7218849.0\n",
      "Epoch 13154: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  7218795.0\n",
      "loss_f =  7218744.5\n",
      "loss_f =  7218696.5\n",
      "loss_f =  7218652.0\n",
      "loss_f =  7218598.5\n",
      "loss_f =  7218559.0\n",
      "Epoch 13185: reducing learning rate of group 0 to 3.9063e-06.\n",
      "thetaf step =  136.8271131515503 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11078.068359375\n",
      "Epoch  7049: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7065: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7081: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7097: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6407.935546875\n",
      "Epoch  6272: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6288: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6304: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6320: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30099.08203125\n",
      "Epoch 13092: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13108: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13124: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13140: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.9296875\n",
      "Epoch  8132: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8148: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8164: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8180: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24291.509765625\n",
      "Epoch  9194: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9210: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9226: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9242: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37793.5703125\n",
      "Epoch  6332: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6348: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6364: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6380: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23250.775390625\n",
      "Epoch  7219: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7235: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7251: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7267: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14115.9169921875\n",
      "Epoch 16538: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 16554: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 16570: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 16586: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2420.5869140625\n",
      "Epoch  7660: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7676: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7692: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7708: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8139.78125\n",
      "Epoch  6169: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6185: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6201: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6217: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19108.111328125\n",
      "Epoch  7702: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7718: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7734: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7750: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10357.2109375\n",
      "Epoch 12690: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12706: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12722: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12738: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9295.859375\n",
      "Epoch  7932: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7948: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7964: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7980: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8871.4794921875\n",
      "Epoch  9130: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9146: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9162: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9178: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15626.28515625\n",
      "Epoch  6247: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6263: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6279: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6295: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1814.2786865234375\n",
      "Epoch  5853: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5869: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5885: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5901: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1351251602172852 s\n",
      "loss_c =  257.33746337890625\n",
      "loss_c =  255.53768920898438\n",
      "loss_c =  255.1473388671875\n",
      "loss_c =  254.8515167236328\n",
      "loss_c =  254.51739501953125\n",
      "loss_c =  253.97767639160156\n",
      "loss_c =  252.79678344726562\n",
      "loss_c =  251.93597412109375\n",
      "loss_c =  251.3689727783203\n",
      "loss_c =  250.89991760253906\n",
      "thetac step =  11.983246803283691 s\n",
      "step =  71\n",
      "loss z =  10198352.0\n",
      "loss z =  10169018.0\n",
      "loss z =  10145685.0\n",
      "loss z =  10126835.0\n",
      "loss z =  10111935.0\n",
      "loss z =  10099489.0\n",
      "loss z =  10088580.0\n",
      "loss z =  10078571.0\n",
      "loss z =  10069418.0\n",
      "loss z =  10061269.0\n",
      "Epoch  3003: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  10053643.0\n",
      "loss z =  10052076.0\n",
      "loss z =  10050652.0\n",
      "loss z =  10049212.0\n",
      "loss z =  10047849.0\n",
      "loss z =  10046500.0\n",
      "loss z =  10045188.0\n",
      "loss z =  10043921.0\n",
      "loss z =  10042604.0\n",
      "loss z =  10039976.0\n",
      "Epoch  3014: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  10038767.0\n",
      "loss z =  10038498.0\n",
      "loss z =  10038225.0\n",
      "loss z =  10037997.0\n",
      "loss z =  10037752.0\n",
      "loss z =  10037502.0\n",
      "loss z =  10037230.0\n",
      "loss z =  10037003.0\n",
      "loss z =  10036756.0\n",
      "loss z =  10036532.0\n",
      "loss z =  10036222.0\n",
      "Epoch  3025: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  10035994.0\n",
      "loss z =  10035949.0\n",
      "loss z =  10035903.0\n",
      "loss z =  10035858.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss z =  10035812.0\n",
      "loss z =  10035745.0\n",
      "loss z =  10035680.0\n",
      "loss z =  10035589.0\n",
      "loss z =  10035522.0\n",
      "loss z =  10035436.0\n",
      "Epoch  3036: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  24.900869607925415 s\n",
      "loss_f =  7425736.0\n",
      "loss_f =  7374698.5\n",
      "loss_f =  7349769.0\n",
      "loss_f =  7323403.0\n",
      "loss_f =  7302257.5\n",
      "loss_f =  7285959.5\n",
      "loss_f =  7273115.5\n",
      "Epoch 13216: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7266305.0\n",
      "loss_f =  7261076.5\n",
      "loss_f =  7256502.5\n",
      "loss_f =  7252312.5\n",
      "loss_f =  7248393.0\n",
      "loss_f =  7244630.0\n",
      "Epoch 13247: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7242160.0\n",
      "loss_f =  7240348.0\n",
      "loss_f =  7238670.0\n",
      "loss_f =  7237024.0\n",
      "loss_f =  7235390.0\n",
      "loss_f =  7233790.0\n",
      "Epoch 13278: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  7232512.5\n",
      "loss_f =  7231728.0\n",
      "loss_f =  7230967.0\n",
      "loss_f =  7230155.0\n",
      "loss_f =  7229412.0\n",
      "loss_f =  7228637.5\n",
      "Epoch 13309: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  7227958.0\n",
      "loss_f =  7227572.0\n",
      "loss_f =  7227156.0\n",
      "loss_f =  7226788.0\n",
      "loss_f =  7226426.5\n",
      "loss_f =  7226066.0\n",
      "Epoch 13340: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  7225695.5\n",
      "loss_f =  7225496.5\n",
      "loss_f =  7225317.0\n",
      "loss_f =  7225139.0\n",
      "loss_f =  7224951.5\n",
      "loss_f =  7224766.0\n",
      "loss_f =  7224551.0\n",
      "Epoch 13371: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  7224442.0\n",
      "loss_f =  7224346.0\n",
      "thetaf step =  140.18830513954163 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11078.6044921875\n",
      "Epoch  7113: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7129: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7145: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7161: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6408.12060546875\n",
      "Epoch  6336: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6352: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6368: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6384: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30100.0234375\n",
      "Epoch 13156: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13172: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13188: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13204: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.765625\n",
      "Epoch  8196: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8212: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8228: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8244: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24292.80078125\n",
      "Epoch  9258: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9274: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9290: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9306: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37793.78125\n",
      "Epoch  6396: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6412: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6428: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6444: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23250.58984375\n",
      "Epoch  7283: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7299: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7315: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7331: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14116.5693359375\n",
      "Epoch 16602: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 16618: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 16634: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 16650: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2420.5400390625\n",
      "Epoch  7724: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7740: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7756: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7772: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8139.6279296875\n",
      "Epoch  6233: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6249: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6265: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6281: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19108.4453125\n",
      "Epoch  7766: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7782: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7798: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7814: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10357.3544921875\n",
      "Epoch 12754: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12770: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12786: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12802: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9295.861328125\n",
      "Epoch  7996: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8012: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8028: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8044: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8871.296875\n",
      "Epoch  9194: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9210: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9226: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9242: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15623.7138671875\n",
      "Epoch  6311: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6327: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6343: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6359: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1814.5823974609375\n",
      "Epoch  5917: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5933: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  5949: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  5965: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.164891004562378 s\n",
      "loss_c =  258.632568359375\n",
      "Epoch 72546: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  255.8963623046875\n",
      "loss_c =  255.64512634277344\n",
      "loss_c =  255.46392822265625\n",
      "loss_c =  255.32687377929688\n",
      "loss_c =  255.21597290039062\n",
      "loss_c =  255.12039184570312\n",
      "loss_c =  255.0337677001953\n",
      "loss_c =  254.95211791992188\n",
      "loss_c =  254.87315368652344\n",
      "thetac step =  12.295734643936157 s\n",
      "step =  72\n",
      "loss z =  9966829.0\n",
      "loss z =  9936278.0\n",
      "loss z =  9912672.0\n",
      "loss z =  9894970.0\n",
      "loss z =  9881263.0\n",
      "loss z =  9870500.0\n",
      "loss z =  9861729.0\n",
      "loss z =  9854852.0\n",
      "loss z =  9849245.0\n",
      "loss z =  9844499.0\n",
      "Epoch  3047: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9840327.0\n",
      "loss z =  9839559.0\n",
      "loss z =  9838804.0\n",
      "loss z =  9838062.0\n",
      "loss z =  9837329.0\n",
      "loss z =  9836609.0\n",
      "loss z =  9835899.0\n",
      "loss z =  9835198.0\n",
      "loss z =  9834507.0\n",
      "loss z =  9833154.0\n",
      "Epoch  3058: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9832488.0\n",
      "loss z =  9832356.0\n",
      "loss z =  9832225.0\n",
      "loss z =  9832094.0\n",
      "loss z =  9831963.0\n",
      "loss z =  9831833.0\n",
      "loss z =  9831702.0\n",
      "loss z =  9831572.0\n",
      "loss z =  9831442.0\n",
      "loss z =  9831314.0\n",
      "loss z =  9831184.0\n",
      "Epoch  3069: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9831058.0\n",
      "loss z =  9831032.0\n",
      "loss z =  9831006.0\n",
      "loss z =  9830981.0\n",
      "loss z =  9830955.0\n",
      "loss z =  9830930.0\n",
      "loss z =  9830904.0\n",
      "loss z =  9830852.0\n",
      "loss z =  9830827.0\n",
      "loss z =  9830801.0\n",
      "Epoch  3080: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  25.363260984420776 s\n",
      "loss_f =  7222281.0\n",
      "loss_f =  7200699.5\n",
      "loss_f =  7195314.5\n",
      "loss_f =  7181824.0\n",
      "Epoch 13402: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7173439.5\n",
      "loss_f =  7167645.0\n",
      "loss_f =  7162857.0\n",
      "loss_f =  7159023.0\n",
      "loss_f =  7155887.5\n",
      "loss_f =  7153300.5\n",
      "Epoch 13433: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7151517.0\n",
      "loss_f =  7150502.0\n",
      "loss_f =  7149548.0\n",
      "loss_f =  7148655.0\n",
      "loss_f =  7147795.0\n",
      "loss_f =  7146956.5\n",
      "Epoch 13464: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  7146227.0\n",
      "loss_f =  7145833.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_f =  7145441.0\n",
      "loss_f =  7145064.5\n",
      "loss_f =  7144694.5\n",
      "loss_f =  7144320.0\n",
      "Epoch 13495: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  7143945.0\n",
      "loss_f =  7143780.0\n",
      "loss_f =  7143589.5\n",
      "loss_f =  7143412.0\n",
      "loss_f =  7143229.0\n",
      "loss_f =  7143051.0\n",
      "loss_f =  7142872.0\n",
      "Epoch 13526: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  7142756.0\n",
      "loss_f =  7142677.0\n",
      "loss_f =  7142586.0\n",
      "loss_f =  7142492.0\n",
      "loss_f =  7142400.0\n",
      "loss_f =  7142317.0\n",
      "Epoch 13557: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  7142243.0\n",
      "loss_f =  7142201.0\n",
      "loss_f =  7142165.0\n",
      "loss_f =  7142117.5\n",
      "loss_f =  7142078.0\n",
      "thetaf step =  145.96098256111145 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11079.1142578125\n",
      "Epoch  7177: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7193: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7209: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7225: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6407.857421875\n",
      "Epoch  6400: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6416: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6432: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6448: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30099.763671875\n",
      "Epoch 13220: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13236: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13252: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13268: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.703125\n",
      "Epoch  8260: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8276: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8292: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8308: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24290.646484375\n",
      "Epoch  9322: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9338: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9354: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9370: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37793.796875\n",
      "Epoch  6460: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6476: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6492: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6508: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23250.908203125\n",
      "Epoch  7347: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7363: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7379: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7395: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14116.888671875\n",
      "Epoch 16666: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 16682: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 16698: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 16714: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2420.576171875\n",
      "Epoch  7788: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7804: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7820: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7836: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8139.79248046875\n",
      "Epoch  6297: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6313: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6329: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6345: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19108.78515625\n",
      "Epoch  7830: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7846: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7862: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7878: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10357.06640625\n",
      "Epoch 12818: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12834: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12850: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12866: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9295.7666015625\n",
      "Epoch  8061: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8077: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8093: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8109: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8870.7939453125\n",
      "Epoch  9258: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9274: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9290: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9306: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15623.583984375\n",
      "Epoch  6375: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6391: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6407: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6423: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1814.3895263671875\n",
      "Epoch  5981: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5997: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6013: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6029: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1859986782073975 s\n",
      "loss_c =  256.39068603515625\n",
      "loss_c =  255.39016723632812\n",
      "loss_c =  255.09884643554688\n",
      "loss_c =  254.82785034179688\n",
      "loss_c =  254.55923461914062\n",
      "loss_c =  254.290771484375\n",
      "loss_c =  254.0214385986328\n",
      "loss_c =  253.75018310546875\n",
      "loss_c =  253.47659301757812\n",
      "loss_c =  253.20005798339844\n",
      "thetac step =  12.589844703674316 s\n",
      "step =  73\n",
      "loss z =  9939139.0\n",
      "loss z =  9909126.0\n",
      "loss z =  9884868.0\n",
      "loss z =  9865003.0\n",
      "loss z =  9848619.0\n",
      "loss z =  9834936.0\n",
      "loss z =  9823410.0\n",
      "loss z =  9813694.0\n",
      "loss z =  9805612.0\n",
      "loss z =  9798959.0\n",
      "Epoch  3091: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9793903.0\n",
      "loss z =  9793056.0\n",
      "loss z =  9792230.0\n",
      "loss z =  9791429.0\n",
      "loss z =  9790649.0\n",
      "loss z =  9789887.0\n",
      "loss z =  9789143.0\n",
      "loss z =  9788412.0\n",
      "loss z =  9787697.0\n",
      "loss z =  9786298.0\n",
      "Epoch  3102: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9785616.0\n",
      "loss z =  9785483.0\n",
      "loss z =  9785347.0\n",
      "loss z =  9785212.0\n",
      "loss z =  9785079.0\n",
      "loss z =  9784945.0\n",
      "loss z =  9784812.0\n",
      "loss z =  9784680.0\n",
      "loss z =  9784548.0\n",
      "loss z =  9784417.0\n",
      "loss z =  9784284.0\n",
      "Epoch  3113: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9784153.0\n",
      "loss z =  9784127.0\n",
      "loss z =  9784101.0\n",
      "loss z =  9784074.0\n",
      "loss z =  9784048.0\n",
      "loss z =  9784021.0\n",
      "loss z =  9783994.0\n",
      "loss z =  9783942.0\n",
      "loss z =  9783916.0\n",
      "loss z =  9783890.0\n",
      "Epoch  3124: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  26.144028902053833 s\n",
      "loss_f =  7177509.0\n",
      "Epoch 13588: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7159575.0\n",
      "loss_f =  7155721.5\n",
      "loss_f =  7149720.0\n",
      "loss_f =  7144553.0\n",
      "loss_f =  7140668.5\n",
      "loss_f =  7137615.5\n",
      "Epoch 13619: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7135348.0\n",
      "loss_f =  7134235.0\n",
      "loss_f =  7133252.0\n",
      "loss_f =  7132387.5\n",
      "loss_f =  7131604.0\n",
      "loss_f =  7130880.0\n",
      "Epoch 13650: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  7130182.0\n",
      "loss_f =  7129850.0\n",
      "loss_f =  7129532.0\n",
      "loss_f =  7129218.0\n",
      "loss_f =  7128908.0\n",
      "loss_f =  7128609.0\n",
      "loss_f =  7128304.5\n",
      "Epoch 13681: reducing learning rate of group 0 to 3.1250e-05.\n",
      "loss_f =  7128128.0\n",
      "loss_f =  7127980.0\n",
      "loss_f =  7127833.5\n",
      "loss_f =  7127685.0\n",
      "loss_f =  7127541.0\n",
      "loss_f =  7127402.5\n",
      "Epoch 13712: reducing learning rate of group 0 to 1.5625e-05.\n",
      "loss_f =  7127301.0\n",
      "loss_f =  7127225.0\n",
      "loss_f =  7127157.5\n",
      "loss_f =  7127084.5\n",
      "loss_f =  7127017.0\n",
      "loss_f =  7126935.0\n",
      "Epoch 13743: reducing learning rate of group 0 to 7.8125e-06.\n",
      "loss_f =  7126878.0\n",
      "loss_f =  7126846.0\n",
      "loss_f =  7126808.5\n",
      "loss_f =  7126771.0\n",
      "loss_f =  7126741.0\n",
      "loss_f =  7126710.0\n",
      "Epoch 13774: reducing learning rate of group 0 to 3.9063e-06.\n",
      "thetaf step =  142.99246859550476 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11078.8828125\n",
      "Epoch  7241: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7257: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7273: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7289: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6407.94873046875\n",
      "Epoch  6464: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6480: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6496: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6512: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30099.95703125\n",
      "Epoch 13284: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13300: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13316: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13332: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.7421875\n",
      "Epoch  8324: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8340: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8356: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8372: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24291.095703125\n",
      "Epoch  9386: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9402: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9418: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9434: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37794.046875\n",
      "Epoch  6524: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6540: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6556: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6572: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23250.671875\n",
      "Epoch  7411: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7427: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7443: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7459: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14116.8388671875\n",
      "Epoch 16730: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 16746: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 16762: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 16778: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2420.461181640625\n",
      "Epoch  7852: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7868: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7884: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7900: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8143.67431640625\n",
      "Epoch  6361: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6377: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6393: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6409: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19108.548828125\n",
      "Epoch  7894: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7910: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7926: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7942: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10357.4931640625\n",
      "Epoch 12882: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12898: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12914: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12930: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9295.6376953125\n",
      "Epoch  8125: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8141: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8157: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8173: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8870.8994140625\n",
      "Epoch  9322: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9338: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9354: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9370: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15623.3271484375\n",
      "Epoch  6439: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6455: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6471: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6487: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1814.6563720703125\n",
      "Epoch  6045: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6061: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6077: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6093: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.2137792110443115 s\n",
      "loss_c =  260.84246826171875\n",
      "Epoch 73547: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  257.2135314941406\n",
      "loss_c =  257.0157165527344\n",
      "loss_c =  256.8586730957031\n",
      "loss_c =  256.7243957519531\n",
      "loss_c =  256.6031799316406\n",
      "loss_c =  256.4893798828125\n",
      "loss_c =  256.3835144042969\n",
      "loss_c =  256.2809753417969\n",
      "loss_c =  256.17999267578125\n",
      "thetac step =  12.9191255569458 s\n",
      "step =  74\n",
      "loss z =  9834281.0\n",
      "loss z =  9813471.0\n",
      "loss z =  9799364.0\n",
      "loss z =  9789404.0\n",
      "loss z =  9781812.0\n",
      "loss z =  9775657.0\n",
      "loss z =  9770444.0\n",
      "loss z =  9765925.0\n",
      "loss z =  9761977.0\n",
      "loss z =  9758481.0\n",
      "Epoch  3135: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9755364.0\n",
      "loss z =  9754792.0\n",
      "loss z =  9754226.0\n",
      "loss z =  9753673.0\n",
      "loss z =  9753126.0\n",
      "loss z =  9752588.0\n",
      "loss z =  9752056.0\n",
      "loss z =  9751538.0\n",
      "loss z =  9751023.0\n",
      "loss z =  9750021.0\n",
      "Epoch  3146: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9749529.0\n",
      "loss z =  9749431.0\n",
      "loss z =  9749334.0\n",
      "loss z =  9749240.0\n",
      "loss z =  9749143.0\n",
      "loss z =  9749046.0\n",
      "loss z =  9748951.0\n",
      "loss z =  9748855.0\n",
      "loss z =  9748760.0\n",
      "loss z =  9748664.0\n",
      "loss z =  9748569.0\n",
      "Epoch  3157: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9748473.0\n",
      "loss z =  9748454.0\n",
      "loss z =  9748434.0\n",
      "loss z =  9748416.0\n",
      "loss z =  9748396.0\n",
      "loss z =  9748377.0\n",
      "loss z =  9748358.0\n",
      "loss z =  9748320.0\n",
      "loss z =  9748301.0\n",
      "loss z =  9748282.0\n",
      "Epoch  3168: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  25.869017839431763 s\n",
      "loss_f =  7144808.0\n",
      "loss_f =  7137817.5\n",
      "loss_f =  7135237.5\n",
      "loss_f =  7126184.0\n",
      "loss_f =  7120316.0\n",
      "loss_f =  7116702.5\n",
      "loss_f =  7113502.5\n",
      "Epoch 13805: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7111753.0\n",
      "loss_f =  7110424.5\n",
      "loss_f =  7109321.0\n",
      "loss_f =  7108363.0\n",
      "loss_f =  7107475.0\n",
      "loss_f =  7106614.0\n",
      "loss_f =  7105778.5\n",
      "loss_f =  7104985.0\n",
      "loss_f =  7104229.5\n",
      "loss_f =  7103501.5\n",
      "loss_f =  7102799.5\n",
      "loss_f =  7102111.0\n",
      "loss_f =  7101437.5\n",
      "loss_f =  7100783.5\n",
      "loss_f =  7100129.0\n",
      "loss_f =  7099501.0\n",
      "loss_f =  7098877.0\n",
      "loss_f =  7098251.0\n",
      "loss_f =  7097645.0\n",
      "loss_f =  7097030.0\n",
      "loss_f =  7096432.5\n",
      "loss_f =  7095838.5\n",
      "loss_f =  7095260.0\n",
      "loss_f =  7094665.0\n",
      "loss_f =  7094062.5\n",
      "loss_f =  7093461.0\n",
      "loss_f =  7092859.0\n",
      "loss_f =  7092283.0\n",
      "loss_f =  7091713.5\n",
      "loss_f =  7091148.0\n",
      "loss_f =  7090592.0\n",
      "loss_f =  7090039.5\n",
      "loss_f =  7089495.0\n",
      "thetaf step =  146.66105842590332 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11079.2587890625\n",
      "Epoch  7305: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7321: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7337: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7353: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6407.537109375\n",
      "Epoch  6528: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6544: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6560: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6576: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30099.88671875\n",
      "Epoch 13348: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13364: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13380: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13396: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.68359375\n",
      "Epoch  8388: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8404: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8420: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8436: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24290.921875\n",
      "Epoch  9450: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9466: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9482: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9498: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37794.015625\n",
      "Epoch  6588: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6604: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6620: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6636: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23251.0625\n",
      "Epoch  7475: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7491: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7507: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7523: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14117.021484375\n",
      "Epoch 16794: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 16810: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 16826: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 16842: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2420.462646484375\n",
      "Epoch  7916: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7932: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7948: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7964: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8139.42578125\n",
      "Epoch  6425: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6441: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6457: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6473: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19108.955078125\n",
      "Epoch  7958: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7974: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7990: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8006: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10356.9013671875\n",
      "Epoch 12946: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12962: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 12978: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 12994: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9295.4267578125\n",
      "Epoch  8189: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8205: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8221: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8237: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8870.1650390625\n",
      "Epoch  9386: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9402: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9418: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9434: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15623.4375\n",
      "Epoch  6503: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6519: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6535: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6551: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1814.886962890625\n",
      "Epoch  6109: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6125: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6141: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6157: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.16233491897583 s\n",
      "loss_c =  256.01116943359375\n",
      "loss_c =  254.614501953125\n",
      "loss_c =  254.28640747070312\n",
      "loss_c =  254.04043579101562\n",
      "loss_c =  253.7974853515625\n",
      "loss_c =  253.55502319335938\n",
      "loss_c =  253.31228637695312\n",
      "loss_c =  253.06829833984375\n",
      "loss_c =  252.82249450683594\n",
      "loss_c =  252.57437133789062\n",
      "thetac step =  12.223215341567993 s\n",
      "step =  75\n",
      "loss z =  9771538.0\n",
      "loss z =  9759660.0\n",
      "loss z =  9751901.0\n",
      "loss z =  9746864.0\n",
      "loss z =  9743166.0\n",
      "loss z =  9740122.0\n",
      "loss z =  9737417.0\n",
      "loss z =  9734921.0\n",
      "loss z =  9732563.0\n",
      "loss z =  9730318.0\n",
      "loss z =  9728151.0\n",
      "Epoch  3180: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9726058.0\n",
      "loss z =  9725650.0\n",
      "loss z =  9725244.0\n",
      "loss z =  9724840.0\n",
      "loss z =  9724440.0\n",
      "loss z =  9724039.0\n",
      "loss z =  9723640.0\n",
      "loss z =  9723244.0\n",
      "loss z =  9722460.0\n",
      "loss z =  9722070.0\n",
      "Epoch  3191: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9721682.0\n",
      "loss z =  9721605.0\n",
      "loss z =  9721526.0\n",
      "loss z =  9721450.0\n",
      "loss z =  9721373.0\n",
      "loss z =  9721296.0\n",
      "loss z =  9721219.0\n",
      "loss z =  9721142.0\n",
      "loss z =  9721064.0\n",
      "loss z =  9720989.0\n",
      "loss z =  9720911.0\n",
      "Epoch  3202: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9720835.0\n",
      "loss z =  9720819.0\n",
      "loss z =  9720804.0\n",
      "loss z =  9720789.0\n",
      "loss z =  9720774.0\n",
      "loss z =  9720758.0\n",
      "loss z =  9720728.0\n",
      "loss z =  9720712.0\n",
      "loss z =  9720698.0\n",
      "loss z =  9720682.0\n",
      "Epoch  3213: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  26.286151885986328 s\n",
      "loss_f =  7120303.0\n",
      "loss_f =  7111762.0\n",
      "loss_f =  7107651.0\n",
      "loss_f =  7102330.0\n",
      "loss_f =  7098444.0\n",
      "loss_f =  7095273.0\n",
      "Epoch 14003: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7092945.0\n",
      "loss_f =  7091862.5\n",
      "loss_f =  7090874.0\n",
      "loss_f =  7089991.5\n",
      "loss_f =  7089176.0\n",
      "loss_f =  7088427.0\n",
      "loss_f =  7087703.0\n",
      "loss_f =  7087008.0\n",
      "loss_f =  7086330.0\n",
      "loss_f =  7085672.0\n",
      "loss_f =  7085039.0\n",
      "loss_f =  7084409.5\n",
      "loss_f =  7083786.0\n",
      "loss_f =  7083165.5\n",
      "loss_f =  7082582.5\n",
      "loss_f =  7081984.5\n",
      "loss_f =  7081395.0\n",
      "loss_f =  7080803.0\n",
      "loss_f =  7080228.0\n",
      "loss_f =  7079661.0\n",
      "loss_f =  7079092.0\n",
      "loss_f =  7078531.0\n",
      "loss_f =  7077974.0\n",
      "loss_f =  7077415.0\n",
      "loss_f =  7076871.5\n",
      "loss_f =  7076320.5\n",
      "loss_f =  7075785.0\n",
      "loss_f =  7075252.0\n",
      "loss_f =  7074721.0\n",
      "loss_f =  7074188.0\n",
      "loss_f =  7073668.0\n",
      "loss_f =  7073138.0\n",
      "loss_f =  7072623.5\n",
      "loss_f =  7072114.0\n",
      "thetaf step =  145.5315101146698 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11078.8388671875\n",
      "Epoch  7369: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7385: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7401: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7417: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6407.76513671875\n",
      "Epoch  6592: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6608: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6624: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6640: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30100.2890625\n",
      "Epoch 13412: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13428: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13444: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13460: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.83203125\n",
      "Epoch  8452: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8468: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8484: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8500: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24290.693359375\n",
      "Epoch  9514: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9530: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9546: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9562: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37794.30859375\n",
      "Epoch  6652: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6668: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6684: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6700: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23250.818359375\n",
      "Epoch  7539: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7555: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7571: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7587: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14116.955078125\n",
      "Epoch 16858: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 16874: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 16890: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 16906: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2420.419189453125\n",
      "Epoch  7980: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7996: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8012: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8028: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8139.6220703125\n",
      "Epoch  6489: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6505: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6521: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6537: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19108.59765625\n",
      "Epoch  8022: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8038: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8054: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8070: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10357.427734375\n",
      "Epoch 13010: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13026: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13042: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13058: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9295.486328125\n",
      "Epoch  8253: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8269: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8285: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8301: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8870.59375\n",
      "Epoch  9450: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9466: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9482: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9498: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15623.056640625\n",
      "Epoch  6567: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6583: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6599: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6615: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1814.5977783203125\n",
      "Epoch  6173: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6189: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6205: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6221: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.185487985610962 s\n",
      "loss_c =  256.26202392578125\n",
      "Epoch 74548: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  255.60238647460938\n",
      "loss_c =  255.54898071289062\n",
      "loss_c =  255.4976806640625\n",
      "loss_c =  255.44723510742188\n",
      "loss_c =  255.3970184326172\n",
      "loss_c =  255.34661865234375\n",
      "loss_c =  255.2956085205078\n",
      "loss_c =  255.2439727783203\n",
      "loss_c =  255.1915740966797\n",
      "thetac step =  13.354941368103027 s\n",
      "step =  76\n",
      "loss z =  9748930.0\n",
      "loss z =  9733432.0\n",
      "loss z =  9723774.0\n",
      "loss z =  9717878.0\n",
      "loss z =  9713854.0\n",
      "loss z =  9710772.0\n",
      "loss z =  9708147.0\n",
      "loss z =  9705766.0\n",
      "loss z =  9703545.0\n",
      "loss z =  9701431.0\n",
      "loss z =  9699398.0\n",
      "Epoch  3225: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9697428.0\n",
      "loss z =  9697042.0\n",
      "loss z =  9696655.0\n",
      "loss z =  9696271.0\n",
      "loss z =  9695891.0\n",
      "loss z =  9695507.0\n",
      "loss z =  9695130.0\n",
      "loss z =  9694753.0\n",
      "loss z =  9694009.0\n",
      "loss z =  9693636.0\n",
      "Epoch  3236: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9693269.0\n",
      "loss z =  9693196.0\n",
      "loss z =  9693123.0\n",
      "loss z =  9693048.0\n",
      "loss z =  9692976.0\n",
      "loss z =  9692901.0\n",
      "loss z =  9692827.0\n",
      "loss z =  9692754.0\n",
      "loss z =  9692680.0\n",
      "loss z =  9692609.0\n",
      "loss z =  9692536.0\n",
      "Epoch  3247: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9692464.0\n",
      "loss z =  9692450.0\n",
      "loss z =  9692436.0\n",
      "loss z =  9692421.0\n",
      "loss z =  9692406.0\n",
      "loss z =  9692392.0\n",
      "loss z =  9692363.0\n",
      "loss z =  9692348.0\n",
      "loss z =  9692334.0\n",
      "loss z =  9692319.0\n",
      "Epoch  3258: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  29.220721006393433 s\n",
      "loss_f =  7095393.0\n",
      "loss_f =  7088413.0\n",
      "loss_f =  7085632.0\n",
      "loss_f =  7081160.0\n",
      "loss_f =  7077114.0\n",
      "loss_f =  7074198.0\n",
      "loss_f =  7072086.5\n",
      "Epoch 14205: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7071026.0\n",
      "loss_f =  7070196.0\n",
      "loss_f =  7069426.0\n",
      "loss_f =  7068725.5\n",
      "loss_f =  7068074.0\n",
      "loss_f =  7067465.0\n",
      "loss_f =  7066868.0\n",
      "loss_f =  7066284.0\n",
      "loss_f =  7065716.0\n",
      "loss_f =  7065167.0\n",
      "loss_f =  7064623.0\n",
      "loss_f =  7064091.0\n",
      "loss_f =  7063547.0\n",
      "loss_f =  7063012.0\n",
      "loss_f =  7062482.0\n",
      "loss_f =  7061941.0\n",
      "loss_f =  7061423.5\n",
      "loss_f =  7060910.5\n",
      "loss_f =  7060406.0\n",
      "loss_f =  7059905.5\n",
      "loss_f =  7059414.0\n",
      "loss_f =  7058917.0\n",
      "loss_f =  7058427.5\n",
      "loss_f =  7057953.5\n",
      "loss_f =  7057464.0\n",
      "loss_f =  7056986.0\n",
      "loss_f =  7056511.0\n",
      "loss_f =  7056040.0\n",
      "loss_f =  7055574.0\n",
      "loss_f =  7055109.5\n",
      "loss_f =  7054642.0\n",
      "loss_f =  7054182.5\n",
      "loss_f =  7053717.5\n",
      "thetaf step =  153.1365258693695 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11078.908203125\n",
      "Epoch  7433: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7449: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7465: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7481: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6407.6728515625\n",
      "Epoch  6656: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6672: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6688: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6704: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30100.328125\n",
      "Epoch 13476: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13492: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13508: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13524: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.8203125\n",
      "Epoch  8516: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8532: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8548: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8564: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24290.798828125\n",
      "Epoch  9578: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9594: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9610: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9626: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37794.3515625\n",
      "Epoch  6716: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6732: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6748: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6764: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23250.919921875\n",
      "Epoch  7603: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7619: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7635: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7651: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14117.025390625\n",
      "Epoch 16922: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 16938: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 16954: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 16970: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2420.40869140625\n",
      "Epoch  8044: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8060: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8076: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8092: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8139.85546875\n",
      "Epoch  6553: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6569: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6585: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6601: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19108.6953125\n",
      "Epoch  8086: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8102: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8118: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8134: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10357.3349609375\n",
      "Epoch 13074: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13090: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13106: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13122: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9295.416015625\n",
      "Epoch  8317: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8333: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8349: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8365: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8870.4150390625\n",
      "Epoch  9514: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9530: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9546: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9562: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15622.947265625\n",
      "Epoch  6631: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6647: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6663: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6679: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1814.6126708984375\n",
      "Epoch  6237: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6253: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6269: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6285: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1514863967895508 s\n",
      "loss_c =  256.47344970703125\n",
      "loss_c =  255.6204071044922\n",
      "loss_c =  255.35824584960938\n",
      "loss_c =  255.11831665039062\n",
      "loss_c =  254.87838745117188\n",
      "loss_c =  254.63731384277344\n",
      "loss_c =  254.3944091796875\n",
      "loss_c =  254.1492156982422\n",
      "loss_c =  253.9016571044922\n",
      "loss_c =  253.6512451171875\n",
      "thetac step =  13.139596939086914 s\n",
      "step =  77\n",
      "loss z =  9731237.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss z =  9716994.0\n",
      "loss z =  9708488.0\n",
      "loss z =  9703434.0\n",
      "loss z =  9699978.0\n",
      "loss z =  9697159.0\n",
      "loss z =  9694627.0\n",
      "loss z =  9692255.0\n",
      "loss z =  9689993.0\n",
      "loss z =  9687815.0\n",
      "loss z =  9685710.0\n",
      "Epoch  3270: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9683664.0\n",
      "loss z =  9683263.0\n",
      "loss z =  9682864.0\n",
      "loss z =  9682467.0\n",
      "loss z =  9682075.0\n",
      "loss z =  9681683.0\n",
      "loss z =  9681293.0\n",
      "loss z =  9680905.0\n",
      "loss z =  9680132.0\n",
      "loss z =  9679750.0\n",
      "Epoch  3281: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9679371.0\n",
      "loss z =  9679297.0\n",
      "loss z =  9679219.0\n",
      "loss z =  9679143.0\n",
      "loss z =  9679068.0\n",
      "loss z =  9678992.0\n",
      "loss z =  9678917.0\n",
      "loss z =  9678842.0\n",
      "loss z =  9678766.0\n",
      "loss z =  9678688.0\n",
      "loss z =  9678614.0\n",
      "Epoch  3292: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9678538.0\n",
      "loss z =  9678522.0\n",
      "loss z =  9678508.0\n",
      "loss z =  9678492.0\n",
      "loss z =  9678477.0\n",
      "loss z =  9678462.0\n",
      "loss z =  9678431.0\n",
      "loss z =  9678416.0\n",
      "loss z =  9678402.0\n",
      "loss z =  9678386.0\n",
      "Epoch  3303: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  27.71867799758911 s\n",
      "loss_f =  7084579.0\n",
      "loss_f =  7078533.5\n",
      "loss_f =  7074756.0\n",
      "loss_f =  7069077.5\n",
      "loss_f =  7065367.0\n",
      "loss_f =  7062288.5\n",
      "loss_f =  7059910.5\n",
      "Epoch 14405: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7058760.0\n",
      "loss_f =  7057833.0\n",
      "loss_f =  7057005.0\n",
      "loss_f =  7056261.0\n",
      "loss_f =  7055576.5\n",
      "loss_f =  7054928.0\n",
      "Epoch 14436: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7054490.0\n",
      "loss_f =  7054187.0\n",
      "loss_f =  7053888.5\n",
      "loss_f =  7053592.0\n",
      "loss_f =  7053294.5\n",
      "loss_f =  7053001.5\n",
      "Epoch 14467: reducing learning rate of group 0 to 6.2500e-05.\n",
      "loss_f =  7052775.5\n",
      "loss_f =  7052629.0\n",
      "loss_f =  7052483.0\n",
      "loss_f =  7052335.0\n",
      "loss_f =  7052199.5\n",
      "loss_f =  7052058.5\n",
      "loss_f =  7051913.5\n",
      "loss_f =  7051776.0\n",
      "loss_f =  7051637.5\n",
      "loss_f =  7051493.0\n",
      "loss_f =  7051354.0\n",
      "loss_f =  7051208.5\n",
      "loss_f =  7051066.5\n",
      "loss_f =  7050931.0\n",
      "loss_f =  7050786.0\n",
      "loss_f =  7050646.0\n",
      "loss_f =  7050507.0\n",
      "loss_f =  7050358.0\n",
      "loss_f =  7050221.5\n",
      "loss_f =  7050079.5\n",
      "loss_f =  7049940.0\n",
      "thetaf step =  142.87961053848267 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11078.830078125\n",
      "Epoch  7497: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7513: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7529: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7545: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6407.599609375\n",
      "Epoch  6720: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6736: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6752: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6768: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30100.634765625\n",
      "Epoch 13540: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13556: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13572: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13588: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.8125\n",
      "Epoch  8580: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8596: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8612: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8628: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24290.74609375\n",
      "Epoch  9642: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9658: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9674: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9690: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37794.484375\n",
      "Epoch  6780: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6796: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6812: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6828: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23250.99609375\n",
      "Epoch  7667: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7683: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7699: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7715: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14117.1064453125\n",
      "Epoch 16986: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 17002: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 17018: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 17034: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2420.36376953125\n",
      "Epoch  8108: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8124: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8140: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8156: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8139.93505859375\n",
      "Epoch  6617: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6633: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6649: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6665: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19108.779296875\n",
      "Epoch  8150: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8166: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8198: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10357.19140625\n",
      "Epoch 13138: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13154: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13170: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13186: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9295.3115234375\n",
      "Epoch  8381: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8397: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8413: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8429: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8870.1416015625\n",
      "Epoch  9578: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9594: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9610: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9626: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15622.9453125\n",
      "Epoch  6695: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6711: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6727: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6743: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1814.6512451171875\n",
      "Epoch  6301: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6317: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6333: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6349: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.1476070880889893 s\n",
      "loss_c =  256.4898986816406\n",
      "Epoch 75549: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss_c =  255.87637329101562\n",
      "loss_c =  255.7003631591797\n",
      "loss_c =  255.60305786132812\n",
      "loss_c =  255.53433227539062\n",
      "loss_c =  255.47586059570312\n",
      "loss_c =  255.420654296875\n",
      "loss_c =  255.36624145507812\n",
      "loss_c =  255.3117218017578\n",
      "loss_c =  255.25657653808594\n",
      "thetac step =  12.241143465042114 s\n",
      "step =  78\n",
      "loss z =  9753705.0\n",
      "loss z =  9727095.0\n",
      "loss z =  9709558.0\n",
      "loss z =  9698820.0\n",
      "loss z =  9691862.0\n",
      "loss z =  9686748.0\n",
      "loss z =  9682796.0\n",
      "loss z =  9679511.0\n",
      "loss z =  9676633.0\n",
      "loss z =  9674007.0\n",
      "loss z =  9671555.0\n",
      "Epoch  3315: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9669246.0\n",
      "loss z =  9668802.0\n",
      "loss z =  9668362.0\n",
      "loss z =  9667926.0\n",
      "loss z =  9667496.0\n",
      "loss z =  9667066.0\n",
      "loss z =  9666644.0\n",
      "loss z =  9666222.0\n",
      "loss z =  9665390.0\n",
      "loss z =  9664979.0\n",
      "Epoch  3326: reducing learning rate of group 0 to 4.0000e-06.\n",
      "loss z =  9664568.0\n",
      "loss z =  9664487.0\n",
      "loss z =  9664406.0\n",
      "loss z =  9664322.0\n",
      "loss z =  9664241.0\n",
      "loss z =  9664160.0\n",
      "loss z =  9664078.0\n",
      "loss z =  9663996.0\n",
      "loss z =  9663916.0\n",
      "loss z =  9663835.0\n",
      "loss z =  9663753.0\n",
      "Epoch  3337: reducing learning rate of group 0 to 8.0000e-07.\n",
      "loss z =  9663670.0\n",
      "loss z =  9663654.0\n",
      "loss z =  9663638.0\n",
      "loss z =  9663622.0\n",
      "loss z =  9663606.0\n",
      "loss z =  9663590.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss z =  9663558.0\n",
      "loss z =  9663542.0\n",
      "loss z =  9663526.0\n",
      "loss z =  9663510.0\n",
      "Epoch  3348: reducing learning rate of group 0 to 1.6000e-07.\n",
      "z step =  25.912616968154907 s\n",
      "loss_f =  7073211.0\n",
      "loss_f =  7066216.5\n",
      "Epoch 14583: reducing learning rate of group 0 to 2.5000e-04.\n",
      "loss_f =  7063882.0\n",
      "loss_f =  7060578.5\n",
      "loss_f =  7057621.5\n",
      "loss_f =  7055419.5\n",
      "loss_f =  7053732.0\n",
      "loss_f =  7052319.5\n",
      "Epoch 14614: reducing learning rate of group 0 to 1.2500e-04.\n",
      "loss_f =  7051157.0\n",
      "loss_f =  7050643.0\n",
      "loss_f =  7050175.0\n",
      "loss_f =  7049719.0\n",
      "loss_f =  7049312.0\n",
      "loss_f =  7048926.0\n",
      "loss_f =  7048550.0\n",
      "loss_f =  7048187.5\n",
      "loss_f =  7047838.5\n",
      "loss_f =  7047500.0\n",
      "loss_f =  7047159.5\n",
      "loss_f =  7046842.0\n",
      "loss_f =  7046522.0\n",
      "loss_f =  7046197.0\n",
      "loss_f =  7045893.0\n",
      "loss_f =  7045583.0\n",
      "loss_f =  7045274.5\n",
      "loss_f =  7044974.0\n",
      "loss_f =  7044678.0\n",
      "loss_f =  7044374.0\n",
      "loss_f =  7044084.0\n",
      "loss_f =  7043793.5\n",
      "loss_f =  7043509.0\n",
      "loss_f =  7043218.5\n",
      "loss_f =  7042942.5\n",
      "loss_f =  7042643.0\n",
      "loss_f =  7042362.0\n",
      "loss_f =  7042083.5\n",
      "loss_f =  7041808.0\n",
      "loss_f =  7041533.0\n",
      "loss_f =  7041252.0\n",
      "loss_f =  7040975.0\n",
      "thetaf step =  149.43700790405273 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  11078.8935546875\n",
      "Epoch  7561: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7577: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7593: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7609: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "loss_lambda_c =  6407.50830078125\n",
      "Epoch  6784: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6800: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6816: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6832: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  2\n",
      "loss_lambda_c =  30100.703125\n",
      "Epoch 13604: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13620: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13636: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13652: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  3\n",
      "loss_lambda_c =  43826.8046875\n",
      "Epoch  8644: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8660: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8676: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8692: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  4\n",
      "loss_lambda_c =  24290.98046875\n",
      "Epoch  9706: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9722: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9738: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9754: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  5\n",
      "loss_lambda_c =  37794.515625\n",
      "Epoch  6844: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6860: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6876: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6892: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  6\n",
      "loss_lambda_c =  23251.080078125\n",
      "Epoch  7731: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7747: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7763: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  7779: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  7\n",
      "loss_lambda_c =  14117.162109375\n",
      "Epoch 17050: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 17066: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 17082: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 17098: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  8\n",
      "loss_lambda_c =  2420.380126953125\n",
      "Epoch  8172: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8188: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8204: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8220: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  9\n",
      "loss_lambda_c =  8139.6953125\n",
      "Epoch  6681: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6697: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6713: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6729: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  10\n",
      "loss_lambda_c =  19108.857421875\n",
      "Epoch  8214: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8230: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8246: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8262: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  11\n",
      "loss_lambda_c =  10357.1015625\n",
      "Epoch 13202: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 13218: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 13234: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 13250: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  12\n",
      "loss_lambda_c =  9295.2392578125\n",
      "Epoch  8445: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  8461: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  8477: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  8493: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  13\n",
      "loss_lambda_c =  8869.9619140625\n",
      "Epoch  9642: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  9658: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  9674: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  9690: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  14\n",
      "loss_lambda_c =  15622.8603515625\n",
      "Epoch  6759: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6775: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6791: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6807: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  15\n",
      "loss_lambda_c =  1814.6629638671875\n",
      "Epoch  6365: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6381: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  6397: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch  6413: reducing learning rate of group 0 to 1.0000e-08.\n",
      "lambdac step =  1.2379415035247803 s\n",
      "loss_c =  256.3704833984375\n",
      "loss_c =  255.40994262695312\n",
      "loss_c =  255.13259887695312\n",
      "loss_c =  254.91038513183594\n",
      "loss_c =  254.6886749267578\n",
      "loss_c =  254.4649200439453\n",
      "loss_c =  254.23851013183594\n",
      "loss_c =  254.00909423828125\n",
      "loss_c =  253.77651977539062\n",
      "loss_c =  253.54058837890625\n",
      "thetac step =  14.086328268051147 s\n",
      "step =  79\n",
      "loss z =  9773087.0\n",
      "loss z =  9742630.0\n",
      "loss z =  9721321.0\n",
      "loss z =  9706041.0\n",
      "loss z =  9694073.0\n",
      "loss z =  9685636.0\n",
      "loss z =  9679623.0\n",
      "loss z =  9674921.0\n",
      "loss z =  9671328.0\n",
      "loss z =  9668357.0\n",
      "loss z =  9665762.0\n",
      "Epoch  3360: reducing learning rate of group 0 to 2.0000e-05.\n",
      "loss z =  9663391.0\n",
      "loss z =  9662939.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8ad0025a2f75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_precisions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthetaf_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthetac_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambdac_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/cluster/python/projects/generative_DR_ROM/GenerativeSurrogate.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, n_steps, z_iterations, thetaf_iterations, lambdac_iterations, thetac_iterations, save_iterations, with_precisions)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m#                   + self.data.n_supervised_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0mbatch_samples_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m      \u001b[0;31m# we currently use all z samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_samples_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0mz_iter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'z step'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cluster/python/projects/generative_DR_ROM/GenerativeSurrogate.py\u001b[0m in \u001b[0;36mz_step\u001b[0;34m(self, batch_samples, step)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mloss_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_z_delta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mloss_z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss z = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/genDRROM/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/genDRROM/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(n_steps=10000, z_iterations=50, with_precisions=True, thetaf_iterations=200, thetac_iterations=500, lambdac_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.tauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.z_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gs.GenerativeSurrogate()\n",
    "# model.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = {n for n in range(0, 4)}\n",
    "testData = dta.StokesData(unsupervised_samples=test_samples)\n",
    "testData.read_data()\n",
    "# trainingData.plotMicrostruct(1)\n",
    "testData.reshape_microstructure_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uf_pred, Z, grad_norm = model.predict(testData, max_iterations=3000, optimizer='SGD', lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(grad_norm)\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "ax = plt.gca()\n",
    "ax.set_ylim(1e1, 1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('iteration')\n",
    "plt.ylabel('gradient norm')\n",
    "plt.title('Gradient norm w.r.t. z in prediction inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grad_norm[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.z_mean[:4, :].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uf_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.data.P[:4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(n_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.outer(np.linspace(0, 1, 129), np.ones(129))\n",
    "y = x.copy().T # transpose\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(x, y, np.reshape(uf_pred[3], (129, 129)),cmap='viridis', edgecolor='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = np.reshape(model.data.P[3, :], (129, 129))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.plot_surface(x, y, pp.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_lambdac = model.pcNet(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_lambdac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_lambdac2 = model.pcNet(model.z_mean[:4, :])\n",
    "lg_lambdac2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.log_lambdac_mean_tensor[:4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uf_pred2 = model.rom_autograd(torch.exp(lg_lambdac2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.outer(np.linspace(0, 1, 129), np.ones(129))\n",
    "y = x.copy().T # transpose\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(x, y, np.reshape(uf_pred2.detach().numpy(), (129, 129)),cmap='viridis', edgecolor='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.log_lambdac_mean_tensor[:4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "figManager = plt.get_current_fig_manager()\n",
    "figManager.window.showMaximized()\n",
    "\n",
    "x = np.outer(np.linspace(0, 1, 129), np.ones(129))\n",
    "y = x.copy().T # transpose\n",
    "\n",
    "ax = []\n",
    "for i in range(4):\n",
    "    ax.append(fig.add_subplot(2, 4, i + 1, projection='3d'))\n",
    "    ax[i].plot_surface(x, y, np.reshape(uf_pred[i], (129, 129)), edgecolor='none')\n",
    "    ax[i].plot_surface(x, y, np.reshape(model.data.P[i, :], (129, 129)).detach().numpy(),\n",
    "                       edgecolor='none', cmap='inferno')\n",
    "    ax[i].set_position([0.025 + .2375*i, 0.55, 0.22, 0.4])\n",
    "    ax[i].elev = 10\n",
    "    ax[i].set_title('pressure field prediction')\n",
    "    \n",
    "xx = np.array([i for i in range(dim_z)])\n",
    "width = 0.42  # the width of the bars\n",
    "for i in range(4):\n",
    "    ax.append(fig.add_subplot(2, 4, i + 5))\n",
    "    ax[i + 4].bar(xx-width/2, Z[i, :].detach().numpy(), width, label='prediction')\n",
    "    ax[i + 4].bar(xx + width/2, model.z_mean[i, :].detach().numpy(), width, label='training')\n",
    "    ax[i + 4].set_position([0.05 + .235*i, 0.08, 0.20, 0.4])\n",
    "    ax[i + 4].set_title('location of z distribution')\n",
    "    ax[i + 4].set_xlabel('component')\n",
    "    ax[i + 4].set_ylabel('z')\n",
    "    ax[i + 4].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_lambdac2 = model.pcNet(model.z_mean[:4, :])\n",
    "# lg_lambdac2 = model.log_lambdac_mean_tensor[:4, :]\n",
    "uf_pred2 = []\n",
    "fig = plt.figure()\n",
    "figManager = plt.get_current_fig_manager()\n",
    "figManager.window.showMaximized()\n",
    "\n",
    "x = np.outer(np.linspace(0, 1, 129), np.ones(129))\n",
    "y = x.copy().T # transpose\n",
    "ax = []\n",
    "for i in range(4):\n",
    "    uf_pred2.append(model.rom_autograd(torch.exp(lg_lambdac2[i])))\n",
    "    ax.append(fig.add_subplot(1, 4, i + 1, projection='3d'))\n",
    "    ax[i].plot_surface(x, y, np.reshape(uf_pred2[i].detach().numpy(), (129, 129)), edgecolor='none')\n",
    "    ax[i].plot_surface(x, y, np.reshape(model.data.P[i, :], (129, 129)).detach().numpy(),\n",
    "                       edgecolor='none', cmap='inferno')\n",
    "    ax[i].set_position([0.025 + .2375*i, 0.55, 0.22, 0.4])\n",
    "    ax[i].elev = 10\n",
    "    ax[i].set_title('pressure field prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "ax.plot_surface(x, y, np.reshape(uf_pred2[i].detach().numpy(), (129, 129)), edgecolor='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax[0].get_position()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax[0].set_position([0.02, 0.3, 0.2, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax[0].elev = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax[0].get_zticks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax[5].set_xlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = torch.nn.Linear(5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.training_iterations = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import constraints\n",
    "from torch.distributions.exp_family import ExponentialFamily\n",
    "from torch.distributions.utils import _standard_normal, broadcast_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = torch.distributions.Normal(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.stddev = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_emp_dist(x):\n",
    "    sigma_e = 2*torch.ones(1)\n",
    "    mu_e = -3*torch.ones(1)\n",
    "    return .5*(1/sigma_e**2)*(x - mu_e)**2\n",
    "\n",
    "def log_emp_dist_grad(x):\n",
    "    sigma_e = 2*torch.ones(1)\n",
    "    mu_e = -3*torch.ones(1)\n",
    "    return (x - mu_e)/sigma_e**2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VI import variationalinference as vi\n",
    "svi = vi.DiagGaussianSVI(log_emp_dist, log_emp_dist_grad, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    loss = elbo.autograd_elbo(elbo.vi_mean, elbo.vi_log_std)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sigma == ', elbo.vi_std)\n",
    "print('mu == ', elbo.vi_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi.vi_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_sigma.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(5) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['mean'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 5}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = torch.distributions.multivariate_normal.MultivariateNormal(torch.zeros(3, 4, requires_grad=True), covariance_matrix=torch.eye(4).unsqueeze(0).repeat(3, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(3, requires_grad=True)\n",
    "b = torch.ones(4, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultivariateNormal(loc: torch.Size([3]), covariance_matrix: torch.Size([3, 3]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultivariateNormal(loc: torch.Size([3, 3]), covariance_matrix: torch.Size([3, 3, 3]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.expand([3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_e = dist.expand([4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_e.covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = torch.eye(2)\n",
    "L[0, 1] = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = L.unsqueeze(0).repeat(3, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.2500, 0.5000],\n",
       "         [0.5000, 1.0000]],\n",
       "\n",
       "        [[1.2500, 0.5000],\n",
       "         [0.5000, 1.0000]],\n",
       "\n",
       "        [[1.2500, 0.5000],\n",
       "         [0.5000, 1.0000]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L @ torch.transpose(L, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7497,  0.3456],\n",
       "        [ 1.9351,  1.3505],\n",
       "        [ 1.1620, -0.4637]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = torch.randn(3,2)\n",
    "eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9225],\n",
       "         [ 0.3456]],\n",
       "\n",
       "        [[ 2.6103],\n",
       "         [ 1.3505]],\n",
       "\n",
       "        [[ 0.9302],\n",
       "         [-0.4637]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(L, eps.unsqueeze(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 1])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps.unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (generative_DR_ROM)",
   "language": "python",
   "name": "pycharm-712517ba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
