{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/constantin/anaconda3/envs/genDRROM/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/constantin/anaconda3/envs/genDRROM/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/constantin/anaconda3/envs/genDRROM/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/constantin/anaconda3/envs/genDRROM/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/constantin/anaconda3/envs/genDRROM/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/constantin/anaconda3/envs/genDRROM/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from poisson_fem import PoissonFEM\n",
    "import ROM\n",
    "import GenerativeSurrogate as gs\n",
    "import Data as dta\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "import scipy.sparse.linalg as lg\n",
    "import time\n",
    "import petsc4py\n",
    "import sys\n",
    "petsc4py.init(sys.argv)\n",
    "from petsc4py import PETSc\n",
    "import torch\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Some parameters\n",
    "lin_dim_rom = 4                      # Linear number of rom elements\n",
    "a = np.array([1, 1, 0])              # Boundary condition function coefficients\n",
    "dtype = torch.float                  # Tensor data type\n",
    "supervised_samples = {n for n in range(16)}\n",
    "unsupervised_samples = {n for n in range(16, 16)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Define mesh and boundary conditions\n",
    "mesh = PoissonFEM.RectangularMesh(np.ones(lin_dim_rom)/lin_dim_rom)\n",
    "# mesh.plot()\n",
    "\n",
    "def origin(x):\n",
    "    return np.abs(x[0]) < np.finfo(float).eps and np.abs(x[1]) < np.finfo(float).eps\n",
    "\n",
    "def ess_boundary_fun(x):\n",
    "    return 0.0\n",
    "mesh.set_essential_boundary(origin, ess_boundary_fun)\n",
    "\n",
    "def domain_boundary(x):\n",
    "    # unit square\n",
    "    return np.abs(x[0]) < np.finfo(float).eps or np.abs(x[1]) < np.finfo(float).eps or \\\n",
    "            np.abs(x[0]) > 1.0 - np.finfo(float).eps or np.abs(x[1]) > 1.0 - np.finfo(float).eps\n",
    "mesh.set_natural_boundary(domain_boundary)\n",
    "\n",
    "def flux(x):\n",
    "    q = np.array([a[0] + a[2]*x[1], a[1] + a[2]*x[0]])\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify right hand side and stiffness matrix\n",
    "rhs = PoissonFEM.RightHandSide(mesh)\n",
    "rhs.set_natural_rhs(mesh, flux)\n",
    "K = PoissonFEM.StiffnessMatrix(mesh)\n",
    "rhs.set_rhs_stencil(mesh, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = dta.StokesData(supervised_samples, unsupervised_samples)\n",
    "trainingData.read_data()\n",
    "# trainingData.plotMicrostruct(1)\n",
    "trainingData.reshape_microstructure_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define rom\n",
    "rom = ROM.ROM(mesh, K, rhs, trainingData.output_resolution**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gs.GenerativeSurrogate(rom, trainingData)\n",
    "# model = gs.GenerativeSurrogate()\n",
    "# model.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample =  0\n",
      "loss_lambda_c =  45945471369216.0\n",
      "loss_lambda_c =  448313819136.0\n",
      "loss_lambda_c =  378070237184.0\n",
      "loss_lambda_c =  361510764544.0\n",
      "loss_lambda_c =  351929892864.0\n",
      "loss_lambda_c =  345759711232.0\n",
      "loss_lambda_c =  341650309120.0\n",
      "loss_lambda_c =  338662195200.0\n",
      "loss_lambda_c =  336362274816.0\n",
      "loss_lambda_c =  334578188288.0\n",
      "loss_lambda_c =  333210025984.0\n",
      "loss_lambda_c =  332175310848.0\n",
      "Epoch  4512: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  4529: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  4545: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4561: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample =  1\n",
      "loss_lambda_c =  55534564671488.0\n",
      "Epoch  4577: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  4593: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  4609: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4625: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample =  2\n",
      "loss_lambda_c =  821097486352384.0\n",
      "Epoch  4641: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  4657: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  4673: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4689: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample =  3\n",
      "loss_lambda_c =  959812447764480.0\n",
      "Epoch  4705: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  4721: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  4737: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4753: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample =  4\n",
      "loss_lambda_c =  865880170823680.0\n",
      "Epoch  4769: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  4785: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  4801: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4817: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample =  5\n",
      "loss_lambda_c =  1605844114341888.0\n",
      "Epoch  4833: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  4849: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  4865: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4881: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample =  6\n",
      "loss_lambda_c =  487715816603648.0\n",
      "Epoch  4897: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  4913: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  4929: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  4945: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample =  7\n",
      "loss_lambda_c =  251341922369536.0\n",
      "Epoch  4961: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  4977: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  4993: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5009: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample =  8\n",
      "loss_lambda_c =  65542295650304.0\n",
      "Epoch  5025: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  5041: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  5057: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5073: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample =  9\n",
      "loss_lambda_c =  215594188144640.0\n",
      "Epoch  5089: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  5105: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  5121: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5137: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample =  10\n",
      "loss_lambda_c =  394107004387328.0\n",
      "Epoch  5153: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  5169: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  5185: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5201: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample =  11\n",
      "loss_lambda_c =  201864972861440.0\n",
      "Epoch  5217: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  5233: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  5249: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5265: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample =  12\n",
      "loss_lambda_c =  107302375915520.0\n",
      "Epoch  5281: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  5297: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  5313: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5329: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample =  13\n",
      "loss_lambda_c =  102988332924928.0\n",
      "Epoch  5345: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  5361: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  5377: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5393: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample =  14\n",
      "loss_lambda_c =  425835571970048.0\n",
      "Epoch  5409: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  5425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  5441: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5457: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample =  15\n",
      "loss_lambda_c =  19498428006400.0\n",
      "Epoch  5473: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  5489: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  5505: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5521: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    }
   ],
   "source": [
    "# optimize lambdac first\n",
    "lambdac_iterations = 3e4\n",
    "for k in range(model.data.n_supervised_samples):\n",
    "    print('sample = ', k)\n",
    "    lr_init = 1e-2\n",
    "    model.lambdacOpt.param_groups[0]['lr'] = lr_init\n",
    "    lambdac_iter = 0\n",
    "    while lambdac_iter < lambdac_iterations and model.lambdacOpt.param_groups[0]['lr'] > 1e-4 * lr_init:\n",
    "        model.lambdac_step(k, lambdac_iter)\n",
    "        lambdac_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  0\n",
      "loss z =  736561.625\n",
      "loss z =  736539.5\n",
      "loss z =  736499.25\n",
      "loss z =  736430.9375\n",
      "loss z =  736333.1875\n",
      "loss z =  736203.0625\n",
      "loss z =  736035.25\n",
      "loss z =  735823.5625\n",
      "loss z =  735562.25\n",
      "loss z =  735245.4375\n",
      "loss z =  734869.8125\n",
      "loss z =  734435.9375\n",
      "loss z =  733950.3125\n",
      "loss z =  733424.5625\n",
      "loss z =  732875.375\n",
      "loss z =  732322.125\n",
      "loss z =  731784.625\n",
      "loss z =  731278.5625\n",
      "loss z =  730820.25\n",
      "loss z =  730061.25\n",
      "loss z =  729761.25\n",
      "loss z =  729510.0625\n",
      "loss z =  729303.5\n",
      "loss z =  729135.5625\n",
      "loss z =  728999.875\n",
      "loss z =  728891.9375\n",
      "loss z =  728804.6875\n",
      "loss z =  728734.625\n",
      "loss z =  728677.0625\n",
      "loss z =  728631.3125\n",
      "loss z =  728596.0\n",
      "loss z =  728566.0625\n",
      "loss z =  728537.1875\n",
      "loss z =  728515.75\n",
      "loss z =  728494.0625\n",
      "loss z =  728478.3125\n",
      "loss z =  728470.875\n",
      "loss z =  728456.875\n",
      "loss z =  728439.75\n",
      "loss z =  728431.5\n",
      "loss z =  728424.0\n",
      "loss z =  728422.1875\n",
      "loss z =  728421.875\n",
      "loss z =  728419.0625\n",
      "loss z =  728418.5\n",
      "loss z =  728416.8125\n",
      "loss z =  728419.3125\n",
      "loss z =  728418.875\n",
      "loss z =  728413.125\n",
      "loss z =  728419.375\n",
      "Epoch    52: reducing learning rate of group 0 to 2.0000e-02.\n",
      "loss z =  728418.125\n",
      "loss z =  728410.5625\n",
      "loss z =  728404.6875\n",
      "loss z =  728402.5\n",
      "loss z =  728401.9375\n",
      "loss z =  728401.6875\n",
      "loss z =  728401.6875\n",
      "loss z =  728401.625\n",
      "loss z =  728401.4375\n",
      "loss z =  728401.375\n",
      "Epoch    63: reducing learning rate of group 0 to 4.0000e-03.\n",
      "loss z =  728401.25\n",
      "loss z =  728400.5625\n",
      "loss z =  728400.3125\n",
      "loss z =  728399.9375\n",
      "loss z =  728400.0\n",
      "loss z =  728399.9375\n",
      "loss z =  728399.6875\n",
      "loss z =  728399.75\n",
      "loss z =  728399.8125\n",
      "loss z =  728399.75\n",
      "loss z =  728399.9375\n",
      "Epoch    74: reducing learning rate of group 0 to 8.0000e-04.\n",
      "loss z =  728399.6875\n",
      "loss z =  728399.6875\n",
      "loss z =  728399.5\n",
      "loss z =  728399.5\n",
      "loss z =  728399.6875\n",
      "loss z =  728399.5625\n",
      "loss z =  728399.5625\n",
      "loss z =  728399.5625\n",
      "loss z =  728399.625\n",
      "loss z =  728399.6875\n",
      "Epoch    85: reducing learning rate of group 0 to 1.6000e-04.\n",
      "z step =  53.1717746257782 s\n",
      "loss_f =  727092.25\n",
      "loss_f =  531484.0\n",
      "loss_f =  433427.65625\n",
      "loss_f =  349281.625\n",
      "loss_f =  278382.71875\n",
      "loss_f =  214469.90625\n",
      "loss_f =  161303.453125\n",
      "loss_f =  117527.5\n",
      "loss_f =  81813.8203125\n",
      "loss_f =  54112.8046875\n",
      "loss_f =  33185.81640625\n",
      "loss_f =  18635.078125\n",
      "loss_f =  9891.90234375\n",
      "loss_f =  5790.1748046875\n",
      "loss_f =  3810.390625\n",
      "loss_f =  2716.81005859375\n",
      "loss_f =  2071.3212890625\n",
      "loss_f =  1677.91357421875\n",
      "loss_f =  1418.309814453125\n",
      "loss_f =  1234.626708984375\n",
      "thetaf step =  96.28070282936096 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  331913756672.0\n",
      "sample ==  1\n",
      "loss_lambda_c =  55527941865472.0\n",
      "Epoch  5583: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  5599: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  5615: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  2\n",
      "loss_lambda_c =  821073998249984.0\n",
      "Epoch  5631: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  5647: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  5663: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  3\n",
      "loss_lambda_c =  959781443469312.0\n",
      "Epoch  5679: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  5695: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  5711: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  4\n",
      "loss_lambda_c =  865856951156736.0\n",
      "Epoch  5727: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  5743: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  5759: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  5\n",
      "loss_lambda_c =  1605815123312640.0\n",
      "Epoch  5775: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  5791: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  5807: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  6\n",
      "loss_lambda_c =  487695348400128.0\n",
      "Epoch  5823: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  5839: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  5855: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  5871: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample ==  7\n",
      "loss_lambda_c =  251323769421824.0\n",
      "Epoch  5887: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  5903: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  5919: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  8\n",
      "loss_lambda_c =  65532082520064.0\n",
      "Epoch  5935: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  5951: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  5967: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  9\n",
      "loss_lambda_c =  215580145614848.0\n",
      "Epoch  5983: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  5999: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  6015: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  10\n",
      "loss_lambda_c =  394091200249856.0\n",
      "Epoch  6031: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  6047: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  6063: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  11\n",
      "loss_lambda_c =  201847725883392.0\n",
      "Epoch  6079: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  6095: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  6111: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  12\n",
      "loss_lambda_c =  107291990818816.0\n",
      "Epoch  6127: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  6143: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  6159: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  13\n",
      "loss_lambda_c =  102980607016960.0\n",
      "Epoch  6175: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  6191: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  6207: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  14\n",
      "loss_lambda_c =  425820506030080.0\n",
      "Epoch  6223: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  6239: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  6255: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6271: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample ==  15\n",
      "loss_lambda_c =  19494965608448.0\n",
      "Epoch  6287: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  6303: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  6319: reducing learning rate of group 0 to 1.0000e-05.\n",
      "lambdac step =  1.0854299068450928 s\n",
      "loss_c =  3150.859375\n",
      "loss_c =  2270.116943359375\n",
      "loss_c =  2229.62353515625\n",
      "loss_c =  2178.94140625\n",
      "loss_c =  2121.54833984375\n",
      "loss_c =  2059.838623046875\n",
      "loss_c =  1995.5933837890625\n",
      "loss_c =  1930.1817626953125\n",
      "loss_c =  1864.669189453125\n",
      "loss_c =  1799.89306640625\n",
      "loss_c =  1736.51611328125\n",
      "loss_c =  1675.06298828125\n",
      "loss_c =  1615.946044921875\n",
      "loss_c =  1559.481689453125\n",
      "loss_c =  1505.900634765625\n",
      "loss_c =  1455.3548583984375\n",
      "loss_c =  1407.9227294921875\n",
      "loss_c =  1363.615478515625\n",
      "loss_c =  1322.3831787109375\n",
      "loss_c =  1284.123046875\n",
      "loss_c =  1248.687744140625\n",
      "loss_c =  1215.8934326171875\n",
      "loss_c =  1185.5302734375\n",
      "loss_c =  1157.37158203125\n",
      "loss_c =  1131.18115234375\n",
      "loss_c =  1106.7216796875\n",
      "loss_c =  1083.76220703125\n",
      "loss_c =  1062.0821533203125\n",
      "loss_c =  1041.477294921875\n",
      "loss_c =  1021.7626953125\n",
      "loss_c =  1002.7742919921875\n",
      "loss_c =  984.370849609375\n",
      "loss_c =  966.4332275390625\n",
      "loss_c =  948.8648071289062\n",
      "loss_c =  931.5889892578125\n",
      "loss_c =  914.5487060546875\n",
      "loss_c =  897.7031860351562\n",
      "loss_c =  881.026611328125\n",
      "loss_c =  864.5056762695312\n",
      "loss_c =  848.1370849609375\n",
      "loss_c =  831.92578125\n",
      "loss_c =  815.88330078125\n",
      "loss_c =  800.0258178710938\n",
      "loss_c =  784.373046875\n",
      "loss_c =  768.94677734375\n",
      "loss_c =  753.7699584960938\n",
      "loss_c =  738.8662109375\n",
      "loss_c =  724.258544921875\n",
      "loss_c =  709.9691162109375\n",
      "loss_c =  696.0183715820312\n",
      "loss_c =  682.425048828125\n",
      "loss_c =  669.2052001953125\n",
      "loss_c =  656.3722534179688\n",
      "loss_c =  643.9364624023438\n",
      "loss_c =  631.9046630859375\n",
      "loss_c =  620.2800903320312\n",
      "loss_c =  609.0618896484375\n",
      "loss_c =  598.2457275390625\n",
      "loss_c =  587.8231811523438\n",
      "loss_c =  577.7818603515625\n",
      "loss_c =  568.105712890625\n",
      "loss_c =  558.7755126953125\n",
      "loss_c =  549.768310546875\n",
      "loss_c =  541.0589599609375\n",
      "loss_c =  532.6197509765625\n",
      "loss_c =  524.4210815429688\n",
      "loss_c =  516.4326171875\n",
      "loss_c =  508.62274169921875\n",
      "loss_c =  500.96014404296875\n",
      "loss_c =  493.4139404296875\n",
      "loss_c =  485.9541931152344\n",
      "loss_c =  478.55218505859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_c =  471.18133544921875\n",
      "loss_c =  463.81695556640625\n",
      "loss_c =  456.4369201660156\n",
      "loss_c =  449.02142333984375\n",
      "loss_c =  441.5534362792969\n",
      "loss_c =  434.01849365234375\n",
      "loss_c =  426.4046325683594\n",
      "loss_c =  418.7023620605469\n",
      "loss_c =  410.9044189453125\n",
      "loss_c =  403.005859375\n",
      "loss_c =  395.0033874511719\n",
      "loss_c =  386.8956604003906\n",
      "loss_c =  378.68280029296875\n",
      "loss_c =  370.366455078125\n",
      "loss_c =  361.9493103027344\n",
      "loss_c =  353.4353942871094\n",
      "loss_c =  344.82952880859375\n",
      "loss_c =  336.13775634765625\n",
      "loss_c =  327.36669921875\n",
      "loss_c =  318.52410888671875\n",
      "loss_c =  309.6183776855469\n",
      "loss_c =  300.6587829589844\n",
      "loss_c =  291.65533447265625\n",
      "loss_c =  282.618896484375\n",
      "loss_c =  273.5609436035156\n",
      "loss_c =  264.493896484375\n",
      "loss_c =  255.43081665039062\n",
      "loss_c =  246.38552856445312\n",
      "thetac step =  73.9135468006134 s\n",
      "step =  1\n",
      "loss z =  33629.265625\n",
      "loss z =  42514.109375\n",
      "loss z =  21288.640625\n",
      "loss z =  13345.576171875\n",
      "loss z =  9678.931640625\n",
      "loss z =  7407.8037109375\n",
      "loss z =  6060.775390625\n",
      "loss z =  5103.45947265625\n",
      "loss z =  4376.7177734375\n",
      "loss z =  3769.373046875\n",
      "loss z =  3206.869873046875\n",
      "Epoch    97: reducing learning rate of group 0 to 2.0000e-02.\n",
      "loss z =  2652.804931640625\n",
      "loss z =  2530.474609375\n",
      "loss z =  2382.68408203125\n",
      "loss z =  2209.218017578125\n",
      "loss z =  1999.127685546875\n",
      "loss z =  1758.19873046875\n",
      "loss z =  1496.68310546875\n",
      "loss z =  1224.964111328125\n",
      "loss z =  726.0309448242188\n",
      "loss z =  525.0074462890625\n",
      "loss z =  370.1664123535156\n",
      "loss z =  268.8353576660156\n",
      "loss z =  196.70037841796875\n",
      "loss z =  152.60438537597656\n",
      "loss z =  122.22233581542969\n",
      "loss z =  99.48097229003906\n",
      "loss z =  84.78308868408203\n",
      "loss z =  79.95330047607422\n",
      "loss z =  73.11138153076172\n",
      "loss z =  69.0225601196289\n",
      "loss z =  60.49577331542969\n",
      "loss z =  61.61212921142578\n",
      "loss z =  59.0597038269043\n",
      "loss z =  54.361175537109375\n",
      "loss z =  55.156646728515625\n",
      "loss z =  51.75710678100586\n",
      "loss z =  45.4964714050293\n",
      "loss z =  44.02842712402344\n",
      "loss z =  50.532432556152344\n",
      "loss z =  45.16954803466797\n",
      "loss z =  46.816993713378906\n",
      "loss z =  47.53571701049805\n",
      "loss z =  41.84967041015625\n",
      "loss z =  41.573814392089844\n",
      "loss z =  40.809452056884766\n",
      "loss z =  43.799072265625\n",
      "loss z =  37.671260833740234\n",
      "loss z =  40.11798858642578\n",
      "loss z =  38.524169921875\n",
      "loss z =  38.48176574707031\n",
      "loss z =  42.211585998535156\n",
      "loss z =  46.26186752319336\n",
      "loss z =  36.442466735839844\n",
      "loss z =  39.88137435913086\n",
      "loss z =  37.191383361816406\n",
      "loss z =  34.09422302246094\n",
      "loss z =  43.47216033935547\n",
      "loss z =  37.17716979980469\n",
      "loss z =  35.59030532836914\n",
      "loss z =  35.024593353271484\n",
      "loss z =  42.02311706542969\n",
      "loss z =  37.959922790527344\n",
      "loss z =  35.83696365356445\n",
      "loss z =  36.098506927490234\n",
      "loss z =  37.426700592041016\n",
      "loss z =  34.62771224975586\n",
      "Epoch   156: reducing learning rate of group 0 to 4.0000e-03.\n",
      "loss z =  38.406944274902344\n",
      "loss z =  28.611181259155273\n",
      "loss z =  22.475536346435547\n",
      "loss z =  21.689050674438477\n",
      "loss z =  21.590171813964844\n",
      "loss z =  21.432647705078125\n",
      "loss z =  21.36775779724121\n",
      "loss z =  21.4161376953125\n",
      "loss z =  21.542194366455078\n",
      "loss z =  21.78353500366211\n",
      "loss z =  21.526344299316406\n",
      "loss z =  21.17544937133789\n",
      "loss z =  20.99753189086914\n",
      "loss z =  20.99075698852539\n",
      "loss z =  21.245119094848633\n",
      "loss z =  21.208637237548828\n",
      "loss z =  20.837535858154297\n",
      "loss z =  21.097776412963867\n",
      "loss z =  20.787229537963867\n",
      "loss z =  20.64706039428711\n",
      "loss z =  20.963397979736328\n",
      "loss z =  20.684391021728516\n",
      "loss z =  20.743072509765625\n",
      "loss z =  20.59882354736328\n",
      "loss z =  20.41210174560547\n",
      "loss z =  20.60603904724121\n",
      "loss z =  20.66651725769043\n",
      "loss z =  20.356536865234375\n",
      "loss z =  20.265514373779297\n",
      "loss z =  20.237468719482422\n",
      "loss z =  20.555898666381836\n",
      "loss z =  20.20064926147461\n",
      "loss z =  20.328758239746094\n",
      "loss z =  20.158920288085938\n",
      "loss z =  19.906274795532227\n",
      "loss z =  20.1077880859375\n",
      "loss z =  20.191747665405273\n",
      "loss z =  20.06173324584961\n",
      "loss z =  20.027305603027344\n",
      "loss z =  20.035438537597656\n",
      "loss z =  19.944236755371094\n",
      "loss z =  20.024442672729492\n",
      "loss z =  19.90775489807129\n",
      "loss z =  19.864517211914062\n",
      "loss z =  19.804916381835938\n",
      "loss z =  19.70798683166504\n",
      "loss z =  19.842477798461914\n",
      "loss z =  19.84954833984375\n",
      "loss z =  19.610118865966797\n",
      "loss z =  19.579696655273438\n",
      "loss z =  19.87519073486328\n",
      "loss z =  19.717830657958984\n",
      "loss z =  19.47050666809082\n",
      "loss z =  19.60076141357422\n",
      "loss z =  19.454225540161133\n",
      "loss z =  19.414405822753906\n",
      "loss z =  19.983333587646484\n",
      "loss z =  19.4182186126709\n",
      "loss z =  19.644800186157227\n",
      "loss z =  19.431957244873047\n",
      "loss z =  19.18661117553711\n",
      "loss z =  19.618629455566406\n",
      "loss z =  19.500606536865234\n",
      "loss z =  19.27948760986328\n",
      "loss z =  19.564537048339844\n",
      "loss z =  19.176908493041992\n",
      "loss z =  19.343381881713867\n",
      "loss z =  19.659664154052734\n",
      "loss z =  19.36952018737793\n",
      "loss z =  19.37102508544922\n",
      "loss z =  19.00372314453125\n",
      "loss z =  19.305387496948242\n",
      "loss z =  19.25747299194336\n",
      "loss z =  19.07124137878418\n",
      "loss z =  19.39967918395996\n",
      "loss z =  19.252307891845703\n",
      "loss z =  19.006397247314453\n",
      "loss z =  18.954509735107422\n",
      "loss z =  19.521724700927734\n",
      "loss z =  19.28115463256836\n",
      "loss z =  19.213035583496094\n",
      "loss z =  19.000934600830078\n",
      "loss z =  19.182815551757812\n",
      "loss z =  19.043254852294922\n",
      "loss z =  19.02744483947754\n",
      "loss z =  18.975048065185547\n",
      "loss z =  19.04277801513672\n",
      "loss z =  19.02938461303711\n",
      "Epoch   249: reducing learning rate of group 0 to 8.0000e-04.\n",
      "loss z =  18.860870361328125\n",
      "loss z =  18.620365142822266\n",
      "loss z =  18.45760154724121\n",
      "loss z =  18.42593765258789\n",
      "loss z =  18.416065216064453\n",
      "loss z =  18.40833282470703\n",
      "loss z =  18.397171020507812\n",
      "loss z =  18.402236938476562\n",
      "loss z =  18.391775131225586\n",
      "loss z =  18.390926361083984\n",
      "loss z =  18.370800018310547\n",
      "loss z =  18.36039924621582\n",
      "loss z =  18.344608306884766\n",
      "loss z =  18.336902618408203\n",
      "loss z =  18.330219268798828\n",
      "loss z =  18.339038848876953\n",
      "loss z =  18.30120849609375\n",
      "loss z =  18.29434585571289\n",
      "loss z =  18.283397674560547\n",
      "loss z =  18.278078079223633\n",
      "loss z =  18.281513214111328\n",
      "loss z =  18.25582504272461\n",
      "loss z =  18.246295928955078\n",
      "loss z =  18.243507385253906\n",
      "loss z =  18.23343276977539\n",
      "loss z =  18.22631072998047\n",
      "loss z =  18.223413467407227\n",
      "loss z =  18.217510223388672\n",
      "loss z =  18.196914672851562\n",
      "loss z =  18.186847686767578\n",
      "loss z =  18.184036254882812\n",
      "loss z =  18.199052810668945\n",
      "loss z =  18.181461334228516\n",
      "loss z =  18.15941047668457\n",
      "loss z =  18.153030395507812\n",
      "z step =  124.1117296218872 s\n",
      "loss_f =  4.489161491394043\n",
      "loss_f =  148.67796325683594\n",
      "loss_f =  363.48583984375\n",
      "Epoch   111: reducing learning rate of group 0 to 1.6000e-04.\n",
      "loss_f =  235.91427612304688\n",
      "loss_f =  131.1090087890625\n",
      "Epoch   122: reducing learning rate of group 0 to 3.2000e-05.\n",
      "loss_f =  82.90570068359375\n",
      "loss_f =  58.640541076660156\n",
      "Epoch   133: reducing learning rate of group 0 to 6.4000e-06.\n",
      "loss_f =  44.62456512451172\n",
      "loss_f =  35.83357238769531\n",
      "Epoch   144: reducing learning rate of group 0 to 1.2800e-06.\n",
      "loss_f =  29.926523208618164\n",
      "loss_f =  25.797054290771484\n",
      "loss_f =  22.811725616455078\n",
      "Epoch   155: reducing learning rate of group 0 to 2.5600e-07.\n",
      "loss_f =  20.57029914855957\n",
      "loss_f =  18.82905387878418\n",
      "Epoch   166: reducing learning rate of group 0 to 5.1200e-08.\n",
      "loss_f =  17.432130813598633\n",
      "loss_f =  16.282791137695312\n",
      "Epoch   177: reducing learning rate of group 0 to 1.0240e-08.\n",
      "loss_f =  15.31525993347168\n",
      "loss_f =  14.485128402709961\n",
      "loss_f =  13.761775970458984\n",
      "loss_f =  13.123693466186523\n",
      "thetaf step =  91.53576731681824 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  331757617152.0\n",
      "sample ==  1\n",
      "loss_lambda_c =  55516495609856.0\n",
      "Epoch  6386: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  6402: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  6418: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  2\n",
      "loss_lambda_c =  821043463716864.0\n",
      "Epoch  6434: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  6450: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  6466: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  3\n",
      "loss_lambda_c =  959752116895744.0\n",
      "Epoch  6482: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  6498: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  6514: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  4\n",
      "loss_lambda_c =  865841985880064.0\n",
      "Epoch  6530: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  6546: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  6562: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  5\n",
      "loss_lambda_c =  1605803446370304.0\n",
      "Epoch  6578: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  6594: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6610: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  6\n",
      "loss_lambda_c =  487681154875392.0\n",
      "Epoch  6626: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  6642: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  6658: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  7\n",
      "loss_lambda_c =  251274276634624.0\n",
      "Epoch  6674: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  6690: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  6706: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  8\n",
      "loss_lambda_c =  65510037258240.0\n",
      "Epoch  6722: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  6738: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  6754: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  6770: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample ==  9\n",
      "loss_lambda_c =  215556657512448.0\n",
      "Epoch  6786: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  6802: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  6818: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  10\n",
      "loss_lambda_c =  394071201808384.0\n",
      "Epoch  6834: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  6850: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  6866: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  11\n",
      "loss_lambda_c =  201831888191488.0\n",
      "Epoch  6882: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  6898: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  6914: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  12\n",
      "loss_lambda_c =  107285464481792.0\n",
      "Epoch  6930: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  6946: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  6962: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  13\n",
      "loss_lambda_c =  102977587118080.0\n",
      "Epoch  6978: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  6994: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  7010: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  14\n",
      "loss_lambda_c =  425810204819456.0\n",
      "Epoch  7026: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  7042: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  7058: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  15\n",
      "loss_lambda_c =  19484299493376.0\n",
      "Epoch  7074: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  7090: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  7106: reducing learning rate of group 0 to 1.0000e-05.\n",
      "lambdac step =  1.153327226638794 s\n",
      "loss_c =  119.58619689941406\n",
      "Epoch 10012: reducing learning rate of group 0 to 2.0000e-02.\n",
      "Epoch 10023: reducing learning rate of group 0 to 4.0000e-03.\n",
      "Epoch 10034: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 10045: reducing learning rate of group 0 to 1.6000e-04.\n",
      "Epoch 10056: reducing learning rate of group 0 to 3.2000e-05.\n",
      "Epoch 10067: reducing learning rate of group 0 to 6.4000e-06.\n",
      "thetac step =  1.266286849975586 s\n",
      "step =  2\n",
      "loss z =  68908.859375\n",
      "loss z =  25350.13671875\n",
      "loss z =  5876.0888671875\n",
      "loss z =  5833.859375\n",
      "loss z =  4168.62353515625\n",
      "loss z =  2954.864990234375\n",
      "loss z =  2064.183349609375\n",
      "loss z =  1442.329833984375\n",
      "loss z =  1088.3721923828125\n",
      "loss z =  776.23291015625\n",
      "Epoch   296: reducing learning rate of group 0 to 2.0000e-02.\n",
      "loss z =  589.217041015625\n",
      "loss z =  508.2219543457031\n",
      "loss z =  441.55120849609375\n",
      "loss z =  391.5839538574219\n",
      "loss z =  346.22332763671875\n",
      "loss z =  298.5132141113281\n",
      "loss z =  252.72711181640625\n",
      "loss z =  209.34524536132812\n",
      "loss z =  169.73777770996094\n",
      "loss z =  108.57234191894531\n",
      "Epoch   307: reducing learning rate of group 0 to 4.0000e-03.\n",
      "loss z =  95.091796875\n",
      "loss z =  83.89599609375\n",
      "loss z =  77.25263977050781\n",
      "loss z =  73.36022186279297\n",
      "loss z =  69.78246307373047\n",
      "loss z =  66.32009887695312\n",
      "loss z =  63.190895080566406\n",
      "loss z =  60.384796142578125\n",
      "loss z =  57.96419906616211\n",
      "loss z =  55.955284118652344\n",
      "loss z =  54.448341369628906\n",
      "Epoch   318: reducing learning rate of group 0 to 8.0000e-04.\n",
      "loss z =  53.214813232421875\n",
      "loss z =  52.647926330566406\n",
      "loss z =  52.177242279052734\n",
      "loss z =  51.87517166137695\n",
      "loss z =  51.589454650878906\n",
      "loss z =  51.289710998535156\n",
      "loss z =  50.98410415649414\n",
      "loss z =  50.36598205566406\n",
      "loss z =  50.05359649658203\n",
      "loss z =  49.75773620605469\n",
      "Epoch   329: reducing learning rate of group 0 to 1.6000e-04.\n",
      "z step =  26.18348002433777 s\n",
      "loss_f =  6.280745029449463\n",
      "loss_f =  626.8287963867188\n",
      "loss_f =  324.703369140625\n",
      "Epoch   210: reducing learning rate of group 0 to 3.2000e-05.\n",
      "loss_f =  154.38223266601562\n",
      "loss_f =  72.76371765136719\n",
      "Epoch   221: reducing learning rate of group 0 to 6.4000e-06.\n",
      "loss_f =  47.139278411865234\n",
      "loss_f =  33.112266540527344\n",
      "Epoch   232: reducing learning rate of group 0 to 1.2800e-06.\n",
      "loss_f =  24.3330078125\n",
      "loss_f =  19.053604125976562\n",
      "Epoch   243: reducing learning rate of group 0 to 2.5600e-07.\n",
      "loss_f =  15.785757064819336\n",
      "loss_f =  13.622222900390625\n",
      "Epoch   254: reducing learning rate of group 0 to 5.1200e-08.\n",
      "loss_f =  12.075164794921875\n",
      "loss_f =  10.909821510314941\n",
      "loss_f =  10.003913879394531\n",
      "Epoch   265: reducing learning rate of group 0 to 1.0240e-08.\n",
      "loss_f =  9.276569366455078\n",
      "loss_f =  8.678337097167969\n",
      "loss_f =  8.176175117492676\n",
      "loss_f =  7.746075630187988\n",
      "loss_f =  7.37091064453125\n",
      "loss_f =  7.039090633392334\n",
      "thetaf step =  95.79140949249268 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  331547344896.0\n",
      "sample ==  1\n",
      "loss_lambda_c =  55480097439744.0\n",
      "Epoch  7185: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  7201: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  7217: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  2\n",
      "loss_lambda_c =  820959644745728.0\n",
      "Epoch  7233: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  7249: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  7265: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  3\n",
      "loss_lambda_c =  959681451261952.0\n",
      "Epoch  7281: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  7297: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  7313: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  4\n",
      "loss_lambda_c =  865809236754432.0\n",
      "Epoch  7329: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  7345: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  7361: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  5\n",
      "loss_lambda_c =  1605778884526080.0\n",
      "Epoch  7377: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  7393: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  7409: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  6\n",
      "loss_lambda_c =  487666156044288.0\n",
      "Epoch  7425: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  7441: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  7457: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  7\n",
      "loss_lambda_c =  251257365200896.0\n",
      "Epoch  7473: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  7489: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  7505: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  8\n",
      "loss_lambda_c =  65492345683968.0\n",
      "Epoch  7521: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  7537: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  7553: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch  7569: reducing learning rate of group 0 to 1.0000e-06.\n",
      "sample ==  9\n",
      "loss_lambda_c =  215472284893184.0\n",
      "Epoch  7585: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  7601: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  7617: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  10\n",
      "loss_lambda_c =  394008757010432.0\n",
      "Epoch  7633: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  7649: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  7665: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  11\n",
      "loss_lambda_c =  201787831222272.0\n",
      "Epoch  7681: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  7697: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  7713: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  12\n",
      "loss_lambda_c =  107268687265792.0\n",
      "Epoch  7729: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  7745: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  7761: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  13\n",
      "loss_lambda_c =  102969928318976.0\n",
      "Epoch  7777: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  7793: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  7809: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample ==  14\n",
      "loss_lambda_c =  425796514611200.0\n",
      "Epoch  7825: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  7841: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  7857: reducing learning rate of group 0 to 1.0000e-05.\n",
      "sample ==  15\n",
      "loss_lambda_c =  19472500916224.0\n",
      "Epoch  7873: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch  7889: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch  7905: reducing learning rate of group 0 to 1.0000e-05.\n",
      "lambdac step =  1.3009321689605713 s\n",
      "loss_c =  176.83328247070312\n",
      "Epoch 10078: reducing learning rate of group 0 to 2.0000e-02.\n",
      "Epoch 10089: reducing learning rate of group 0 to 4.0000e-03.\n",
      "Epoch 10100: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 10111: reducing learning rate of group 0 to 1.6000e-04.\n",
      "Epoch 10122: reducing learning rate of group 0 to 3.2000e-05.\n",
      "Epoch 10133: reducing learning rate of group 0 to 6.4000e-06.\n",
      "thetac step =  0.4677610397338867 s\n",
      "step =  3\n",
      "loss z =  6597.0693359375\n",
      "loss z =  10137.6328125\n",
      "loss z =  8598.876953125\n",
      "loss z =  7727.72314453125\n",
      "loss z =  6662.33251953125\n",
      "loss z =  5362.1953125\n",
      "loss z =  4132.11767578125\n",
      "loss z =  3081.072265625\n",
      "loss z =  2217.848388671875\n",
      "loss z =  1500.241455078125\n",
      "Epoch   340: reducing learning rate of group 0 to 2.0000e-02.\n",
      "loss z =  925.7276611328125\n",
      "loss z =  831.4248046875\n",
      "loss z =  731.357421875\n",
      "loss z =  624.6304931640625\n",
      "loss z =  516.054443359375\n",
      "loss z =  411.56610107421875\n",
      "loss z =  324.0091247558594\n",
      "loss z =  259.2292175292969\n",
      "loss z =  217.8463897705078\n",
      "loss z =  176.2749481201172\n",
      "Epoch   351: reducing learning rate of group 0 to 4.0000e-03.\n",
      "loss z =  162.9291534423828\n",
      "loss z =  157.80824279785156\n",
      "loss z =  153.94737243652344\n",
      "loss z =  150.50283813476562\n",
      "loss z =  146.98268127441406\n",
      "loss z =  143.4356689453125\n",
      "loss z =  139.8059844970703\n",
      "loss z =  136.19390869140625\n",
      "loss z =  132.7132568359375\n",
      "loss z =  129.36061096191406\n",
      "loss z =  126.37898254394531\n",
      "Epoch   362: reducing learning rate of group 0 to 8.0000e-04.\n",
      "loss z =  123.67357635498047\n",
      "loss z =  122.81649780273438\n",
      "loss z =  122.05075073242188\n",
      "loss z =  121.4296875\n",
      "loss z =  120.8326187133789\n",
      "loss z =  120.22795104980469\n",
      "loss z =  119.57866668701172\n",
      "loss z =  118.28772735595703\n",
      "loss z =  117.67483520507812\n",
      "loss z =  117.03820037841797\n",
      "Epoch   373: reducing learning rate of group 0 to 1.6000e-04.\n",
      "loss z =  116.43565368652344\n",
      "loss z =  116.30986022949219\n",
      "loss z =  116.18328857421875\n",
      "loss z =  116.05879974365234\n",
      "loss z =  115.93390655517578\n",
      "loss z =  115.8092041015625\n",
      "loss z =  115.68524932861328\n",
      "loss z =  115.56741333007812\n",
      "loss z =  115.4441909790039\n",
      "loss z =  115.32440185546875\n",
      "loss z =  115.20761108398438\n",
      "Epoch   384: reducing learning rate of group 0 to 3.2000e-05.\n",
      "loss z =  115.08561706542969\n",
      "loss z =  115.06076049804688\n",
      "loss z =  115.0367431640625\n",
      "loss z =  115.01229858398438\n",
      "loss z =  114.98793029785156\n",
      "loss z =  114.9401626586914\n",
      "loss z =  114.91741943359375\n",
      "loss z =  114.89364624023438\n",
      "loss z =  114.86946105957031\n",
      "loss z =  114.84613800048828\n",
      "Epoch   395: reducing learning rate of group 0 to 6.4000e-06.\n",
      "loss z =  114.82225036621094\n",
      "loss z =  114.8171615600586\n",
      "loss z =  114.81259155273438\n",
      "loss z =  114.8077621459961\n",
      "loss z =  114.80299377441406\n",
      "loss z =  114.7982177734375\n",
      "loss z =  114.79345703125\n",
      "loss z =  114.7887191772461\n",
      "loss z =  114.78390502929688\n",
      "loss z =  114.779296875\n",
      "loss z =  114.77452850341797\n",
      "Epoch   406: reducing learning rate of group 0 to 1.2800e-06.\n",
      "z step =  47.28311061859131 s\n",
      "loss_f =  19.088964462280273\n",
      "loss_f =  12.847753524780273\n",
      "Epoch   309: reducing learning rate of group 0 to 2.5600e-07.\n",
      "loss_f =  11.06974983215332\n",
      "loss_f =  10.343659400939941\n",
      "loss_f =  9.960421562194824\n",
      "Epoch   320: reducing learning rate of group 0 to 5.1200e-08.\n",
      "loss_f =  9.714689254760742\n",
      "loss_f =  9.540140151977539\n",
      "Epoch   331: reducing learning rate of group 0 to 1.0240e-08.\n",
      "loss_f =  9.403277397155762\n",
      "loss_f =  9.288192749023438\n",
      "loss_f =  9.18665599822998\n",
      "loss_f =  9.094079971313477\n",
      "loss_f =  9.007850646972656\n",
      "loss_f =  8.926336288452148\n",
      "loss_f =  8.8485107421875\n",
      "loss_f =  8.773767471313477\n",
      "loss_f =  8.701574325561523\n",
      "loss_f =  8.631670951843262\n",
      "loss_f =  8.563802719116211\n",
      "loss_f =  8.497861862182617\n",
      "loss_f =  8.433589935302734\n",
      "thetaf step =  98.2963182926178 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  331281793024.0\n",
      "Epoch  7936: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch  7952: reducing learning rate of group 0 to 1.0000e-07.\n",
      "sample ==  1\n",
      "sample ==  2\n",
      "sample ==  3\n",
      "sample ==  4\n",
      "sample ==  5\n",
      "sample ==  6\n",
      "sample ==  7\n",
      "sample ==  8\n",
      "sample ==  9\n",
      "sample ==  10\n",
      "sample ==  11\n",
      "sample ==  12\n",
      "sample ==  13\n",
      "sample ==  14\n",
      "sample ==  15\n",
      "lambdac step =  0.06322550773620605 s\n",
      "loss_c =  175.09912109375\n",
      "Epoch 10144: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 10155: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 10166: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 10177: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 10188: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 10199: reducing learning rate of group 0 to 3.2000e-07.\n",
      "thetac step =  0.5725717544555664 s\n",
      "step =  4\n",
      "loss z =  942884.9375\n",
      "loss z =  61042.10546875\n",
      "loss z =  11269.7353515625\n",
      "loss z =  5681.08056640625\n",
      "loss z =  3212.9111328125\n",
      "loss z =  2137.21630859375\n",
      "loss z =  1586.94189453125\n",
      "loss z =  1269.9644775390625\n",
      "loss z =  1044.6068115234375\n",
      "loss z =  888.5925903320312\n",
      "Epoch   417: reducing learning rate of group 0 to 2.0000e-02.\n",
      "loss z =  4198.1044921875\n",
      "loss z =  719.127685546875\n",
      "loss z =  673.5911865234375\n",
      "loss z =  628.4432983398438\n",
      "loss z =  578.08544921875\n",
      "loss z =  522.4755859375\n",
      "loss z =  463.80462646484375\n",
      "loss z =  405.381591796875\n",
      "loss z =  350.81829833984375\n",
      "loss z =  250.28724670410156\n",
      "Epoch   428: reducing learning rate of group 0 to 4.0000e-03.\n",
      "loss z =  213.17327880859375\n",
      "loss z =  204.56561279296875\n",
      "loss z =  196.44207763671875\n",
      "loss z =  187.7762908935547\n",
      "loss z =  178.39901733398438\n",
      "loss z =  168.82138061523438\n",
      "loss z =  159.41390991210938\n",
      "loss z =  150.54556274414062\n",
      "loss z =  142.5672607421875\n",
      "loss z =  135.26864624023438\n",
      "loss z =  128.68939208984375\n",
      "Epoch   439: reducing learning rate of group 0 to 8.0000e-04.\n",
      "loss z =  122.80868530273438\n",
      "loss z =  121.65876770019531\n",
      "loss z =  120.50912475585938\n",
      "loss z =  119.34723663330078\n",
      "loss z =  118.17284393310547\n",
      "loss z =  116.98575592041016\n",
      "loss z =  115.81593322753906\n",
      "loss z =  113.56716918945312\n",
      "loss z =  112.49444580078125\n",
      "loss z =  111.46012115478516\n",
      "Epoch   450: reducing learning rate of group 0 to 1.6000e-04.\n",
      "loss z =  110.45375061035156\n",
      "loss z =  110.25205993652344\n",
      "loss z =  110.05089569091797\n",
      "loss z =  109.85070037841797\n",
      "loss z =  109.65169525146484\n",
      "loss z =  109.45409393310547\n",
      "loss z =  109.25801849365234\n",
      "loss z =  109.06356048583984\n",
      "loss z =  108.8708267211914\n",
      "loss z =  108.6797866821289\n",
      "loss z =  108.49091339111328\n",
      "Epoch   461: reducing learning rate of group 0 to 3.2000e-05.\n",
      "loss z =  108.30381774902344\n",
      "loss z =  108.26641845703125\n",
      "loss z =  108.22904205322266\n",
      "loss z =  108.19178009033203\n",
      "loss z =  108.15457153320312\n",
      "loss z =  108.08021545410156\n",
      "loss z =  108.0430908203125\n",
      "loss z =  108.0060806274414\n",
      "loss z =  107.96908569335938\n",
      "loss z =  107.93217468261719\n",
      "Epoch   472: reducing learning rate of group 0 to 6.4000e-06.\n",
      "loss z =  107.8953857421875\n",
      "loss z =  107.88799285888672\n",
      "loss z =  107.88065338134766\n",
      "loss z =  107.87332153320312\n",
      "loss z =  107.86597442626953\n",
      "loss z =  107.858642578125\n",
      "loss z =  107.85127258300781\n",
      "loss z =  107.84397888183594\n",
      "loss z =  107.83663940429688\n",
      "loss z =  107.82929992675781\n",
      "loss z =  107.82199096679688\n",
      "Epoch   483: reducing learning rate of group 0 to 1.2800e-06.\n",
      "z step =  45.828280448913574 s\n",
      "loss_f =  15.804373741149902\n",
      "loss_f =  14.106729507446289\n",
      "Epoch   408: reducing learning rate of group 0 to 2.5600e-07.\n",
      "loss_f =  15.350821495056152\n",
      "loss_f =  16.607419967651367\n",
      "Epoch   419: reducing learning rate of group 0 to 5.1200e-08.\n",
      "loss_f =  17.457645416259766\n",
      "loss_f =  17.908418655395508\n",
      "loss_f =  18.054292678833008\n",
      "Epoch   430: reducing learning rate of group 0 to 1.0240e-08.\n",
      "loss_f =  17.99531364440918\n",
      "loss_f =  17.81428337097168\n",
      "loss_f =  17.56639862060547\n",
      "loss_f =  17.285795211791992\n",
      "loss_f =  16.993093490600586\n",
      "loss_f =  16.699899673461914\n",
      "loss_f =  16.4124755859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_f =  16.134037017822266\n",
      "loss_f =  15.865945816040039\n",
      "loss_f =  15.608524322509766\n",
      "loss_f =  15.361631393432617\n",
      "loss_f =  15.124823570251465\n",
      "loss_f =  14.897549629211426\n",
      "thetaf step =  94.93670725822449 s\n",
      "sample ==  0\n",
      "sample ==  1\n",
      "sample ==  2\n",
      "sample ==  3\n",
      "sample ==  4\n",
      "sample ==  5\n",
      "sample ==  6\n",
      "sample ==  7\n",
      "sample ==  8\n",
      "sample ==  9\n",
      "sample ==  10\n",
      "sample ==  11\n",
      "sample ==  12\n",
      "sample ==  13\n",
      "sample ==  14\n",
      "sample ==  15\n",
      "lambdac step =  0.0032701492309570312 s\n",
      "loss_c =  165.431396484375\n",
      "Epoch 10210: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 10221: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 10232: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 10243: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 10254: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 10265: reducing learning rate of group 0 to 3.2000e-07.\n",
      "thetac step =  0.4186978340148926 s\n",
      "step =  5\n",
      "loss z =  664668.8125\n",
      "loss z =  3426.40966796875\n",
      "loss z =  1334.35986328125\n",
      "loss z =  1170.23828125\n",
      "loss z =  1134.296142578125\n",
      "loss z =  737.862548828125\n",
      "loss z =  581.414306640625\n",
      "loss z =  475.06353759765625\n",
      "loss z =  379.31964111328125\n",
      "loss z =  280.73968505859375\n",
      "Epoch   494: reducing learning rate of group 0 to 2.0000e-02.\n",
      "loss z =  281.99432373046875\n",
      "loss z =  226.10781860351562\n",
      "loss z =  195.93600463867188\n",
      "loss z =  187.02706909179688\n",
      "loss z =  177.71331787109375\n",
      "loss z =  166.73809814453125\n",
      "loss z =  155.7381591796875\n",
      "loss z =  143.49900817871094\n",
      "loss z =  136.07693481445312\n",
      "loss z =  121.62539672851562\n",
      "Epoch   505: reducing learning rate of group 0 to 4.0000e-03.\n",
      "loss z =  114.85508728027344\n",
      "loss z =  108.55806732177734\n",
      "loss z =  103.67436981201172\n",
      "loss z =  101.05745697021484\n",
      "loss z =  98.92749786376953\n",
      "loss z =  96.46875\n",
      "loss z =  93.92025756835938\n",
      "loss z =  90.73377227783203\n",
      "loss z =  87.84404754638672\n",
      "loss z =  85.30552673339844\n",
      "loss z =  82.8323974609375\n",
      "Epoch   516: reducing learning rate of group 0 to 8.0000e-04.\n",
      "loss z =  80.58963012695312\n",
      "loss z =  80.00788116455078\n",
      "loss z =  79.51183319091797\n",
      "loss z =  79.06134033203125\n",
      "loss z =  78.60535430908203\n",
      "loss z =  78.14586639404297\n",
      "loss z =  77.68830871582031\n",
      "loss z =  76.78359985351562\n",
      "loss z =  76.33397674560547\n",
      "loss z =  75.89366149902344\n",
      "Epoch   527: reducing learning rate of group 0 to 1.6000e-04.\n",
      "loss z =  75.45524597167969\n",
      "loss z =  75.36634063720703\n",
      "loss z =  75.27806854248047\n",
      "loss z =  75.19034576416016\n",
      "loss z =  75.10322570800781\n",
      "loss z =  75.01544952392578\n",
      "loss z =  74.92781829833984\n",
      "loss z =  74.84149169921875\n",
      "loss z =  74.75508117675781\n",
      "loss z =  74.66905975341797\n",
      "loss z =  74.58287048339844\n",
      "Epoch   538: reducing learning rate of group 0 to 3.2000e-05.\n",
      "loss z =  74.49774169921875\n",
      "loss z =  74.48028564453125\n",
      "loss z =  74.46311950683594\n",
      "loss z =  74.44609069824219\n",
      "loss z =  74.42893981933594\n",
      "loss z =  74.3949203491211\n",
      "loss z =  74.37788391113281\n",
      "loss z =  74.36077880859375\n",
      "loss z =  74.34388732910156\n",
      "loss z =  74.32687377929688\n",
      "Epoch   549: reducing learning rate of group 0 to 6.4000e-06.\n",
      "loss z =  74.30982971191406\n",
      "loss z =  74.3064193725586\n",
      "loss z =  74.30299377441406\n",
      "loss z =  74.29959106445312\n",
      "loss z =  74.29618072509766\n",
      "loss z =  74.2928237915039\n",
      "loss z =  74.28939056396484\n",
      "loss z =  74.28599548339844\n",
      "loss z =  74.2826156616211\n",
      "loss z =  74.27921295166016\n",
      "loss z =  74.27583312988281\n",
      "Epoch   560: reducing learning rate of group 0 to 1.2800e-06.\n",
      "z step =  48.60924196243286 s\n",
      "loss_f =  6.031379699707031\n",
      "loss_f =  5.910915851593018\n",
      "Epoch   507: reducing learning rate of group 0 to 2.5600e-07.\n",
      "loss_f =  5.841315269470215\n",
      "loss_f =  5.798450469970703\n",
      "Epoch   518: reducing learning rate of group 0 to 5.1200e-08.\n",
      "loss_f =  5.77030086517334\n",
      "loss_f =  5.750486850738525\n",
      "Epoch   529: reducing learning rate of group 0 to 1.0240e-08.\n",
      "loss_f =  5.735529899597168\n",
      "loss_f =  5.72343111038208\n",
      "loss_f =  5.7130045890808105\n",
      "loss_f =  5.703608512878418\n",
      "loss_f =  5.694788455963135\n",
      "loss_f =  5.686370849609375\n",
      "loss_f =  5.678195953369141\n",
      "loss_f =  5.6701765060424805\n",
      "loss_f =  5.662250995635986\n",
      "loss_f =  5.6544108390808105\n",
      "loss_f =  5.646605491638184\n",
      "loss_f =  5.6388678550720215\n",
      "loss_f =  5.631159782409668\n",
      "loss_f =  5.623488426208496\n",
      "thetaf step =  92.31962585449219 s\n",
      "sample ==  0\n",
      "loss_lambda_c =  331281727488.0\n",
      "Epoch  7968: reducing learning rate of group 0 to 1.0000e-08.\n",
      "sample ==  1\n",
      "sample ==  2\n",
      "sample ==  3\n",
      "sample ==  4\n",
      "sample ==  5\n",
      "sample ==  6\n",
      "sample ==  7\n",
      "sample ==  8\n",
      "sample ==  9\n",
      "sample ==  10\n",
      "sample ==  11\n",
      "sample ==  12\n",
      "sample ==  13\n",
      "sample ==  14\n",
      "sample ==  15\n",
      "lambdac step =  0.03935694694519043 s\n",
      "loss_c =  114.97042083740234\n",
      "Epoch 10276: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 10287: reducing learning rate of group 0 to 2.0000e-05.\n",
      "Epoch 10298: reducing learning rate of group 0 to 4.0000e-06.\n",
      "Epoch 10309: reducing learning rate of group 0 to 8.0000e-07.\n",
      "Epoch 10320: reducing learning rate of group 0 to 1.6000e-07.\n",
      "Epoch 10331: reducing learning rate of group 0 to 3.2000e-08.\n",
      "thetac step =  0.3234415054321289 s\n",
      "step =  6\n",
      "loss z =  87631.5703125\n",
      "loss z =  32021.3828125\n",
      "loss z =  3104.69775390625\n",
      "loss z =  1763.6529541015625\n",
      "loss z =  1347.993896484375\n",
      "loss z =  1130.0693359375\n",
      "loss z =  894.4173583984375\n",
      "loss z =  701.3535766601562\n",
      "loss z =  539.383056640625\n",
      "loss z =  405.7180480957031\n",
      "Epoch   571: reducing learning rate of group 0 to 2.0000e-02.\n",
      "loss z =  305.96990966796875\n",
      "loss z =  282.8163146972656\n",
      "loss z =  261.24652099609375\n",
      "loss z =  237.14297485351562\n",
      "loss z =  210.36085510253906\n",
      "loss z =  182.96339416503906\n",
      "loss z =  157.2508087158203\n",
      "loss z =  134.55825805664062\n",
      "loss z =  117.96437072753906\n",
      "loss z =  103.36579132080078\n",
      "Epoch   582: reducing learning rate of group 0 to 4.0000e-03.\n",
      "loss z =  95.1280517578125\n",
      "loss z =  91.59564208984375\n",
      "loss z =  88.73622131347656\n",
      "loss z =  86.7935791015625\n",
      "loss z =  84.71168518066406\n",
      "loss z =  82.39572143554688\n",
      "loss z =  80.00997924804688\n",
      "loss z =  77.5185317993164\n",
      "loss z =  75.07533264160156\n",
      "loss z =  72.55210876464844\n",
      "loss z =  70.20858001708984\n",
      "Epoch   593: reducing learning rate of group 0 to 8.0000e-04.\n",
      "loss z =  68.19403076171875\n",
      "loss z =  67.56927490234375\n",
      "loss z =  67.02464294433594\n",
      "loss z =  66.55622863769531\n",
      "loss z =  66.09286499023438\n",
      "loss z =  65.62277221679688\n",
      "loss z =  65.1561279296875\n",
      "loss z =  64.22706604003906\n",
      "loss z =  63.7736701965332\n",
      "loss z =  63.3279914855957\n",
      "Epoch   604: reducing learning rate of group 0 to 1.6000e-04.\n",
      "loss z =  62.89374542236328\n",
      "loss z =  62.80442428588867\n",
      "loss z =  62.71685791015625\n",
      "loss z =  62.630062103271484\n",
      "loss z =  62.543270111083984\n",
      "loss z =  62.45734405517578\n",
      "loss z =  62.37101745605469\n",
      "loss z =  62.28506088256836\n",
      "loss z =  62.19987869262695\n",
      "loss z =  62.11450958251953\n",
      "loss z =  62.02964782714844\n",
      "Epoch   615: reducing learning rate of group 0 to 3.2000e-05.\n",
      "loss z =  61.9454345703125\n",
      "loss z =  61.928497314453125\n",
      "loss z =  61.91150665283203\n",
      "loss z =  61.89451599121094\n",
      "loss z =  61.87750244140625\n",
      "loss z =  61.84381866455078\n",
      "loss z =  61.826847076416016\n",
      "loss z =  61.81001281738281\n",
      "loss z =  61.793357849121094\n",
      "loss z =  61.776546478271484\n",
      "Epoch   626: reducing learning rate of group 0 to 6.4000e-06.\n",
      "loss z =  61.759674072265625\n",
      "loss z =  61.756324768066406\n",
      "loss z =  61.75296401977539\n",
      "loss z =  61.74961471557617\n",
      "loss z =  61.74627685546875\n",
      "loss z =  61.742919921875\n",
      "loss z =  61.73955535888672\n",
      "loss z =  61.73621368408203\n",
      "loss z =  61.732852935791016\n",
      "loss z =  61.729496002197266\n",
      "loss z =  61.72617721557617\n",
      "Epoch   637: reducing learning rate of group 0 to 1.2800e-06.\n",
      "z step =  44.820744037628174 s\n",
      "loss_f =  6.376090049743652\n",
      "loss_f =  6.2627716064453125\n",
      "Epoch   606: reducing learning rate of group 0 to 2.5600e-07.\n",
      "loss_f =  6.197895526885986\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-fd4e915eaf4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_precisions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthetac_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambdac_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/cluster/python/projects/generative_DR_ROM/GenerativeSurrogate.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, n_steps, z_iterations, thetaf_iterations, lambdac_iterations, thetac_iterations, save_iterations, with_precisions)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 batch_samples_thetaf = torch.multinomial(torch.ones(\n\u001b[1;32m    110\u001b[0m                     self.data.n_supervised_samples + self.data.n_unsupervised_samples), self.batch_size_N_thetaf)\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthetaf_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_samples_thetaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthetaf_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0mthetaf_iter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'thetaf step'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cluster/python/projects/generative_DR_ROM/GenerativeSurrogate.py\u001b[0m in \u001b[0;36mthetaf_step\u001b[0;34m(self, batch_samples, step)\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpfOpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpfOpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/genDRROM/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/genDRROM/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(n_steps=50, with_precisions=False, z_iterations=200, thetac_iterations=10000, lambdac_iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(n_steps=50, z_iterations=100, with_precisions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.7111, -10.6361, -11.9003, -12.0129, -12.5206, -10.2839, -11.5509,\n",
       "         -12.7573, -11.3595, -10.9674, -12.2795, -10.3174, -11.9982, -11.1459,\n",
       "         -11.2162, -11.3010],\n",
       "        [ -4.7459,  -3.7773,  -4.8892,  -4.1067,  -3.8800,  -3.8114,  -4.7494,\n",
       "          -3.7266,  -5.0267,  -3.5613,  -4.6837,  -3.5709,  -5.5742,  -3.4835,\n",
       "          -5.0206,  -3.2741],\n",
       "        [ -3.9399,  -3.8375,  -3.3843,  -3.8173,  -3.0564,  -4.4577,  -3.8999,\n",
       "          -4.2297,  -4.1843,  -3.1487,  -3.3023,  -3.4083,  -4.6911,  -3.9021,\n",
       "          -3.5473,  -3.5670],\n",
       "        [ -3.8914,  -2.9802,  -4.3098,  -3.7129,  -3.7588,  -2.3178,  -3.4761,\n",
       "          -3.0459,  -3.5163,  -3.0546,  -3.5428,  -3.7573,  -4.2452,  -3.1094,\n",
       "          -2.7218,  -2.9236],\n",
       "        [ -2.6843,  -3.2286,  -1.6321,  -3.1479,  -3.5123,  -2.1555,  -4.2139,\n",
       "          -3.7112,  -2.6154,  -2.8926,  -2.6891,  -2.4665,  -2.7087,  -2.6506,\n",
       "          -2.2848,  -3.0849],\n",
       "        [ -1.9935,  -3.7032,  -2.7261,  -2.3773,  -2.4934,  -2.5300,  -2.6341,\n",
       "          -2.0847,  -2.2717,  -1.0658,  -1.8831,  -3.5176,  -1.9097,  -1.3108,\n",
       "          -1.7861,  -2.4718],\n",
       "        [ -2.3787,  -2.4050,  -2.8827,  -1.9905,  -2.9388,  -2.6768,  -1.9167,\n",
       "          -2.3094,  -1.9977,  -2.1427,  -2.8775,  -3.1400,  -2.2484,  -1.9335,\n",
       "          -2.9389,  -3.2101],\n",
       "        [ -3.6777,  -3.4739,  -2.5525,  -4.3657,  -1.7906,  -2.4141,  -3.4251,\n",
       "          -2.6441,  -2.6341,  -2.9575,  -2.2037,  -2.5760,  -4.1869,  -3.6375,\n",
       "          -2.6607,  -2.5284],\n",
       "        [ -4.0126,  -3.8999,  -3.4669,  -4.2463,  -3.0343,  -2.6419,  -4.0655,\n",
       "          -4.1560,  -2.6744,  -3.6674,  -2.0262,  -2.7695,  -4.3225,  -3.0332,\n",
       "          -3.0655,  -2.7780],\n",
       "        [ -4.3501,  -5.6444,  -4.4426,  -5.2585,  -4.6287,  -4.5673,  -4.2550,\n",
       "          -4.9179,  -3.9644,  -4.5606,  -4.6392,  -5.3448,  -3.5580,  -4.9330,\n",
       "          -4.3413,  -4.9720],\n",
       "        [ -4.2679,  -3.6561,  -3.5155,  -3.8689,  -3.4737,  -3.3714,  -4.0787,\n",
       "          -3.7189,  -3.5100,  -3.4099,  -3.6862,  -4.9060,  -4.0661,  -3.9661,\n",
       "          -3.0753,  -4.3797],\n",
       "        [ -4.1607,  -4.1537,  -4.3576,  -4.6237,  -3.2680,  -2.8713,  -3.9276,\n",
       "          -3.9352,  -3.0707,  -4.3263,  -2.7815,  -4.6477,  -4.1108,  -4.6012,\n",
       "          -3.5750,  -3.1531],\n",
       "        [ -3.3324,  -3.5398,  -3.2670,  -2.6021,  -2.5712,  -3.6938,  -3.0189,\n",
       "          -3.2891,  -3.2733,  -3.1509,  -2.7062,  -2.6131,  -2.8640,  -2.4339,\n",
       "          -3.2277,  -2.9980],\n",
       "        [ -2.8699,  -3.1089,  -1.9598,  -2.9892,  -1.4909,  -1.9963,  -2.6549,\n",
       "          -2.5205,  -1.8662,  -1.9436,  -1.8230,  -2.8157,  -3.0953,  -2.0075,\n",
       "          -2.4189,  -2.3737],\n",
       "        [ -2.0807,  -2.2856,  -2.5611,  -1.6816,  -3.2134,  -2.6298,  -2.6749,\n",
       "          -2.3430,  -3.2690,  -1.6518,  -2.5258,  -2.2220,  -2.9701,  -1.2514,\n",
       "          -2.0995,  -2.1340],\n",
       "        [ -3.2125,  -3.3186,  -3.1877,  -4.1638,  -3.5233,  -2.3366,  -3.3708,\n",
       "          -3.5312,  -3.1271,  -3.6358,  -2.9584,  -3.5581,  -3.5729,  -4.2600,\n",
       "          -3.9646,  -3.1700]], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.log_lambdac_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gs.GenerativeSurrogate()\n",
    "# model.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = {n for n in range(0, 4)}\n",
    "testData = dta.StokesData(unsupervised_samples=test_samples)\n",
    "testData.read_data()\n",
    "# trainingData.plotMicrostruct(1)\n",
    "testData.reshape_microstructure_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_z_prediction =  293811.90625\n",
      "loss_z_prediction =  18.48788833618164\n"
     ]
    }
   ],
   "source": [
    "uf_pred, Z = model.predict(testData, max_iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uf_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(n_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.outer(np.linspace(0, 1, 129), np.ones(129))\n",
    "y = x.copy().T # transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.plot_surface(x, y, np.reshape(uf_pred[0], (129, 129)),cmap='viridis', edgecolor='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = np.reshape(model.data.P[0, :], (129, 129))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.plot_surface(x, y, pp.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_input_reconstruction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_generated_microstructures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (generative_DR_ROM)",
   "language": "python",
   "name": "pycharm-712517ba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
